{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "da214ca6",
   "metadata": {},
   "source": [
    "# Factorisation-aware matrix element emulator\n",
    "\n",
    "This is a quickstart notebook that will take you through the necessary steps to reproduce the method described in the paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5b09063e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "24672827",
   "metadata": {},
   "outputs": [],
   "source": [
    "# comment this out if you want to use GPU\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '-1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ddc3fd07",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fame.phase_space import phasespace\n",
    "from fame.utilities import utility_functions, tests\n",
    "from fame.data_generation import cs_dipole, model_inputs\n",
    "from fame.model.dipole_model import DipoleModel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d04f3ef7",
   "metadata": {},
   "source": [
    "# Generate phase-space points using RAMBO with FastJet clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "738a5b1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "w = 1000 # sqrt(s)\n",
    "num_jets = 4 # number of final state jets\n",
    "train_points = 500000\n",
    "test_points = 100000\n",
    "num_points = train_points + test_points # number of phase-space points to generate\n",
    "y_global_cut = 0.001 # global phase-space cut\n",
    "num_cores = 16 # number of cores for parallel clustering of jets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "27aa759b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "06b5579db61e457a85657363b147f26e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Phase-space points:   0%|          | 0/600000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "######## Finished generating generic phase-space points #########\n"
     ]
    }
   ],
   "source": [
    "# clusters points using e+e- kt algorithm\n",
    "X = phasespace.generate_generic(\n",
    "    num_jets,\n",
    "    num_points,\n",
    "    w,\n",
    "    y_global_cut,\n",
    "    num_cores\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f445b6b",
   "metadata": {},
   "source": [
    "Phase-space points are structured in the following way:\n",
    "1. First and seconds rows are e+ and e-\n",
    "2. Third row is quark\n",
    "3. Fourth row is anti-quark\n",
    "4. Fifth and onwards are gluons\n",
    "\n",
    "This helps us to keep consistency between the inputs we calculate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1b6c3aef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 500.        ,    0.        ,    0.        ,  500.        ],\n",
       "       [ 500.        ,    0.        ,    0.        , -500.        ],\n",
       "       [ 268.19993517,  -27.5631736 ,  -16.54318263,  266.26640756],\n",
       "       [ 118.69287146,  -65.239343  ,  -27.94847027,  -95.13521362],\n",
       "       [ 347.29117021,  124.57254434, -207.69860101, -248.90586421],\n",
       "       [ 265.81602316,  -31.77002774,  252.19025391,   77.77467027]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3add144b",
   "metadata": {},
   "source": [
    "# Evaluate phase-space points using matrix element provider\n",
    "## here we use NJet but others are available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5d463b83",
   "metadata": {},
   "outputs": [],
   "source": [
    "njet_data, njet_order = utility_functions.run_njet(num_jets)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c52759c5",
   "metadata": {},
   "source": [
    "Check 'njet_data' has the correct incoming and outgoing particles.\n",
    "\n",
    "'inc': [11, -11] -> e+ e-\n",
    "\n",
    "'out': [1, -1, 21, 21, 21] -> ddxggg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "255d6ce0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'born': 0, 'inc': [11, -11], 'loop': 0, 'mcn': 1, 'name': 'eeddxGG', 'out': [1, -1, 21, 21]}]\n"
     ]
    }
   ],
   "source": [
    "print(njet_data[0][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "84b5adb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- channel eeddxGG -------- (600000 points)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0a56daff498646c980ea53c63b690f65",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/600000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "Y = utility_functions.generate_LO_njet([x.tolist() for x in X], njet_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37760ec3",
   "metadata": {},
   "source": [
    "# Split data into training and testing sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dd4a51bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test = X[:train_points], X[train_points:]\n",
    "Y_train, Y_test = Y[:train_points], Y[train_points:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "355c298a",
   "metadata": {},
   "source": [
    "Check magnitude of training and testing datasets are similar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dea862ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "bins = np.logspace(np.log10(np.min(Y_train)), np.log10(np.max(Y_train)), 50)\n",
    "plt.figure()\n",
    "plt.hist(Y_train, bins, weights=np.ones(train_points) / train_points, alpha=0.5, label='Training')\n",
    "plt.hist(Y_test, bins, weights=np.ones(test_points) / test_points, alpha=0.5, label='Testing')\n",
    "plt.legend()\n",
    "plt.xscale(\"log\")\n",
    "plt.xlabel(r\"$|\\mathcal{M}|^{2}$\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21cb5e86",
   "metadata": {},
   "source": [
    "# Generate neural network inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18277946",
   "metadata": {},
   "source": [
    "## Calculate dipoles and recoil factors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdeb97b5",
   "metadata": {},
   "source": [
    "First need to get relevant permutations where each permutation is in the form $(i, j, k)$\n",
    "\n",
    "$i$ = emitter,\n",
    "$j$ = emitted,\n",
    "$k$ = spectator\n",
    "\n",
    "Partons are numbered from $(1, ..., n_{j})$ where 1 = $q$, 2 = $\\bar{q}$, and everything 3+ = $g$, like the phase-space points.\n",
    "\n",
    "For example permutation $(2, 4, 1)$ would mean the anti-quark emits a gluon and the quark is the spectator.\n",
    "\n",
    "**For each permutation we have a corresponding dipole and recoil factor.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "06033d1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "relevant_permutations = model_inputs.get_relevant_permutations(num_jets)\n",
    "tests.check_relevant_permutations(relevant_permutations, num_jets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1b701f68",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, 3, 2),\n",
       " (1, 3, 4),\n",
       " (1, 4, 2),\n",
       " (1, 4, 3),\n",
       " (2, 3, 1),\n",
       " (2, 3, 4),\n",
       " (2, 4, 1),\n",
       " (2, 4, 3),\n",
       " (3, 4, 1),\n",
       " (3, 4, 2)]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "relevant_permutations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "db8fd33a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialise classes that will generate inputs\n",
    "CS = cs_dipole.CS_dipole(mode='gluon')\n",
    "relevant_inputs = model_inputs.ModelInputsGenerator(relevant_permutations, CS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "790e65ea",
   "metadata": {},
   "source": [
    "If there are two or more gluons in the final state we need to account for spin-correlation effects that have been averaged out in the spin-averaged Catani-Seymour dipoles.\n",
    "\n",
    "**For each pair of gluons in the final state, $(i, j)$, there are $sin(2\\phi_{ij})$ and $cos(2\\phi_{ij})$ terms.**\n",
    "\n",
    "**The total set of dipoles is therefore the Catani-Seymour dipoles and the spin-correlation terms.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f28cd241",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate model inputs\n",
    "if num_jets > 3:\n",
    "    # extra spin-correlation terms\n",
    "    train_phi_terms = model_inputs.calculate_cs_phis(p=X_train, num_jets=num_jets, cast=False)\n",
    "    tests.check_phi_terms(train_phi_terms, num_jets)\n",
    "    \n",
    "    # Catani-Seymour dipoles and recoil factors\n",
    "    # concatenate phi terms with Catani-Seymour dipoles\n",
    "    train_dipoles, train_ys = relevant_inputs.calculate_inputs(\n",
    "        p_array=X_train,\n",
    "        to_concat=[*train_phi_terms]\n",
    "    )\n",
    "    \n",
    "    tests.check_recoil_factors(train_ys, num_jets)\n",
    "    tests.check_all_dipoles(train_dipoles, num_jets)\n",
    "else:\n",
    "    # no phi terms for 3-jet case\n",
    "    train_dipoles, train_ys = relevant_inputs.calculate_inputs(p_array=X_train)\n",
    "    tests.check_all_dipoles(train_dipoles, num_jets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "58bbdccf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set scales of problem\n",
    "pred_scale = np.min(Y_train)\n",
    "dipole_scale = np.mean(train_dipoles)\n",
    "coef_scale = pred_scale / dipole_scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0fdba134",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X shape = (500000, 6, 4), Y shape = (500000,), dipoles shape = (500000, 12), ys shape = (500000, 10)\n",
      "pred_scale = 5.5057297814751424e-12, dipole_scale = 0.0006558569568096939, coef_scale = 8.394711261822763e-09\n"
     ]
    }
   ],
   "source": [
    "print(f\"X shape = {X_train.shape}, Y shape = {Y_train.shape}, dipoles shape = {train_dipoles.shape}, ys shape = {train_ys.shape}\")\n",
    "print(f\"pred_scale = {pred_scale}, dipole_scale = {dipole_scale}, coef_scale = {coef_scale}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0489c3bb",
   "metadata": {},
   "source": [
    "Now we have all the inputs generated we feed it to our emulator."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f31aa27",
   "metadata": {},
   "source": [
    "# Constructing neural network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7c33a25",
   "metadata": {},
   "source": [
    "## Define neural network hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8ab5040e",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 4096\n",
    "lr = 0.001\n",
    "# min_delta for EarlyStopping, should go smaller for lower multiplicity as higher accuracy there\n",
    "min_delta = 1E-6\n",
    "# J is tuned manually such that f_pen << mse\n",
    "J = 1E6"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e18dabc1",
   "metadata": {},
   "source": [
    "## Initialise class for building dipole NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2acf1b2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "dipole_model = DipoleModel(\n",
    "    num_jets=num_jets,\n",
    "    permutations=relevant_permutations,\n",
    "    X=X_train,\n",
    "    Y=Y_train,\n",
    "    recoil_factors=train_ys,\n",
    "    dipoles=train_dipoles,\n",
    "    pred_scale=pred_scale,\n",
    "    coef_scale=coef_scale,\n",
    "    J=J\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26b2ff96",
   "metadata": {},
   "source": [
    "## Preprocesssing inputs involves standardising inputs and outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d0f7000d",
   "metadata": {},
   "outputs": [],
   "source": [
    "dipole_model.preprocess_inputs()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04dc269b",
   "metadata": {},
   "source": [
    "All scalers can be accessed through the DipoleModel class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c3f3be33",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Variable 'mean:0' shape=(1,) dtype=float32, numpy=array([4.296902], dtype=float32)>,\n",
       " <tf.Variable 'variance:0' shape=(1,) dtype=float32, numpy=array([3.103604], dtype=float32)>,\n",
       " <tf.Variable 'count:0' shape=() dtype=int64, numpy=500000>]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dipole_model.y_scaler.variables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f2790f2",
   "metadata": {},
   "source": [
    "## Building model: create densely connected neural network with custom loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "80dfac09",
   "metadata": {},
   "outputs": [],
   "source": [
    "dipole_model.build_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d3216a9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"dipole_4_jets\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, 4, 4)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "normalization (Normalization)   (None, 4, 4)         9           input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            [(None, 10)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "flatten (Flatten)               (None, 16)           0           normalization[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "normalization_1 (Normalization) (None, 10)           21          input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "concatenate (Concatenate)       (None, 26)           0           flatten[0][0]                    \n",
      "                                                                 normalization_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 64)           1728        concatenate[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 128)          8320        dense[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 256)          33024       dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 512)          131584      dense_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_4 (Dense)                 (None, 768)          393984      dense_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_5 (Dense)                 (None, 386)          296834      dense_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_6 (Dense)                 (None, 128)          49536       dense_5[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_7 (Dense)                 (None, 64)           8256        dense_6[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_8 (Dense)                 (None, 12)           780         dense_7[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "tf.math.sinh (TFOpLambda)       (None, 12)           0           dense_8[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "input_4 (InputLayer)            [(None, 12)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_3 (InputLayer)            [(None, 1)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "tf.math.multiply (TFOpLambda)   (None, 12)           0           tf.math.sinh[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "tf.math.multiply_1 (TFOpLambda) (None, 12)           0           tf.math.multiply[0][0]           \n",
      "                                                                 input_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "tf.math.reduce_sum (TFOpLambda) (None,)              0           tf.math.multiply_1[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "tf.math.truediv (TFOpLambda)    (None,)              0           tf.math.reduce_sum[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "tf.math.multiply_2 (TFOpLambda) (None, 12)           0           tf.math.multiply[0][0]           \n",
      "                                                                 input_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "tf.math.asinh (TFOpLambda)      (None,)              0           tf.math.truediv[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "tf.math.abs (TFOpLambda)        (None, 12)           0           tf.math.multiply_2[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "tf.math.pow (TFOpLambda)        (None, 12)           0           input_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "normalization_2 (Normalization) (None, 1)            3           tf.math.asinh[0][0]              \n",
      "                                                                 tf.math.asinh_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "tf.math.multiply_3 (TFOpLambda) (None, 12)           0           tf.math.abs[0][0]                \n",
      "                                                                 tf.math.pow[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "tf.convert_to_tensor (TFOpLambd (None, 1)            0           normalization_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "tf.cast (TFOpLambda)            (None, 1)            0           input_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "tf.math.reduce_sum_2 (TFOpLambd (None,)              0           tf.math.multiply_3[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "tf.math.reduce_sum_1 (TFOpLambd (None,)              0           tf.math.pow[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "tf.math.squared_difference (TFO (None, 1)            0           tf.convert_to_tensor[0][0]       \n",
      "                                                                 tf.cast[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "tf.math.truediv_1 (TFOpLambda)  (None,)              0           tf.math.reduce_sum_2[0][0]       \n",
      "                                                                 tf.math.reduce_sum_1[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "tf.math.reduce_mean (TFOpLambda (None,)              0           tf.math.squared_difference[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "tf.math.multiply_4 (TFOpLambda) (None,)              0           tf.math.truediv_1[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "tf.__operators__.add (TFOpLambd (None,)              0           tf.math.reduce_mean[0][0]        \n",
      "                                                                 tf.math.multiply_4[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "add_loss (AddLoss)              (None,)              0           tf.__operators__.add[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "tf.math.multiply_5 (TFOpLambda) (None, 12)           0           tf.math.multiply[0][0]           \n",
      "                                                                 input_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "tf.math.reduce_sum_3 (TFOpLambd (None,)              0           tf.math.multiply_5[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "tf.math.truediv_2 (TFOpLambda)  (None,)              0           tf.math.reduce_sum_3[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "tf.math.asinh_1 (TFOpLambda)    (None,)              0           tf.math.truediv_2[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "tf.convert_to_tensor_1 (TFOpLam (None, 1)            0           normalization_2[1][0]            \n",
      "__________________________________________________________________________________________________\n",
      "tf.cast_1 (TFOpLambda)          (None, 1)            0           input_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "tf.math.squared_difference_1 (T (None, 1)            0           tf.convert_to_tensor_1[0][0]     \n",
      "                                                                 tf.cast_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "tf.math.reduce_mean_1 (TFOpLamb (None,)              0           tf.math.squared_difference_1[0][0\n",
      "__________________________________________________________________________________________________\n",
      "add_metric (AddMetric)          (None,)              0           tf.math.reduce_mean_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "tf.math.multiply_6 (TFOpLambda) (None, 12)           0           tf.math.multiply[0][0]           \n",
      "                                                                 input_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "tf.math.abs_1 (TFOpLambda)      (None, 12)           0           tf.math.multiply_6[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "tf.math.pow_1 (TFOpLambda)      (None, 12)           0           input_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "tf.math.multiply_7 (TFOpLambda) (None, 12)           0           tf.math.abs_1[0][0]              \n",
      "                                                                 tf.math.pow_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "tf.math.reduce_sum_5 (TFOpLambd (None,)              0           tf.math.multiply_7[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "tf.math.reduce_sum_4 (TFOpLambd (None,)              0           tf.math.pow_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "tf.math.truediv_3 (TFOpLambda)  (None,)              0           tf.math.reduce_sum_5[0][0]       \n",
      "                                                                 tf.math.reduce_sum_4[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "tf.math.multiply_8 (TFOpLambda) (None,)              0           tf.math.truediv_3[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "add_metric_1 (AddMetric)        (None,)              0           tf.math.multiply_8[0][0]         \n",
      "==================================================================================================\n",
      "Total params: 924,079\n",
      "Trainable params: 924,046\n",
      "Non-trainable params: 33\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "dipole_model.model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c8cc61b",
   "metadata": {},
   "source": [
    "# Train model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b557f3e4",
   "metadata": {},
   "source": [
    "The following wrapper functions will train the network until the desired min_delta is reached in the validation loss.\n",
    "\n",
    "It is possible to terminate training prematurely by interrupting the Jupyter kernel. The model will have the most up-to-date weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f9905192",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10000\n",
      "98/98 [==============================] - 5s 38ms/step - loss: 0.3393 - mse: 0.3393 - f_pen: 1.0352e-06 - val_loss: 0.0663 - val_mse: 0.0663 - val_f_pen: 7.8489e-07\n",
      "Epoch 2/10000\n",
      "98/98 [==============================] - 4s 38ms/step - loss: 0.0389 - mse: 0.0389 - f_pen: 6.6785e-07 - val_loss: 0.0216 - val_mse: 0.0216 - val_f_pen: 5.9924e-07\n",
      "Epoch 3/10000\n",
      "98/98 [==============================] - 4s 38ms/step - loss: 0.0150 - mse: 0.0150 - f_pen: 5.7138e-07 - val_loss: 0.0105 - val_mse: 0.0105 - val_f_pen: 5.4679e-07\n",
      "Epoch 4/10000\n",
      "98/98 [==============================] - 4s 39ms/step - loss: 0.0086 - mse: 0.0086 - f_pen: 5.3845e-07 - val_loss: 0.0069 - val_mse: 0.0069 - val_f_pen: 5.2210e-07\n",
      "Epoch 5/10000\n",
      "98/98 [==============================] - 4s 37ms/step - loss: 0.0059 - mse: 0.0059 - f_pen: 5.1937e-07 - val_loss: 0.0049 - val_mse: 0.0049 - val_f_pen: 5.1223e-07\n",
      "Epoch 6/10000\n",
      "98/98 [==============================] - 4s 37ms/step - loss: 0.0047 - mse: 0.0047 - f_pen: 5.0440e-07 - val_loss: 0.0037 - val_mse: 0.0037 - val_f_pen: 4.9813e-07\n",
      "Epoch 7/10000\n",
      "98/98 [==============================] - 4s 37ms/step - loss: 0.0035 - mse: 0.0035 - f_pen: 4.9374e-07 - val_loss: 0.0030 - val_mse: 0.0030 - val_f_pen: 4.8998e-07\n",
      "Epoch 8/10000\n",
      "98/98 [==============================] - 4s 36ms/step - loss: 0.0030 - mse: 0.0030 - f_pen: 4.8585e-07 - val_loss: 0.0025 - val_mse: 0.0025 - val_f_pen: 4.7971e-07\n",
      "Epoch 9/10000\n",
      "98/98 [==============================] - 4s 37ms/step - loss: 0.0025 - mse: 0.0025 - f_pen: 4.7863e-07 - val_loss: 0.0022 - val_mse: 0.0022 - val_f_pen: 4.7690e-07\n",
      "Epoch 10/10000\n",
      "98/98 [==============================] - 4s 37ms/step - loss: 0.0021 - mse: 0.0021 - f_pen: 4.7522e-07 - val_loss: 0.0018 - val_mse: 0.0018 - val_f_pen: 4.7113e-07\n",
      "Epoch 11/10000\n",
      "98/98 [==============================] - 4s 36ms/step - loss: 0.0020 - mse: 0.0020 - f_pen: 4.7091e-07 - val_loss: 0.0016 - val_mse: 0.0016 - val_f_pen: 4.7258e-07\n",
      "Epoch 12/10000\n",
      "98/98 [==============================] - 4s 37ms/step - loss: 0.0016 - mse: 0.0016 - f_pen: 4.7006e-07 - val_loss: 0.0014 - val_mse: 0.0014 - val_f_pen: 4.7271e-07\n",
      "Epoch 13/10000\n",
      "98/98 [==============================] - 4s 36ms/step - loss: 0.0015 - mse: 0.0015 - f_pen: 4.6894e-07 - val_loss: 0.0012 - val_mse: 0.0012 - val_f_pen: 4.6822e-07\n",
      "Epoch 14/10000\n",
      "98/98 [==============================] - 4s 37ms/step - loss: 0.0013 - mse: 0.0013 - f_pen: 4.6806e-07 - val_loss: 0.0011 - val_mse: 0.0011 - val_f_pen: 4.6924e-07\n",
      "Epoch 15/10000\n",
      "98/98 [==============================] - 4s 37ms/step - loss: 0.0013 - mse: 0.0013 - f_pen: 4.6877e-07 - val_loss: 0.0010 - val_mse: 0.0010 - val_f_pen: 4.6861e-07\n",
      "Epoch 16/10000\n",
      "98/98 [==============================] - 4s 37ms/step - loss: 0.0012 - mse: 0.0012 - f_pen: 4.7067e-07 - val_loss: 9.6404e-04 - val_mse: 9.6356e-04 - val_f_pen: 4.7491e-07\n",
      "Epoch 17/10000\n",
      "98/98 [==============================] - 4s 36ms/step - loss: 0.0010 - mse: 0.0010 - f_pen: 4.7241e-07 - val_loss: 8.7448e-04 - val_mse: 8.7400e-04 - val_f_pen: 4.7540e-07\n",
      "Epoch 18/10000\n",
      "98/98 [==============================] - 4s 36ms/step - loss: 0.0010 - mse: 0.0010 - f_pen: 4.7498e-07 - val_loss: 8.0925e-04 - val_mse: 8.0878e-04 - val_f_pen: 4.7377e-07\n",
      "Epoch 19/10000\n",
      "98/98 [==============================] - 4s 37ms/step - loss: 8.5000e-04 - mse: 8.4952e-04 - f_pen: 4.7775e-07 - val_loss: 8.6775e-04 - val_mse: 8.6727e-04 - val_f_pen: 4.7655e-07\n",
      "Epoch 20/10000\n",
      "98/98 [==============================] - 3s 35ms/step - loss: 9.6914e-04 - mse: 9.6866e-04 - f_pen: 4.8090e-07 - val_loss: 7.2062e-04 - val_mse: 7.2013e-04 - val_f_pen: 4.8090e-07\n",
      "Epoch 21/10000\n",
      "98/98 [==============================] - 4s 36ms/step - loss: 8.0458e-04 - mse: 8.0409e-04 - f_pen: 4.8440e-07 - val_loss: 8.9382e-04 - val_mse: 8.9333e-04 - val_f_pen: 4.8292e-07\n",
      "Epoch 22/10000\n",
      "98/98 [==============================] - 4s 36ms/step - loss: 8.4342e-04 - mse: 8.4293e-04 - f_pen: 4.8682e-07 - val_loss: 5.8832e-04 - val_mse: 5.8783e-04 - val_f_pen: 4.8990e-07\n",
      "Epoch 23/10000\n",
      "98/98 [==============================] - 4s 36ms/step - loss: 6.8061e-04 - mse: 6.8012e-04 - f_pen: 4.9139e-07 - val_loss: 5.4904e-04 - val_mse: 5.4854e-04 - val_f_pen: 4.9549e-07\n",
      "Epoch 24/10000\n",
      "98/98 [==============================] - 4s 37ms/step - loss: 7.7390e-04 - mse: 7.7340e-04 - f_pen: 4.9586e-07 - val_loss: 6.3301e-04 - val_mse: 6.3251e-04 - val_f_pen: 5.0019e-07\n",
      "Epoch 25/10000\n",
      "98/98 [==============================] - 4s 36ms/step - loss: 6.1465e-04 - mse: 6.1415e-04 - f_pen: 5.0100e-07 - val_loss: 5.6076e-04 - val_mse: 5.6026e-04 - val_f_pen: 4.9893e-07\n",
      "Epoch 26/10000\n",
      "98/98 [==============================] - 4s 37ms/step - loss: 6.5502e-04 - mse: 6.5451e-04 - f_pen: 5.0569e-07 - val_loss: 4.7181e-04 - val_mse: 4.7130e-04 - val_f_pen: 5.0998e-07\n",
      "Epoch 27/10000\n",
      "98/98 [==============================] - 4s 36ms/step - loss: 6.3660e-04 - mse: 6.3609e-04 - f_pen: 5.0598e-07 - val_loss: 5.0009e-04 - val_mse: 4.9958e-04 - val_f_pen: 5.0858e-07\n",
      "Epoch 28/10000\n",
      "98/98 [==============================] - 4s 37ms/step - loss: 6.0639e-04 - mse: 6.0587e-04 - f_pen: 5.1051e-07 - val_loss: 4.9025e-04 - val_mse: 4.8973e-04 - val_f_pen: 5.1077e-07\n",
      "Epoch 29/10000\n",
      "98/98 [==============================] - 4s 36ms/step - loss: 6.0917e-04 - mse: 6.0865e-04 - f_pen: 5.1589e-07 - val_loss: 4.5339e-04 - val_mse: 4.5287e-04 - val_f_pen: 5.1839e-07\n",
      "Epoch 30/10000\n",
      "98/98 [==============================] - 4s 36ms/step - loss: 5.3337e-04 - mse: 5.3285e-04 - f_pen: 5.1988e-07 - val_loss: 4.3108e-04 - val_mse: 4.3056e-04 - val_f_pen: 5.2032e-07\n",
      "Epoch 31/10000\n",
      "98/98 [==============================] - 4s 36ms/step - loss: 5.2367e-04 - mse: 5.2315e-04 - f_pen: 5.2240e-07 - val_loss: 4.0568e-04 - val_mse: 4.0516e-04 - val_f_pen: 5.2674e-07\n",
      "Epoch 32/10000\n",
      "98/98 [==============================] - 4s 37ms/step - loss: 6.2713e-04 - mse: 6.2660e-04 - f_pen: 5.2736e-07 - val_loss: 4.2000e-04 - val_mse: 4.1947e-04 - val_f_pen: 5.2360e-07\n",
      "Epoch 33/10000\n",
      "98/98 [==============================] - 4s 36ms/step - loss: 6.0429e-04 - mse: 6.0375e-04 - f_pen: 5.3988e-07 - val_loss: 3.0650e-04 - val_mse: 3.0596e-04 - val_f_pen: 5.4003e-07\n",
      "Epoch 34/10000\n",
      "98/98 [==============================] - 4s 37ms/step - loss: 4.3428e-04 - mse: 4.3374e-04 - f_pen: 5.3875e-07 - val_loss: 3.5618e-04 - val_mse: 3.5564e-04 - val_f_pen: 5.4249e-07\n",
      "Epoch 35/10000\n",
      "98/98 [==============================] - 4s 36ms/step - loss: 5.7778e-04 - mse: 5.7723e-04 - f_pen: 5.4241e-07 - val_loss: 3.0625e-04 - val_mse: 3.0570e-04 - val_f_pen: 5.4631e-07\n",
      "Epoch 36/10000\n",
      "98/98 [==============================] - 4s 37ms/step - loss: 3.9894e-04 - mse: 3.9840e-04 - f_pen: 5.4249e-07 - val_loss: 2.8838e-04 - val_mse: 2.8783e-04 - val_f_pen: 5.4364e-07\n",
      "Epoch 37/10000\n",
      "98/98 [==============================] - 4s 37ms/step - loss: 5.3075e-04 - mse: 5.3020e-04 - f_pen: 5.5274e-07 - val_loss: 3.2850e-04 - val_mse: 3.2795e-04 - val_f_pen: 5.4919e-07\n",
      "Epoch 38/10000\n",
      "98/98 [==============================] - 3s 36ms/step - loss: 5.3734e-04 - mse: 5.3678e-04 - f_pen: 5.5341e-07 - val_loss: 2.7364e-04 - val_mse: 2.7308e-04 - val_f_pen: 5.5713e-07\n",
      "Epoch 39/10000\n",
      "98/98 [==============================] - 4s 36ms/step - loss: 4.5610e-04 - mse: 4.5554e-04 - f_pen: 5.5760e-07 - val_loss: 3.1272e-04 - val_mse: 3.1216e-04 - val_f_pen: 5.6362e-07\n",
      "Epoch 40/10000\n",
      "98/98 [==============================] - 4s 37ms/step - loss: 4.0967e-04 - mse: 4.0910e-04 - f_pen: 5.6290e-07 - val_loss: 2.9141e-04 - val_mse: 2.9085e-04 - val_f_pen: 5.6466e-07\n",
      "Epoch 41/10000\n",
      "98/98 [==============================] - 4s 37ms/step - loss: 3.8637e-04 - mse: 3.8580e-04 - f_pen: 5.6687e-07 - val_loss: 2.8790e-04 - val_mse: 2.8733e-04 - val_f_pen: 5.6789e-07\n",
      "Epoch 42/10000\n",
      "98/98 [==============================] - 4s 36ms/step - loss: 5.5107e-04 - mse: 5.5049e-04 - f_pen: 5.7864e-07 - val_loss: 2.5612e-04 - val_mse: 2.5553e-04 - val_f_pen: 5.8289e-07\n",
      "Epoch 43/10000\n",
      "98/98 [==============================] - 4s 36ms/step - loss: 3.5346e-04 - mse: 3.5288e-04 - f_pen: 5.7854e-07 - val_loss: 2.2041e-04 - val_mse: 2.1983e-04 - val_f_pen: 5.7625e-07\n",
      "Epoch 44/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "98/98 [==============================] - 4s 36ms/step - loss: 5.2722e-04 - mse: 5.2664e-04 - f_pen: 5.8379e-07 - val_loss: 2.2813e-04 - val_mse: 2.2754e-04 - val_f_pen: 5.8991e-07\n",
      "Epoch 45/10000\n",
      "98/98 [==============================] - 4s 36ms/step - loss: 3.2864e-04 - mse: 3.2806e-04 - f_pen: 5.8655e-07 - val_loss: 2.2614e-04 - val_mse: 2.2555e-04 - val_f_pen: 5.8736e-07\n",
      "Epoch 46/10000\n",
      "98/98 [==============================] - 4s 36ms/step - loss: 4.4654e-04 - mse: 4.4595e-04 - f_pen: 5.9160e-07 - val_loss: 2.4126e-04 - val_mse: 2.4066e-04 - val_f_pen: 5.9812e-07\n",
      "Epoch 47/10000\n",
      "98/98 [==============================] - 4s 37ms/step - loss: 4.5145e-04 - mse: 4.5085e-04 - f_pen: 5.9488e-07 - val_loss: 2.4082e-04 - val_mse: 2.4022e-04 - val_f_pen: 6.0233e-07\n",
      "Epoch 48/10000\n",
      "98/98 [==============================] - 4s 37ms/step - loss: 3.9464e-04 - mse: 3.9404e-04 - f_pen: 6.0116e-07 - val_loss: 4.0045e-04 - val_mse: 3.9984e-04 - val_f_pen: 6.1015e-07\n",
      "Epoch 49/10000\n",
      "98/98 [==============================] - 4s 36ms/step - loss: 4.1536e-04 - mse: 4.1475e-04 - f_pen: 6.0701e-07 - val_loss: 2.7497e-04 - val_mse: 2.7437e-04 - val_f_pen: 5.9983e-07\n",
      "Epoch 50/10000\n",
      "98/98 [==============================] - 4s 37ms/step - loss: 3.5365e-04 - mse: 3.5304e-04 - f_pen: 6.1023e-07 - val_loss: 2.0041e-04 - val_mse: 1.9979e-04 - val_f_pen: 6.1353e-07\n",
      "Epoch 51/10000\n",
      "98/98 [==============================] - 4s 37ms/step - loss: 4.9073e-04 - mse: 4.9012e-04 - f_pen: 6.1406e-07 - val_loss: 2.2405e-04 - val_mse: 2.2344e-04 - val_f_pen: 6.1580e-07\n",
      "Epoch 52/10000\n",
      "98/98 [==============================] - 3s 36ms/step - loss: 3.3394e-04 - mse: 3.3332e-04 - f_pen: 6.1927e-07 - val_loss: 3.9105e-04 - val_mse: 3.9043e-04 - val_f_pen: 6.2011e-07\n",
      "Epoch 53/10000\n",
      "98/98 [==============================] - 3s 36ms/step - loss: 3.8025e-04 - mse: 3.7963e-04 - f_pen: 6.2419e-07 - val_loss: 2.2077e-04 - val_mse: 2.2015e-04 - val_f_pen: 6.2151e-07\n",
      "Epoch 54/10000\n",
      "98/98 [==============================] - 3s 36ms/step - loss: 3.1746e-04 - mse: 3.1683e-04 - f_pen: 6.2685e-07 - val_loss: 3.3063e-04 - val_mse: 3.3000e-04 - val_f_pen: 6.3612e-07\n",
      "Epoch 55/10000\n",
      "98/98 [==============================] - 4s 36ms/step - loss: 8.2638e-04 - mse: 8.2576e-04 - f_pen: 6.2463e-07 - val_loss: 2.0184e-04 - val_mse: 2.0121e-04 - val_f_pen: 6.2468e-07\n",
      "Epoch 56/10000\n",
      "98/98 [==============================] - 4s 38ms/step - loss: 1.7096e-04 - mse: 1.7033e-04 - f_pen: 6.2958e-07 - val_loss: 1.5440e-04 - val_mse: 1.5377e-04 - val_f_pen: 6.3097e-07\n",
      "Epoch 57/10000\n",
      "98/98 [==============================] - 4s 37ms/step - loss: 2.8419e-04 - mse: 2.8355e-04 - f_pen: 6.4141e-07 - val_loss: 1.7458e-04 - val_mse: 1.7394e-04 - val_f_pen: 6.4127e-07\n",
      "Epoch 58/10000\n",
      "98/98 [==============================] - 4s 36ms/step - loss: 3.6616e-04 - mse: 3.6552e-04 - f_pen: 6.4586e-07 - val_loss: 2.5851e-04 - val_mse: 2.5786e-04 - val_f_pen: 6.4688e-07\n",
      "Epoch 59/10000\n",
      "98/98 [==============================] - 4s 36ms/step - loss: 4.3777e-04 - mse: 4.3712e-04 - f_pen: 6.4940e-07 - val_loss: 1.7997e-04 - val_mse: 1.7932e-04 - val_f_pen: 6.4972e-07\n",
      "Epoch 60/10000\n",
      "98/98 [==============================] - 4s 36ms/step - loss: 2.7007e-04 - mse: 2.6942e-04 - f_pen: 6.5327e-07 - val_loss: 2.2076e-04 - val_mse: 2.2011e-04 - val_f_pen: 6.4883e-07\n",
      "Epoch 61/10000\n",
      "98/98 [==============================] - 4s 36ms/step - loss: 3.9800e-04 - mse: 3.9735e-04 - f_pen: 6.5435e-07 - val_loss: 1.6161e-04 - val_mse: 1.6095e-04 - val_f_pen: 6.5580e-07\n",
      "Epoch 62/10000\n",
      "98/98 [==============================] - 4s 36ms/step - loss: 3.2860e-04 - mse: 3.2794e-04 - f_pen: 6.6045e-07 - val_loss: 1.4970e-04 - val_mse: 1.4903e-04 - val_f_pen: 6.6652e-07\n",
      "Epoch 63/10000\n",
      "98/98 [==============================] - 4s 36ms/step - loss: 3.5688e-04 - mse: 3.5622e-04 - f_pen: 6.6710e-07 - val_loss: 2.4922e-04 - val_mse: 2.4855e-04 - val_f_pen: 6.7084e-07\n",
      "Epoch 64/10000\n",
      "98/98 [==============================] - 4s 36ms/step - loss: 2.9115e-04 - mse: 2.9048e-04 - f_pen: 6.6857e-07 - val_loss: 1.9508e-04 - val_mse: 1.9440e-04 - val_f_pen: 6.7727e-07\n",
      "Epoch 65/10000\n",
      "98/98 [==============================] - 4s 36ms/step - loss: 3.5786e-04 - mse: 3.5719e-04 - f_pen: 6.7613e-07 - val_loss: 2.0318e-04 - val_mse: 2.0250e-04 - val_f_pen: 6.7677e-07\n",
      "Epoch 66/10000\n",
      "98/98 [==============================] - 4s 37ms/step - loss: 3.9552e-04 - mse: 3.9484e-04 - f_pen: 6.7794e-07 - val_loss: 4.0894e-04 - val_mse: 4.0825e-04 - val_f_pen: 6.8743e-07\n",
      "Epoch 67/10000\n",
      "98/98 [==============================] - 4s 37ms/step - loss: 3.1340e-04 - mse: 3.1271e-04 - f_pen: 6.8466e-07 - val_loss: 1.5837e-04 - val_mse: 1.5768e-04 - val_f_pen: 6.8985e-07\n",
      "Epoch 68/10000\n",
      "98/98 [==============================] - 4s 36ms/step - loss: 4.4416e-04 - mse: 4.4347e-04 - f_pen: 6.9000e-07 - val_loss: 2.0434e-04 - val_mse: 2.0364e-04 - val_f_pen: 6.9490e-07\n",
      "Epoch 69/10000\n",
      "98/98 [==============================] - 4s 36ms/step - loss: 2.2087e-04 - mse: 2.2017e-04 - f_pen: 6.9673e-07 - val_loss: 1.5737e-04 - val_mse: 1.5668e-04 - val_f_pen: 6.8922e-07\n",
      "Epoch 70/10000\n",
      "98/98 [==============================] - 3s 35ms/step - loss: 3.5530e-04 - mse: 3.5459e-04 - f_pen: 7.0206e-07 - val_loss: 1.3361e-04 - val_mse: 1.3291e-04 - val_f_pen: 6.9888e-07\n",
      "Epoch 71/10000\n",
      "98/98 [==============================] - 4s 37ms/step - loss: 2.9725e-04 - mse: 2.9655e-04 - f_pen: 6.9968e-07 - val_loss: 1.2634e-04 - val_mse: 1.2564e-04 - val_f_pen: 7.0332e-07\n",
      "Epoch 72/10000\n",
      "98/98 [==============================] - 4s 37ms/step - loss: 3.5570e-04 - mse: 3.5499e-04 - f_pen: 7.0487e-07 - val_loss: 2.2760e-04 - val_mse: 2.2690e-04 - val_f_pen: 7.0202e-07\n",
      "Epoch 73/10000\n",
      "98/98 [==============================] - 4s 37ms/step - loss: 3.3161e-04 - mse: 3.3091e-04 - f_pen: 7.0333e-07 - val_loss: 1.2211e-04 - val_mse: 1.2140e-04 - val_f_pen: 7.0815e-07\n",
      "Epoch 74/10000\n",
      "98/98 [==============================] - 4s 37ms/step - loss: 3.3303e-04 - mse: 3.3232e-04 - f_pen: 7.0980e-07 - val_loss: 2.8757e-04 - val_mse: 2.8686e-04 - val_f_pen: 7.1559e-07\n",
      "Epoch 75/10000\n",
      "98/98 [==============================] - 4s 36ms/step - loss: 2.8053e-04 - mse: 2.7981e-04 - f_pen: 7.1926e-07 - val_loss: 1.8112e-04 - val_mse: 1.8039e-04 - val_f_pen: 7.2503e-07\n",
      "Epoch 76/10000\n",
      "98/98 [==============================] - 4s 36ms/step - loss: 6.9419e-04 - mse: 6.9345e-04 - f_pen: 7.4625e-07 - val_loss: 1.3573e-04 - val_mse: 1.3498e-04 - val_f_pen: 7.4736e-07\n",
      "Epoch 77/10000\n",
      "98/98 [==============================] - 4s 36ms/step - loss: 1.3339e-04 - mse: 1.3265e-04 - f_pen: 7.4071e-07 - val_loss: 1.2785e-04 - val_mse: 1.2711e-04 - val_f_pen: 7.3974e-07\n",
      "Epoch 78/10000\n",
      "98/98 [==============================] - 4s 37ms/step - loss: 2.6215e-04 - mse: 2.6142e-04 - f_pen: 7.3146e-07 - val_loss: 1.5869e-04 - val_mse: 1.5796e-04 - val_f_pen: 7.3032e-07\n",
      "Epoch 79/10000\n",
      "98/98 [==============================] - 4s 36ms/step - loss: 3.3245e-04 - mse: 3.3172e-04 - f_pen: 7.3508e-07 - val_loss: 1.5289e-04 - val_mse: 1.5215e-04 - val_f_pen: 7.3835e-07\n",
      "Epoch 80/10000\n",
      "98/98 [==============================] - 4s 37ms/step - loss: 3.8486e-04 - mse: 3.8411e-04 - f_pen: 7.4608e-07 - val_loss: 2.5808e-04 - val_mse: 2.5733e-04 - val_f_pen: 7.5378e-07\n",
      "Epoch 81/10000\n",
      "98/98 [==============================] - 4s 38ms/step - loss: 2.4307e-04 - mse: 2.4232e-04 - f_pen: 7.4918e-07 - val_loss: 2.2712e-04 - val_mse: 2.2638e-04 - val_f_pen: 7.4152e-07\n",
      "Epoch 82/10000\n",
      "98/98 [==============================] - 3s 36ms/step - loss: 2.8917e-04 - mse: 2.8842e-04 - f_pen: 7.5177e-07 - val_loss: 1.6325e-04 - val_mse: 1.6250e-04 - val_f_pen: 7.5199e-07\n",
      "Epoch 83/10000\n",
      "98/98 [==============================] - 4s 36ms/step - loss: 3.7541e-04 - mse: 3.7465e-04 - f_pen: 7.6066e-07 - val_loss: 1.5354e-04 - val_mse: 1.5277e-04 - val_f_pen: 7.6584e-07\n",
      "Epoch 84/10000\n",
      "98/98 [==============================] - 4s 38ms/step - loss: 2.8521e-04 - mse: 2.8445e-04 - f_pen: 7.6440e-07 - val_loss: 2.5686e-04 - val_mse: 2.5611e-04 - val_f_pen: 7.5353e-07\n",
      "Epoch 85/10000\n",
      "98/98 [==============================] - 4s 36ms/step - loss: 2.8036e-04 - mse: 2.7959e-04 - f_pen: 7.6800e-07 - val_loss: 1.2722e-04 - val_mse: 1.2645e-04 - val_f_pen: 7.7180e-07\n",
      "Epoch 86/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "98/98 [==============================] - 4s 36ms/step - loss: 3.1902e-04 - mse: 3.1826e-04 - f_pen: 7.6387e-07 - val_loss: 1.3529e-04 - val_mse: 1.3452e-04 - val_f_pen: 7.6566e-07\n",
      "Epoch 87/10000\n",
      "98/98 [==============================] - 4s 37ms/step - loss: 2.7933e-04 - mse: 2.7856e-04 - f_pen: 7.7467e-07 - val_loss: 1.1617e-04 - val_mse: 1.1539e-04 - val_f_pen: 7.7327e-07\n",
      "Epoch 88/10000\n",
      "98/98 [==============================] - 4s 37ms/step - loss: 3.1582e-04 - mse: 3.1504e-04 - f_pen: 7.7590e-07 - val_loss: 1.5700e-04 - val_mse: 1.5622e-04 - val_f_pen: 7.7552e-07\n",
      "Epoch 89/10000\n",
      "98/98 [==============================] - 4s 37ms/step - loss: 2.9597e-04 - mse: 2.9519e-04 - f_pen: 7.8644e-07 - val_loss: 1.7611e-04 - val_mse: 1.7533e-04 - val_f_pen: 7.8106e-07\n",
      "Epoch 90/10000\n",
      "98/98 [==============================] - 4s 37ms/step - loss: 3.0041e-04 - mse: 2.9962e-04 - f_pen: 7.8728e-07 - val_loss: 6.9544e-04 - val_mse: 6.9464e-04 - val_f_pen: 7.9961e-07\n",
      "Epoch 91/10000\n",
      "98/98 [==============================] - 4s 37ms/step - loss: 0.0010 - mse: 0.0010 - f_pen: 8.1063e-07 - val_loss: 1.4067e-04 - val_mse: 1.3984e-04 - val_f_pen: 8.2251e-07\n",
      "Epoch 92/10000\n",
      "98/98 [==============================] - 4s 37ms/step - loss: 1.0339e-04 - mse: 1.0258e-04 - f_pen: 8.1319e-07 - val_loss: 1.0413e-04 - val_mse: 1.0332e-04 - val_f_pen: 8.1208e-07\n",
      "Epoch 93/10000\n",
      "98/98 [==============================] - 4s 37ms/step - loss: 1.3898e-04 - mse: 1.3817e-04 - f_pen: 8.1138e-07 - val_loss: 1.1426e-04 - val_mse: 1.1344e-04 - val_f_pen: 8.1559e-07\n",
      "Epoch 94/10000\n",
      "98/98 [==============================] - 4s 37ms/step - loss: 2.5812e-04 - mse: 2.5730e-04 - f_pen: 8.1687e-07 - val_loss: 1.4297e-04 - val_mse: 1.4215e-04 - val_f_pen: 8.1676e-07\n",
      "Epoch 95/10000\n",
      "98/98 [==============================] - 4s 36ms/step - loss: 2.2076e-04 - mse: 2.1995e-04 - f_pen: 8.1248e-07 - val_loss: 7.4197e-04 - val_mse: 7.4117e-04 - val_f_pen: 7.9600e-07\n",
      "Epoch 96/10000\n",
      "98/98 [==============================] - 4s 36ms/step - loss: 0.0011 - mse: 0.0011 - f_pen: 8.6199e-07 - val_loss: 1.1734e-04 - val_mse: 1.1647e-04 - val_f_pen: 8.6542e-07\n",
      "Epoch 97/10000\n",
      "98/98 [==============================] - 4s 37ms/step - loss: 1.0812e-04 - mse: 1.0726e-04 - f_pen: 8.5769e-07 - val_loss: 1.0103e-04 - val_mse: 1.0018e-04 - val_f_pen: 8.5033e-07\n",
      "Epoch 98/10000\n",
      "98/98 [==============================] - 4s 38ms/step - loss: 1.8138e-04 - mse: 1.8054e-04 - f_pen: 8.4436e-07 - val_loss: 1.6902e-04 - val_mse: 1.6820e-04 - val_f_pen: 8.2727e-07\n",
      "Epoch 99/10000\n",
      "98/98 [==============================] - 4s 36ms/step - loss: 2.3436e-04 - mse: 2.3352e-04 - f_pen: 8.3167e-07 - val_loss: 1.0100e-04 - val_mse: 1.0016e-04 - val_f_pen: 8.3697e-07\n",
      "Epoch 100/10000\n",
      "98/98 [==============================] - 4s 36ms/step - loss: 1.8029e-04 - mse: 1.7945e-04 - f_pen: 8.3875e-07 - val_loss: 1.1793e-04 - val_mse: 1.1708e-04 - val_f_pen: 8.4741e-07\n",
      "Epoch 101/10000\n",
      "98/98 [==============================] - 4s 37ms/step - loss: 2.2391e-04 - mse: 2.2307e-04 - f_pen: 8.3799e-07 - val_loss: 1.3965e-04 - val_mse: 1.3881e-04 - val_f_pen: 8.4213e-07\n",
      "Epoch 102/10000\n",
      "98/98 [==============================] - 4s 36ms/step - loss: 3.0427e-04 - mse: 3.0343e-04 - f_pen: 8.3857e-07 - val_loss: 9.0482e-05 - val_mse: 8.9639e-05 - val_f_pen: 8.4318e-07\n",
      "Epoch 103/10000\n",
      "98/98 [==============================] - 4s 37ms/step - loss: 2.4048e-04 - mse: 2.3963e-04 - f_pen: 8.4645e-07 - val_loss: 2.2658e-04 - val_mse: 2.2574e-04 - val_f_pen: 8.3465e-07\n",
      "Epoch 104/10000\n",
      "98/98 [==============================] - 4s 37ms/step - loss: 3.5971e-04 - mse: 3.5884e-04 - f_pen: 8.7128e-07 - val_loss: 1.5612e-04 - val_mse: 1.5525e-04 - val_f_pen: 8.6675e-07\n",
      "Epoch 105/10000\n",
      "98/98 [==============================] - 4s 37ms/step - loss: 1.9196e-04 - mse: 1.9109e-04 - f_pen: 8.6042e-07 - val_loss: 9.1513e-05 - val_mse: 9.0648e-05 - val_f_pen: 8.6500e-07\n",
      "Epoch 106/10000\n",
      "98/98 [==============================] - 4s 37ms/step - loss: 2.1593e-04 - mse: 2.1507e-04 - f_pen: 8.6139e-07 - val_loss: 1.8408e-04 - val_mse: 1.8322e-04 - val_f_pen: 8.5717e-07\n",
      "Epoch 107/10000\n",
      "98/98 [==============================] - 4s 37ms/step - loss: 3.5771e-04 - mse: 3.5685e-04 - f_pen: 8.6330e-07 - val_loss: 9.7500e-05 - val_mse: 9.6623e-05 - val_f_pen: 8.7697e-07\n",
      "Epoch 108/10000\n",
      "98/98 [==============================] - 4s 36ms/step - loss: 2.5099e-04 - mse: 2.5012e-04 - f_pen: 8.7373e-07 - val_loss: 2.5936e-04 - val_mse: 2.5848e-04 - val_f_pen: 8.7646e-07\n",
      "Epoch 109/10000\n",
      "98/98 [==============================] - 4s 37ms/step - loss: 2.1016e-04 - mse: 2.0929e-04 - f_pen: 8.6824e-07 - val_loss: 1.1011e-04 - val_mse: 1.0924e-04 - val_f_pen: 8.7157e-07\n",
      "Epoch 110/10000\n",
      "98/98 [==============================] - 4s 38ms/step - loss: 2.8860e-04 - mse: 2.8772e-04 - f_pen: 8.8963e-07 - val_loss: 1.6372e-04 - val_mse: 1.6282e-04 - val_f_pen: 8.9792e-07\n",
      "Epoch 111/10000\n",
      "98/98 [==============================] - 4s 37ms/step - loss: 1.9756e-04 - mse: 1.9668e-04 - f_pen: 8.8529e-07 - val_loss: 9.5654e-05 - val_mse: 9.4763e-05 - val_f_pen: 8.9057e-07\n",
      "Epoch 112/10000\n",
      "98/98 [==============================] - 4s 37ms/step - loss: 2.5212e-04 - mse: 2.5123e-04 - f_pen: 8.8765e-07 - val_loss: 9.6697e-05 - val_mse: 9.5815e-05 - val_f_pen: 8.8201e-07\n",
      "Epoch 113/10000\n",
      "98/98 [==============================] - 4s 37ms/step - loss: 1.9016e-04 - mse: 1.8927e-04 - f_pen: 8.8809e-07 - val_loss: 1.5881e-04 - val_mse: 1.5792e-04 - val_f_pen: 8.9372e-07\n",
      "Epoch 114/10000\n",
      "98/98 [==============================] - 4s 36ms/step - loss: 3.0407e-04 - mse: 3.0318e-04 - f_pen: 8.8774e-07 - val_loss: 1.1735e-04 - val_mse: 1.1646e-04 - val_f_pen: 8.9139e-07\n",
      "Epoch 115/10000\n",
      "98/98 [==============================] - 4s 37ms/step - loss: 2.2870e-04 - mse: 2.2781e-04 - f_pen: 8.9809e-07 - val_loss: 1.0984e-04 - val_mse: 1.0895e-04 - val_f_pen: 8.9680e-07\n",
      "Epoch 116/10000\n",
      "98/98 [==============================] - 4s 36ms/step - loss: 2.4844e-04 - mse: 2.4754e-04 - f_pen: 9.0287e-07 - val_loss: 1.0058e-04 - val_mse: 9.9669e-05 - val_f_pen: 9.0712e-07\n",
      "Epoch 117/10000\n",
      "98/98 [==============================] - 4s 36ms/step - loss: 2.2931e-04 - mse: 2.2839e-04 - f_pen: 9.1162e-07 - val_loss: 1.8225e-04 - val_mse: 1.8135e-04 - val_f_pen: 8.9969e-07\n",
      "Epoch 118/10000\n",
      "98/98 [==============================] - 4s 37ms/step - loss: 3.9169e-04 - mse: 3.9077e-04 - f_pen: 9.2246e-07 - val_loss: 1.0780e-04 - val_mse: 1.0688e-04 - val_f_pen: 9.2427e-07\n",
      "Epoch 119/10000\n",
      "98/98 [==============================] - 4s 37ms/step - loss: 1.2376e-04 - mse: 1.2284e-04 - f_pen: 9.2328e-07 - val_loss: 9.5035e-05 - val_mse: 9.4114e-05 - val_f_pen: 9.2117e-07\n",
      "Epoch 120/10000\n",
      "98/98 [==============================] - 4s 37ms/step - loss: 3.0770e-04 - mse: 3.0678e-04 - f_pen: 9.2158e-07 - val_loss: 1.0033e-04 - val_mse: 9.9399e-05 - val_f_pen: 9.2750e-07\n",
      "Epoch 121/10000\n",
      "98/98 [==============================] - 4s 36ms/step - loss: 1.5110e-04 - mse: 1.5018e-04 - f_pen: 9.2158e-07 - val_loss: 1.0462e-04 - val_mse: 1.0370e-04 - val_f_pen: 9.2494e-07\n",
      "Epoch 122/10000\n",
      "98/98 [==============================] - 4s 37ms/step - loss: 2.2576e-04 - mse: 2.2483e-04 - f_pen: 9.3217e-07 - val_loss: 1.0012e-04 - val_mse: 9.9185e-05 - val_f_pen: 9.3402e-07\n",
      "\n",
      "Epoch 00122: ReduceLROnPlateau reducing learning rate to 0.0007000000332482159.\n",
      "Epoch 123/10000\n",
      "98/98 [==============================] - 4s 36ms/step - loss: 5.3174e-05 - mse: 5.2241e-05 - f_pen: 9.3235e-07 - val_loss: 5.5699e-05 - val_mse: 5.4772e-05 - val_f_pen: 9.2730e-07\n",
      "Epoch 124/10000\n",
      "98/98 [==============================] - 4s 36ms/step - loss: 4.5761e-05 - mse: 4.4832e-05 - f_pen: 9.2974e-07 - val_loss: 5.7178e-05 - val_mse: 5.6249e-05 - val_f_pen: 9.2880e-07\n",
      "Epoch 125/10000\n",
      "98/98 [==============================] - 4s 37ms/step - loss: 4.4842e-05 - mse: 4.3913e-05 - f_pen: 9.2939e-07 - val_loss: 6.1214e-05 - val_mse: 6.0283e-05 - val_f_pen: 9.3173e-07\n",
      "Epoch 126/10000\n",
      "98/98 [==============================] - 4s 36ms/step - loss: 4.6345e-05 - mse: 4.5415e-05 - f_pen: 9.3007e-07 - val_loss: 5.2863e-05 - val_mse: 5.1937e-05 - val_f_pen: 9.2681e-07\n",
      "Epoch 127/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "98/98 [==============================] - 4s 36ms/step - loss: 5.0640e-05 - mse: 4.9711e-05 - f_pen: 9.2948e-07 - val_loss: 4.6286e-05 - val_mse: 4.5355e-05 - val_f_pen: 9.3098e-07\n",
      "Epoch 128/10000\n",
      "98/98 [==============================] - 4s 37ms/step - loss: 8.1357e-05 - mse: 8.0426e-05 - f_pen: 9.3040e-07 - val_loss: 6.3887e-05 - val_mse: 6.2957e-05 - val_f_pen: 9.2976e-07\n",
      "Epoch 129/10000\n",
      "98/98 [==============================] - 4s 36ms/step - loss: 1.2963e-04 - mse: 1.2870e-04 - f_pen: 9.2828e-07 - val_loss: 1.5518e-04 - val_mse: 1.5425e-04 - val_f_pen: 9.3036e-07\n",
      "Epoch 130/10000\n",
      "98/98 [==============================] - 4s 37ms/step - loss: 1.4763e-04 - mse: 1.4671e-04 - f_pen: 9.2541e-07 - val_loss: 4.9366e-05 - val_mse: 4.8438e-05 - val_f_pen: 9.2827e-07\n",
      "Epoch 131/10000\n",
      "98/98 [==============================] - 4s 37ms/step - loss: 1.0162e-04 - mse: 1.0069e-04 - f_pen: 9.3030e-07 - val_loss: 7.9160e-05 - val_mse: 7.8220e-05 - val_f_pen: 9.3976e-07\n",
      "Epoch 132/10000\n",
      "98/98 [==============================] - 4s 37ms/step - loss: 1.3742e-04 - mse: 1.3649e-04 - f_pen: 9.3374e-07 - val_loss: 5.8403e-05 - val_mse: 5.7468e-05 - val_f_pen: 9.3485e-07\n",
      "Epoch 133/10000\n",
      "98/98 [==============================] - 4s 37ms/step - loss: 1.5140e-04 - mse: 1.5047e-04 - f_pen: 9.3690e-07 - val_loss: 1.1882e-04 - val_mse: 1.1788e-04 - val_f_pen: 9.3996e-07\n",
      "Epoch 134/10000\n",
      "98/98 [==============================] - 4s 37ms/step - loss: 1.8602e-04 - mse: 1.8507e-04 - f_pen: 9.4381e-07 - val_loss: 5.2417e-05 - val_mse: 5.1472e-05 - val_f_pen: 9.4510e-07\n",
      "Epoch 135/10000\n",
      "98/98 [==============================] - 4s 36ms/step - loss: 7.9728e-05 - mse: 7.8785e-05 - f_pen: 9.4326e-07 - val_loss: 8.5813e-05 - val_mse: 8.4878e-05 - val_f_pen: 9.3521e-07\n",
      "Epoch 136/10000\n",
      "98/98 [==============================] - 4s 36ms/step - loss: 1.5782e-04 - mse: 1.5688e-04 - f_pen: 9.4319e-07 - val_loss: 5.6809e-05 - val_mse: 5.5862e-05 - val_f_pen: 9.4788e-07\n",
      "Epoch 137/10000\n",
      "98/98 [==============================] - 4s 37ms/step - loss: 1.0980e-04 - mse: 1.0885e-04 - f_pen: 9.4962e-07 - val_loss: 8.3693e-05 - val_mse: 8.2744e-05 - val_f_pen: 9.4859e-07\n",
      "Epoch 138/10000\n",
      "98/98 [==============================] - 4s 37ms/step - loss: 1.5265e-04 - mse: 1.5170e-04 - f_pen: 9.5333e-07 - val_loss: 9.3949e-05 - val_mse: 9.2996e-05 - val_f_pen: 9.5284e-07\n",
      "Epoch 139/10000\n",
      "98/98 [==============================] - 4s 36ms/step - loss: 1.1209e-04 - mse: 1.1114e-04 - f_pen: 9.5357e-07 - val_loss: 7.3300e-05 - val_mse: 7.2350e-05 - val_f_pen: 9.5026e-07\n",
      "Epoch 140/10000\n",
      "98/98 [==============================] - 3s 35ms/step - loss: 1.6756e-04 - mse: 1.6661e-04 - f_pen: 9.5180e-07 - val_loss: 1.3616e-04 - val_mse: 1.3519e-04 - val_f_pen: 9.6479e-07\n",
      "Epoch 141/10000\n",
      "98/98 [==============================] - 3s 35ms/step - loss: 1.2153e-04 - mse: 1.2057e-04 - f_pen: 9.6422e-07 - val_loss: 6.0203e-05 - val_mse: 5.9238e-05 - val_f_pen: 9.6464e-07\n",
      "Epoch 142/10000\n",
      "98/98 [==============================] - 3s 36ms/step - loss: 8.1158e-05 - mse: 8.0195e-05 - f_pen: 9.6287e-07 - val_loss: 7.1060e-05 - val_mse: 7.0097e-05 - val_f_pen: 9.6289e-07\n",
      "Epoch 143/10000\n",
      "98/98 [==============================] - 4s 37ms/step - loss: 1.6473e-04 - mse: 1.6377e-04 - f_pen: 9.5846e-07 - val_loss: 6.9978e-05 - val_mse: 6.9011e-05 - val_f_pen: 9.6718e-07\n",
      "Epoch 144/10000\n",
      "98/98 [==============================] - 4s 37ms/step - loss: 7.0053e-05 - mse: 6.9093e-05 - f_pen: 9.6003e-07 - val_loss: 6.3751e-05 - val_mse: 6.2786e-05 - val_f_pen: 9.6540e-07\n",
      "Epoch 145/10000\n",
      "98/98 [==============================] - 4s 37ms/step - loss: 2.3185e-04 - mse: 2.3088e-04 - f_pen: 9.7188e-07 - val_loss: 5.7447e-05 - val_mse: 5.6476e-05 - val_f_pen: 9.7089e-07\n",
      "Epoch 146/10000\n",
      "98/98 [==============================] - 4s 37ms/step - loss: 9.5126e-05 - mse: 9.4153e-05 - f_pen: 9.7308e-07 - val_loss: 8.4979e-05 - val_mse: 8.4006e-05 - val_f_pen: 9.7276e-07\n",
      "Epoch 147/10000\n",
      "98/98 [==============================] - 4s 37ms/step - loss: 9.7103e-05 - mse: 9.6130e-05 - f_pen: 9.7275e-07 - val_loss: 6.8539e-05 - val_mse: 6.7556e-05 - val_f_pen: 9.8240e-07\n",
      "\n",
      "Epoch 00147: ReduceLROnPlateau reducing learning rate to 0.0004900000232737511.\n",
      "Epoch 148/10000\n",
      "98/98 [==============================] - 4s 37ms/step - loss: 3.5476e-05 - mse: 3.4504e-05 - f_pen: 9.7175e-07 - val_loss: 3.8324e-05 - val_mse: 3.7353e-05 - val_f_pen: 9.7046e-07\n",
      "Epoch 149/10000\n",
      "98/98 [==============================] - 4s 37ms/step - loss: 2.8620e-05 - mse: 2.7649e-05 - f_pen: 9.7113e-07 - val_loss: 3.4179e-05 - val_mse: 3.3209e-05 - val_f_pen: 9.7071e-07\n",
      "Epoch 150/10000\n",
      "98/98 [==============================] - 4s 36ms/step - loss: 2.8594e-05 - mse: 2.7624e-05 - f_pen: 9.6965e-07 - val_loss: 3.6845e-05 - val_mse: 3.5873e-05 - val_f_pen: 9.7235e-07\n",
      "Epoch 151/10000\n",
      "98/98 [==============================] - 4s 36ms/step - loss: 3.0069e-05 - mse: 2.9100e-05 - f_pen: 9.6912e-07 - val_loss: 3.4688e-05 - val_mse: 3.3719e-05 - val_f_pen: 9.6938e-07\n",
      "Epoch 152/10000\n",
      "98/98 [==============================] - 4s 36ms/step - loss: 3.0998e-05 - mse: 3.0029e-05 - f_pen: 9.6929e-07 - val_loss: 3.6005e-05 - val_mse: 3.5032e-05 - val_f_pen: 9.7305e-07\n",
      "Epoch 153/10000\n",
      "98/98 [==============================] - 4s 36ms/step - loss: 3.3590e-05 - mse: 3.2621e-05 - f_pen: 9.6952e-07 - val_loss: 3.6009e-05 - val_mse: 3.5038e-05 - val_f_pen: 9.7069e-07\n",
      "Epoch 154/10000\n",
      "98/98 [==============================] - 3s 36ms/step - loss: 6.3094e-05 - mse: 6.2125e-05 - f_pen: 9.6834e-07 - val_loss: 3.7636e-05 - val_mse: 3.6664e-05 - val_f_pen: 9.7242e-07\n",
      "Epoch 155/10000\n",
      "98/98 [==============================] - 4s 37ms/step - loss: 8.1503e-05 - mse: 8.0533e-05 - f_pen: 9.7019e-07 - val_loss: 3.9964e-05 - val_mse: 3.8992e-05 - val_f_pen: 9.7213e-07\n",
      "Epoch 156/10000\n",
      "98/98 [==============================] - 4s 37ms/step - loss: 6.9555e-05 - mse: 6.8583e-05 - f_pen: 9.7166e-07 - val_loss: 4.5831e-05 - val_mse: 4.4856e-05 - val_f_pen: 9.7462e-07\n",
      "Epoch 157/10000\n",
      "98/98 [==============================] - 4s 36ms/step - loss: 6.6420e-05 - mse: 6.5449e-05 - f_pen: 9.7087e-07 - val_loss: 4.9574e-05 - val_mse: 4.8603e-05 - val_f_pen: 9.7089e-07\n",
      "Epoch 158/10000\n",
      "98/98 [==============================] - 4s 36ms/step - loss: 9.0155e-05 - mse: 8.9183e-05 - f_pen: 9.7209e-07 - val_loss: 8.8161e-05 - val_mse: 8.7195e-05 - val_f_pen: 9.6616e-07\n",
      "Epoch 159/10000\n",
      "98/98 [==============================] - 4s 37ms/step - loss: 6.1925e-05 - mse: 6.0951e-05 - f_pen: 9.7395e-07 - val_loss: 3.7690e-05 - val_mse: 3.6715e-05 - val_f_pen: 9.7558e-07\n",
      "Epoch 160/10000\n",
      "98/98 [==============================] - 4s 37ms/step - loss: 6.5712e-05 - mse: 6.4736e-05 - f_pen: 9.7581e-07 - val_loss: 5.1332e-05 - val_mse: 5.0351e-05 - val_f_pen: 9.8086e-07\n",
      "Epoch 161/10000\n",
      "98/98 [==============================] - 4s 36ms/step - loss: 7.2102e-05 - mse: 7.1126e-05 - f_pen: 9.7613e-07 - val_loss: 4.0225e-05 - val_mse: 3.9242e-05 - val_f_pen: 9.8326e-07\n",
      "Epoch 162/10000\n",
      "98/98 [==============================] - 4s 38ms/step - loss: 7.3729e-05 - mse: 7.2752e-05 - f_pen: 9.7679e-07 - val_loss: 4.7619e-05 - val_mse: 4.6644e-05 - val_f_pen: 9.7537e-07\n",
      "Epoch 163/10000\n",
      "98/98 [==============================] - 4s 36ms/step - loss: 9.5839e-05 - mse: 9.4863e-05 - f_pen: 9.7593e-07 - val_loss: 4.0056e-05 - val_mse: 3.9075e-05 - val_f_pen: 9.8151e-07\n",
      "Epoch 164/10000\n",
      "98/98 [==============================] - 3s 35ms/step - loss: 6.5278e-05 - mse: 6.4300e-05 - f_pen: 9.7807e-07 - val_loss: 6.0026e-05 - val_mse: 5.9047e-05 - val_f_pen: 9.7937e-07\n",
      "Epoch 165/10000\n",
      "98/98 [==============================] - 4s 36ms/step - loss: 7.2375e-05 - mse: 7.1397e-05 - f_pen: 9.7862e-07 - val_loss: 5.0811e-05 - val_mse: 4.9835e-05 - val_f_pen: 9.7584e-07\n",
      "Epoch 166/10000\n",
      "98/98 [==============================] - 4s 37ms/step - loss: 9.1581e-05 - mse: 9.0598e-05 - f_pen: 9.8306e-07 - val_loss: 3.9743e-05 - val_mse: 3.8755e-05 - val_f_pen: 9.8735e-07\n",
      "Epoch 167/10000\n",
      "98/98 [==============================] - 4s 37ms/step - loss: 6.0229e-05 - mse: 5.9247e-05 - f_pen: 9.8185e-07 - val_loss: 4.9596e-05 - val_mse: 4.8611e-05 - val_f_pen: 9.8564e-07\n",
      "Epoch 168/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "98/98 [==============================] - 4s 37ms/step - loss: 6.8128e-05 - mse: 6.7147e-05 - f_pen: 9.8112e-07 - val_loss: 4.7337e-05 - val_mse: 4.6357e-05 - val_f_pen: 9.8048e-07\n",
      "Epoch 169/10000\n",
      "98/98 [==============================] - 4s 37ms/step - loss: 9.7960e-05 - mse: 9.6970e-05 - f_pen: 9.9048e-07 - val_loss: 4.1499e-05 - val_mse: 4.0510e-05 - val_f_pen: 9.8959e-07\n",
      "\n",
      "Epoch 00169: ReduceLROnPlateau reducing learning rate to 0.00034300000406801696.\n",
      "Epoch 170/10000\n",
      "98/98 [==============================] - 4s 37ms/step - loss: 2.2881e-05 - mse: 2.1894e-05 - f_pen: 9.8652e-07 - val_loss: 2.6767e-05 - val_mse: 2.5780e-05 - val_f_pen: 9.8782e-07\n",
      "Epoch 171/10000\n",
      "98/98 [==============================] - 4s 36ms/step - loss: 2.0315e-05 - mse: 1.9330e-05 - f_pen: 9.8555e-07 - val_loss: 2.6634e-05 - val_mse: 2.5650e-05 - val_f_pen: 9.8425e-07\n",
      "Epoch 172/10000\n",
      "98/98 [==============================] - 4s 37ms/step - loss: 2.0754e-05 - mse: 1.9769e-05 - f_pen: 9.8472e-07 - val_loss: 2.7777e-05 - val_mse: 2.6791e-05 - val_f_pen: 9.8548e-07\n",
      "Epoch 173/10000\n",
      "98/98 [==============================] - 4s 36ms/step - loss: 1.9937e-05 - mse: 1.8953e-05 - f_pen: 9.8453e-07 - val_loss: 2.6486e-05 - val_mse: 2.5504e-05 - val_f_pen: 9.8277e-07\n",
      "Epoch 174/10000\n",
      "98/98 [==============================] - 4s 37ms/step - loss: 2.0917e-05 - mse: 1.9933e-05 - f_pen: 9.8429e-07 - val_loss: 2.8374e-05 - val_mse: 2.7389e-05 - val_f_pen: 9.8556e-07\n",
      "Epoch 175/10000\n",
      "98/98 [==============================] - 4s 36ms/step - loss: 2.3047e-05 - mse: 2.2063e-05 - f_pen: 9.8364e-07 - val_loss: 2.7241e-05 - val_mse: 2.6255e-05 - val_f_pen: 9.8610e-07\n",
      "Epoch 176/10000\n",
      "98/98 [==============================] - 4s 37ms/step - loss: 2.7416e-05 - mse: 2.6432e-05 - f_pen: 9.8322e-07 - val_loss: 3.9309e-05 - val_mse: 3.8328e-05 - val_f_pen: 9.8150e-07\n",
      "Epoch 177/10000\n",
      "98/98 [==============================] - 4s 37ms/step - loss: 6.9050e-05 - mse: 6.8070e-05 - f_pen: 9.8026e-07 - val_loss: 2.8683e-05 - val_mse: 2.7701e-05 - val_f_pen: 9.8160e-07\n",
      "Epoch 178/10000\n",
      "98/98 [==============================] - 4s 37ms/step - loss: 2.4393e-05 - mse: 2.3411e-05 - f_pen: 9.8222e-07 - val_loss: 2.6674e-05 - val_mse: 2.5690e-05 - val_f_pen: 9.8413e-07\n",
      "Epoch 179/10000\n",
      "98/98 [==============================] - 4s 37ms/step - loss: 2.9016e-05 - mse: 2.8034e-05 - f_pen: 9.8241e-07 - val_loss: 3.4160e-05 - val_mse: 3.3177e-05 - val_f_pen: 9.8242e-07\n",
      "Epoch 180/10000\n",
      "98/98 [==============================] - 4s 37ms/step - loss: 4.4654e-05 - mse: 4.3670e-05 - f_pen: 9.8332e-07 - val_loss: 3.6815e-05 - val_mse: 3.5830e-05 - val_f_pen: 9.8503e-07\n",
      "Epoch 181/10000\n",
      "98/98 [==============================] - 4s 37ms/step - loss: 4.0449e-05 - mse: 3.9465e-05 - f_pen: 9.8387e-07 - val_loss: 3.3062e-05 - val_mse: 3.2077e-05 - val_f_pen: 9.8421e-07\n",
      "Epoch 182/10000\n",
      "98/98 [==============================] - 4s 37ms/step - loss: 4.3962e-05 - mse: 4.2978e-05 - f_pen: 9.8387e-07 - val_loss: 3.2438e-05 - val_mse: 3.1453e-05 - val_f_pen: 9.8489e-07\n",
      "Epoch 183/10000\n",
      "98/98 [==============================] - 4s 37ms/step - loss: 4.1737e-05 - mse: 4.0753e-05 - f_pen: 9.8427e-07 - val_loss: 3.6874e-05 - val_mse: 3.5892e-05 - val_f_pen: 9.8160e-07\n",
      "Epoch 184/10000\n",
      "98/98 [==============================] - 4s 37ms/step - loss: 4.2068e-05 - mse: 4.1085e-05 - f_pen: 9.8280e-07 - val_loss: 3.1018e-05 - val_mse: 3.0035e-05 - val_f_pen: 9.8330e-07\n",
      "Epoch 185/10000\n",
      "98/98 [==============================] - 4s 36ms/step - loss: 7.0783e-05 - mse: 6.9798e-05 - f_pen: 9.8444e-07 - val_loss: 2.6499e-05 - val_mse: 2.5512e-05 - val_f_pen: 9.8723e-07\n",
      "Epoch 186/10000\n",
      "98/98 [==============================] - 4s 37ms/step - loss: 2.3312e-05 - mse: 2.2327e-05 - f_pen: 9.8541e-07 - val_loss: 2.7279e-05 - val_mse: 2.6294e-05 - val_f_pen: 9.8564e-07\n",
      "Epoch 187/10000\n",
      "98/98 [==============================] - 4s 37ms/step - loss: 8.4488e-05 - mse: 8.3496e-05 - f_pen: 9.9178e-07 - val_loss: 2.6162e-05 - val_mse: 2.5169e-05 - val_f_pen: 9.9284e-07\n",
      "Epoch 188/10000\n",
      "98/98 [==============================] - 4s 37ms/step - loss: 2.2468e-05 - mse: 2.1477e-05 - f_pen: 9.9171e-07 - val_loss: 2.4087e-05 - val_mse: 2.3097e-05 - val_f_pen: 9.8997e-07\n",
      "Epoch 189/10000\n",
      "98/98 [==============================] - 4s 37ms/step - loss: 2.9614e-05 - mse: 2.8624e-05 - f_pen: 9.8952e-07 - val_loss: 2.3622e-05 - val_mse: 2.2632e-05 - val_f_pen: 9.8950e-07\n",
      "Epoch 190/10000\n",
      "98/98 [==============================] - 4s 37ms/step - loss: 3.5905e-05 - mse: 3.4917e-05 - f_pen: 9.8808e-07 - val_loss: 4.7216e-05 - val_mse: 4.6229e-05 - val_f_pen: 9.8730e-07\n",
      "Epoch 191/10000\n",
      "98/98 [==============================] - 4s 37ms/step - loss: 8.7366e-05 - mse: 8.6382e-05 - f_pen: 9.8439e-07 - val_loss: 2.3535e-05 - val_mse: 2.2549e-05 - val_f_pen: 9.8592e-07\n",
      "Epoch 192/10000\n",
      "98/98 [==============================] - 4s 37ms/step - loss: 2.2274e-05 - mse: 2.1287e-05 - f_pen: 9.8724e-07 - val_loss: 2.6835e-05 - val_mse: 2.5847e-05 - val_f_pen: 9.8858e-07\n",
      "Epoch 193/10000\n",
      "98/98 [==============================] - 4s 36ms/step - loss: 3.4228e-05 - mse: 3.3241e-05 - f_pen: 9.8750e-07 - val_loss: 4.8738e-05 - val_mse: 4.7748e-05 - val_f_pen: 9.8926e-07\n",
      "Epoch 194/10000\n",
      "98/98 [==============================] - 4s 36ms/step - loss: 8.7351e-05 - mse: 8.6365e-05 - f_pen: 9.8643e-07 - val_loss: 2.1624e-05 - val_mse: 2.0634e-05 - val_f_pen: 9.8984e-07\n",
      "Epoch 195/10000\n",
      "98/98 [==============================] - 4s 36ms/step - loss: 1.9199e-05 - mse: 1.8211e-05 - f_pen: 9.8819e-07 - val_loss: 2.7363e-05 - val_mse: 2.6374e-05 - val_f_pen: 9.8896e-07\n",
      "Epoch 196/10000\n",
      "98/98 [==============================] - 4s 37ms/step - loss: 2.3054e-05 - mse: 2.2066e-05 - f_pen: 9.8805e-07 - val_loss: 2.6464e-05 - val_mse: 2.5477e-05 - val_f_pen: 9.8667e-07\n",
      "Epoch 197/10000\n",
      "98/98 [==============================] - 4s 36ms/step - loss: 3.5210e-05 - mse: 3.4223e-05 - f_pen: 9.8776e-07 - val_loss: 3.5306e-05 - val_mse: 3.4318e-05 - val_f_pen: 9.8808e-07\n",
      "Epoch 198/10000\n",
      "98/98 [==============================] - 4s 36ms/step - loss: 6.5286e-05 - mse: 6.4299e-05 - f_pen: 9.8783e-07 - val_loss: 2.2984e-05 - val_mse: 2.1993e-05 - val_f_pen: 9.9148e-07\n",
      "Epoch 199/10000\n",
      "98/98 [==============================] - 4s 36ms/step - loss: 2.2661e-05 - mse: 2.1672e-05 - f_pen: 9.8902e-07 - val_loss: 2.3454e-05 - val_mse: 2.2463e-05 - val_f_pen: 9.9134e-07\n",
      "Epoch 200/10000\n",
      "98/98 [==============================] - 4s 37ms/step - loss: 5.1483e-05 - mse: 5.0494e-05 - f_pen: 9.8889e-07 - val_loss: 2.5296e-05 - val_mse: 2.4306e-05 - val_f_pen: 9.9045e-07\n",
      "Epoch 201/10000\n",
      "98/98 [==============================] - 4s 37ms/step - loss: 3.3727e-05 - mse: 3.2739e-05 - f_pen: 9.8829e-07 - val_loss: 3.8373e-05 - val_mse: 3.7383e-05 - val_f_pen: 9.8991e-07\n",
      "Epoch 202/10000\n",
      "98/98 [==============================] - 4s 36ms/step - loss: 3.4961e-05 - mse: 3.3972e-05 - f_pen: 9.8869e-07 - val_loss: 2.9502e-05 - val_mse: 2.8513e-05 - val_f_pen: 9.8950e-07\n",
      "Epoch 203/10000\n",
      "98/98 [==============================] - 4s 37ms/step - loss: 4.0934e-05 - mse: 3.9944e-05 - f_pen: 9.8992e-07 - val_loss: 3.2487e-05 - val_mse: 3.1496e-05 - val_f_pen: 9.9095e-07\n",
      "Epoch 204/10000\n",
      "98/98 [==============================] - 4s 37ms/step - loss: 4.1150e-05 - mse: 4.0160e-05 - f_pen: 9.8965e-07 - val_loss: 2.8897e-05 - val_mse: 2.7906e-05 - val_f_pen: 9.9103e-07\n",
      "Epoch 205/10000\n",
      "98/98 [==============================] - 4s 37ms/step - loss: 4.1972e-05 - mse: 4.0980e-05 - f_pen: 9.9112e-07 - val_loss: 2.9874e-05 - val_mse: 2.8883e-05 - val_f_pen: 9.9167e-07\n",
      "Epoch 206/10000\n",
      "98/98 [==============================] - 4s 36ms/step - loss: 4.5928e-05 - mse: 4.4937e-05 - f_pen: 9.9078e-07 - val_loss: 3.7151e-05 - val_mse: 3.6163e-05 - val_f_pen: 9.8808e-07\n",
      "Epoch 207/10000\n",
      "98/98 [==============================] - 4s 36ms/step - loss: 4.2487e-05 - mse: 4.1497e-05 - f_pen: 9.8960e-07 - val_loss: 2.7708e-05 - val_mse: 2.6715e-05 - val_f_pen: 9.9305e-07\n",
      "Epoch 208/10000\n",
      "98/98 [==============================] - 4s 37ms/step - loss: 2.7436e-05 - mse: 2.6445e-05 - f_pen: 9.9124e-07 - val_loss: 3.1733e-05 - val_mse: 3.0737e-05 - val_f_pen: 9.9551e-07\n",
      "Epoch 209/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "98/98 [==============================] - 3s 35ms/step - loss: 5.3607e-05 - mse: 5.2615e-05 - f_pen: 9.9213e-07 - val_loss: 2.5110e-05 - val_mse: 2.4116e-05 - val_f_pen: 9.9395e-07\n",
      "Epoch 210/10000\n",
      "98/98 [==============================] - 4s 36ms/step - loss: 2.8285e-05 - mse: 2.7294e-05 - f_pen: 9.9189e-07 - val_loss: 4.5347e-05 - val_mse: 4.4353e-05 - val_f_pen: 9.9415e-07\n",
      "Epoch 211/10000\n",
      "98/98 [==============================] - 4s 37ms/step - loss: 3.8487e-05 - mse: 3.7496e-05 - f_pen: 9.9024e-07 - val_loss: 2.2601e-05 - val_mse: 2.1607e-05 - val_f_pen: 9.9374e-07\n",
      "Epoch 212/10000\n",
      "98/98 [==============================] - 4s 37ms/step - loss: 4.9589e-05 - mse: 4.8598e-05 - f_pen: 9.9048e-07 - val_loss: 2.7420e-05 - val_mse: 2.6425e-05 - val_f_pen: 9.9474e-07\n",
      "Epoch 213/10000\n",
      "98/98 [==============================] - 4s 36ms/step - loss: 3.6630e-05 - mse: 3.5639e-05 - f_pen: 9.9089e-07 - val_loss: 2.3609e-05 - val_mse: 2.2614e-05 - val_f_pen: 9.9421e-07\n",
      "Epoch 214/10000\n",
      "98/98 [==============================] - 4s 36ms/step - loss: 4.8795e-05 - mse: 4.7803e-05 - f_pen: 9.9226e-07 - val_loss: 3.7732e-05 - val_mse: 3.6736e-05 - val_f_pen: 9.9542e-07\n",
      "\n",
      "Epoch 00214: ReduceLROnPlateau reducing learning rate to 0.00024009999469853935.\n",
      "Epoch 215/10000\n",
      "98/98 [==============================] - 4s 38ms/step - loss: 1.5143e-05 - mse: 1.4150e-05 - f_pen: 9.9297e-07 - val_loss: 1.8384e-05 - val_mse: 1.7392e-05 - val_f_pen: 9.9236e-07\n",
      "Epoch 216/10000\n",
      "98/98 [==============================] - 4s 37ms/step - loss: 1.2391e-05 - mse: 1.1399e-05 - f_pen: 9.9211e-07 - val_loss: 1.7016e-05 - val_mse: 1.6023e-05 - val_f_pen: 9.9382e-07\n",
      "Epoch 217/10000\n",
      "98/98 [==============================] - 3s 35ms/step - loss: 1.2825e-05 - mse: 1.1833e-05 - f_pen: 9.9209e-07 - val_loss: 1.8023e-05 - val_mse: 1.7029e-05 - val_f_pen: 9.9400e-07\n",
      "Epoch 218/10000\n",
      "98/98 [==============================] - 4s 37ms/step - loss: 1.3142e-05 - mse: 1.2150e-05 - f_pen: 9.9184e-07 - val_loss: 1.8848e-05 - val_mse: 1.7857e-05 - val_f_pen: 9.9072e-07\n",
      "Epoch 219/10000\n",
      "98/98 [==============================] - 4s 36ms/step - loss: 1.3351e-05 - mse: 1.2359e-05 - f_pen: 9.9168e-07 - val_loss: 1.7279e-05 - val_mse: 1.6286e-05 - val_f_pen: 9.9257e-07\n",
      "Epoch 220/10000\n",
      "98/98 [==============================] - 4s 36ms/step - loss: 1.4167e-05 - mse: 1.3175e-05 - f_pen: 9.9157e-07 - val_loss: 1.8705e-05 - val_mse: 1.7711e-05 - val_f_pen: 9.9413e-07\n",
      "Epoch 221/10000\n",
      "98/98 [==============================] - 4s 36ms/step - loss: 1.5101e-05 - mse: 1.4110e-05 - f_pen: 9.9124e-07 - val_loss: 1.8411e-05 - val_mse: 1.7421e-05 - val_f_pen: 9.9053e-07\n",
      "Epoch 222/10000\n",
      "98/98 [==============================] - 4s 36ms/step - loss: 1.8806e-05 - mse: 1.7815e-05 - f_pen: 9.9088e-07 - val_loss: 1.9634e-05 - val_mse: 1.8645e-05 - val_f_pen: 9.8982e-07\n",
      "Epoch 223/10000\n",
      "98/98 [==============================] - 4s 37ms/step - loss: 2.5764e-05 - mse: 2.4775e-05 - f_pen: 9.8977e-07 - val_loss: 1.8161e-05 - val_mse: 1.7168e-05 - val_f_pen: 9.9303e-07\n",
      "Epoch 224/10000\n",
      "98/98 [==============================] - 3s 36ms/step - loss: 2.0638e-05 - mse: 1.9649e-05 - f_pen: 9.8940e-07 - val_loss: 2.1817e-05 - val_mse: 2.0826e-05 - val_f_pen: 9.9099e-07\n",
      "Epoch 225/10000\n",
      "98/98 [==============================] - 4s 36ms/step - loss: 2.6493e-05 - mse: 2.5503e-05 - f_pen: 9.8986e-07 - val_loss: 2.6511e-05 - val_mse: 2.5517e-05 - val_f_pen: 9.9456e-07\n",
      "Epoch 226/10000\n",
      "98/98 [==============================] - 4s 37ms/step - loss: 3.2290e-05 - mse: 3.1300e-05 - f_pen: 9.9037e-07 - val_loss: 2.0430e-05 - val_mse: 1.9439e-05 - val_f_pen: 9.9054e-07\n",
      "Epoch 227/10000\n",
      "98/98 [==============================] - 4s 36ms/step - loss: 1.6768e-05 - mse: 1.5778e-05 - f_pen: 9.9030e-07 - val_loss: 1.8884e-05 - val_mse: 1.7895e-05 - val_f_pen: 9.8962e-07\n",
      "Epoch 228/10000\n",
      "98/98 [==============================] - 4s 36ms/step - loss: 2.2016e-05 - mse: 2.1026e-05 - f_pen: 9.9058e-07 - val_loss: 2.2046e-05 - val_mse: 2.1056e-05 - val_f_pen: 9.8986e-07\n",
      "Epoch 229/10000\n",
      "98/98 [==============================] - 4s 37ms/step - loss: 2.7762e-05 - mse: 2.6771e-05 - f_pen: 9.9059e-07 - val_loss: 2.5796e-05 - val_mse: 2.4810e-05 - val_f_pen: 9.8548e-07\n",
      "Epoch 230/10000\n",
      "98/98 [==============================] - 4s 36ms/step - loss: 3.2289e-05 - mse: 3.1298e-05 - f_pen: 9.9032e-07 - val_loss: 1.9011e-05 - val_mse: 1.8016e-05 - val_f_pen: 9.9527e-07\n",
      "Epoch 231/10000\n",
      "98/98 [==============================] - 4s 37ms/step - loss: 1.6647e-05 - mse: 1.5657e-05 - f_pen: 9.9013e-07 - val_loss: 1.8216e-05 - val_mse: 1.7224e-05 - val_f_pen: 9.9216e-07\n",
      "Epoch 232/10000\n",
      "98/98 [==============================] - 4s 37ms/step - loss: 1.8606e-05 - mse: 1.7616e-05 - f_pen: 9.8995e-07 - val_loss: 2.2696e-05 - val_mse: 2.1704e-05 - val_f_pen: 9.9176e-07\n",
      "Epoch 233/10000\n",
      "98/98 [==============================] - 4s 37ms/step - loss: 2.6134e-05 - mse: 2.5143e-05 - f_pen: 9.9101e-07 - val_loss: 1.6812e-05 - val_mse: 1.5822e-05 - val_f_pen: 9.9005e-07\n",
      "Epoch 234/10000\n",
      "98/98 [==============================] - 4s 37ms/step - loss: 2.6716e-05 - mse: 2.5726e-05 - f_pen: 9.8978e-07 - val_loss: 2.6627e-05 - val_mse: 2.5635e-05 - val_f_pen: 9.9155e-07\n",
      "Epoch 235/10000\n",
      "98/98 [==============================] - 4s 36ms/step - loss: 2.5974e-05 - mse: 2.4985e-05 - f_pen: 9.8906e-07 - val_loss: 3.3155e-05 - val_mse: 3.2161e-05 - val_f_pen: 9.9395e-07\n",
      "Epoch 236/10000\n",
      "98/98 [==============================] - 4s 36ms/step - loss: 3.9892e-05 - mse: 3.8902e-05 - f_pen: 9.8992e-07 - val_loss: 2.1538e-05 - val_mse: 2.0546e-05 - val_f_pen: 9.9264e-07\n",
      "Epoch 237/10000\n",
      "98/98 [==============================] - 4s 36ms/step - loss: 1.8094e-05 - mse: 1.7104e-05 - f_pen: 9.9011e-07 - val_loss: 1.6621e-05 - val_mse: 1.5631e-05 - val_f_pen: 9.8972e-07\n",
      "Epoch 238/10000\n",
      "98/98 [==============================] - 4s 36ms/step - loss: 1.3474e-05 - mse: 1.2484e-05 - f_pen: 9.9005e-07 - val_loss: 2.0109e-05 - val_mse: 1.9118e-05 - val_f_pen: 9.9085e-07\n",
      "Epoch 239/10000\n",
      "98/98 [==============================] - 4s 36ms/step - loss: 2.7030e-05 - mse: 2.6041e-05 - f_pen: 9.8960e-07 - val_loss: 1.7634e-05 - val_mse: 1.6645e-05 - val_f_pen: 9.8948e-07\n",
      "Epoch 240/10000\n",
      "98/98 [==============================] - 3s 36ms/step - loss: 2.1791e-05 - mse: 2.0800e-05 - f_pen: 9.9015e-07 - val_loss: 2.2717e-05 - val_mse: 2.1728e-05 - val_f_pen: 9.8954e-07\n",
      "Epoch 241/10000\n",
      "98/98 [==============================] - 4s 38ms/step - loss: 2.2065e-05 - mse: 2.1075e-05 - f_pen: 9.8941e-07 - val_loss: 1.6536e-05 - val_mse: 1.5546e-05 - val_f_pen: 9.9005e-07\n",
      "Epoch 242/10000\n",
      "98/98 [==============================] - 4s 36ms/step - loss: 2.1260e-05 - mse: 2.0271e-05 - f_pen: 9.8895e-07 - val_loss: 2.3935e-05 - val_mse: 2.2946e-05 - val_f_pen: 9.8893e-07\n",
      "Epoch 243/10000\n",
      "98/98 [==============================] - 4s 37ms/step - loss: 2.4497e-05 - mse: 2.3508e-05 - f_pen: 9.8885e-07 - val_loss: 1.6478e-05 - val_mse: 1.5488e-05 - val_f_pen: 9.9020e-07\n",
      "Epoch 244/10000\n",
      "98/98 [==============================] - 4s 37ms/step - loss: 2.4189e-05 - mse: 2.3200e-05 - f_pen: 9.8927e-07 - val_loss: 2.1485e-05 - val_mse: 2.0493e-05 - val_f_pen: 9.9117e-07\n",
      "Epoch 245/10000\n",
      "98/98 [==============================] - 4s 37ms/step - loss: 2.6789e-05 - mse: 2.5801e-05 - f_pen: 9.8828e-07 - val_loss: 1.5992e-05 - val_mse: 1.5003e-05 - val_f_pen: 9.8954e-07\n",
      "Epoch 246/10000\n",
      "98/98 [==============================] - 4s 37ms/step - loss: 1.5801e-05 - mse: 1.4813e-05 - f_pen: 9.8807e-07 - val_loss: 1.7571e-05 - val_mse: 1.6583e-05 - val_f_pen: 9.8758e-07\n",
      "Epoch 247/10000\n",
      "98/98 [==============================] - 4s 37ms/step - loss: 2.8731e-05 - mse: 2.7744e-05 - f_pen: 9.8721e-07 - val_loss: 1.6442e-05 - val_mse: 1.5454e-05 - val_f_pen: 9.8865e-07\n",
      "Epoch 248/10000\n",
      "98/98 [==============================] - 4s 36ms/step - loss: 1.7555e-05 - mse: 1.6567e-05 - f_pen: 9.8752e-07 - val_loss: 1.8039e-05 - val_mse: 1.7051e-05 - val_f_pen: 9.8813e-07\n",
      "Epoch 249/10000\n",
      "98/98 [==============================] - 4s 36ms/step - loss: 2.7367e-05 - mse: 2.6378e-05 - f_pen: 9.8952e-07 - val_loss: 2.3315e-05 - val_mse: 2.2326e-05 - val_f_pen: 9.8873e-07\n",
      "Epoch 250/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "98/98 [==============================] - 4s 37ms/step - loss: 1.8680e-05 - mse: 1.7692e-05 - f_pen: 9.8834e-07 - val_loss: 1.9684e-05 - val_mse: 1.8697e-05 - val_f_pen: 9.8674e-07\n",
      "Epoch 251/10000\n",
      "98/98 [==============================] - 4s 36ms/step - loss: 2.5193e-05 - mse: 2.4204e-05 - f_pen: 9.8851e-07 - val_loss: 1.7202e-05 - val_mse: 1.6211e-05 - val_f_pen: 9.9050e-07\n",
      "Epoch 252/10000\n",
      "98/98 [==============================] - 4s 36ms/step - loss: 2.5047e-05 - mse: 2.4058e-05 - f_pen: 9.8841e-07 - val_loss: 2.5669e-05 - val_mse: 2.4682e-05 - val_f_pen: 9.8620e-07\n",
      "Epoch 253/10000\n",
      "98/98 [==============================] - 4s 36ms/step - loss: 2.9109e-05 - mse: 2.8122e-05 - f_pen: 9.8713e-07 - val_loss: 1.7458e-05 - val_mse: 1.6467e-05 - val_f_pen: 9.9119e-07\n",
      "Epoch 254/10000\n",
      "98/98 [==============================] - 4s 37ms/step - loss: 1.5510e-05 - mse: 1.4521e-05 - f_pen: 9.8893e-07 - val_loss: 1.9348e-05 - val_mse: 1.8358e-05 - val_f_pen: 9.9018e-07\n",
      "Epoch 255/10000\n",
      "98/98 [==============================] - 4s 36ms/step - loss: 2.2797e-05 - mse: 2.1809e-05 - f_pen: 9.8794e-07 - val_loss: 2.0240e-05 - val_mse: 1.9252e-05 - val_f_pen: 9.8890e-07\n",
      "Epoch 256/10000\n",
      "98/98 [==============================] - 4s 36ms/step - loss: 2.9814e-05 - mse: 2.8825e-05 - f_pen: 9.8856e-07 - val_loss: 1.5806e-05 - val_mse: 1.4817e-05 - val_f_pen: 9.8899e-07\n",
      "Epoch 257/10000\n",
      "98/98 [==============================] - 3s 36ms/step - loss: 1.7295e-05 - mse: 1.6306e-05 - f_pen: 9.8908e-07 - val_loss: 1.5950e-05 - val_mse: 1.4961e-05 - val_f_pen: 9.8915e-07\n",
      "Epoch 258/10000\n",
      "98/98 [==============================] - 3s 35ms/step - loss: 1.9198e-05 - mse: 1.8210e-05 - f_pen: 9.8787e-07 - val_loss: 2.9392e-05 - val_mse: 2.8403e-05 - val_f_pen: 9.8877e-07\n",
      "Epoch 259/10000\n",
      "98/98 [==============================] - 4s 36ms/step - loss: 4.3700e-05 - mse: 4.2715e-05 - f_pen: 9.8516e-07 - val_loss: 1.4342e-05 - val_mse: 1.3354e-05 - val_f_pen: 9.8785e-07\n",
      "Epoch 260/10000\n",
      "98/98 [==============================] - 4s 36ms/step - loss: 1.2060e-05 - mse: 1.1073e-05 - f_pen: 9.8711e-07 - val_loss: 1.4834e-05 - val_mse: 1.3847e-05 - val_f_pen: 9.8701e-07\n",
      "Epoch 261/10000\n",
      "98/98 [==============================] - 4s 36ms/step - loss: 1.4609e-05 - mse: 1.3623e-05 - f_pen: 9.8657e-07 - val_loss: 1.5164e-05 - val_mse: 1.4176e-05 - val_f_pen: 9.8860e-07\n",
      "Epoch 262/10000\n",
      "98/98 [==============================] - 4s 36ms/step - loss: 2.5581e-05 - mse: 2.4594e-05 - f_pen: 9.8768e-07 - val_loss: 1.8329e-05 - val_mse: 1.7337e-05 - val_f_pen: 9.9255e-07\n",
      "Epoch 263/10000\n",
      "98/98 [==============================] - 4s 37ms/step - loss: 2.2476e-05 - mse: 2.1488e-05 - f_pen: 9.8843e-07 - val_loss: 1.6113e-05 - val_mse: 1.5125e-05 - val_f_pen: 9.8780e-07\n",
      "Epoch 264/10000\n",
      "98/98 [==============================] - 4s 37ms/step - loss: 1.7836e-05 - mse: 1.6849e-05 - f_pen: 9.8739e-07 - val_loss: 1.9497e-05 - val_mse: 1.8509e-05 - val_f_pen: 9.8741e-07\n",
      "Epoch 265/10000\n",
      "98/98 [==============================] - 4s 36ms/step - loss: 4.9268e-05 - mse: 4.8285e-05 - f_pen: 9.8283e-07 - val_loss: 1.5480e-05 - val_mse: 1.4493e-05 - val_f_pen: 9.8752e-07\n",
      "Epoch 266/10000\n",
      "98/98 [==============================] - 4s 37ms/step - loss: 1.2082e-05 - mse: 1.1096e-05 - f_pen: 9.8565e-07 - val_loss: 1.6480e-05 - val_mse: 1.5489e-05 - val_f_pen: 9.9009e-07\n",
      "Epoch 267/10000\n",
      "98/98 [==============================] - 4s 37ms/step - loss: 1.3891e-05 - mse: 1.2904e-05 - f_pen: 9.8672e-07 - val_loss: 2.3719e-05 - val_mse: 2.2736e-05 - val_f_pen: 9.8262e-07\n",
      "Epoch 268/10000\n",
      "98/98 [==============================] - 4s 37ms/step - loss: 3.9042e-05 - mse: 3.8055e-05 - f_pen: 9.8670e-07 - val_loss: 1.4686e-05 - val_mse: 1.3698e-05 - val_f_pen: 9.8746e-07\n",
      "Epoch 269/10000\n",
      "98/98 [==============================] - 4s 37ms/step - loss: 1.1161e-05 - mse: 1.0173e-05 - f_pen: 9.8742e-07 - val_loss: 1.4384e-05 - val_mse: 1.3398e-05 - val_f_pen: 9.8685e-07\n",
      "Epoch 270/10000\n",
      "98/98 [==============================] - 4s 36ms/step - loss: 1.2314e-05 - mse: 1.1327e-05 - f_pen: 9.8699e-07 - val_loss: 1.6020e-05 - val_mse: 1.5032e-05 - val_f_pen: 9.8802e-07\n",
      "Epoch 271/10000\n",
      "98/98 [==============================] - 4s 36ms/step - loss: 2.3398e-05 - mse: 2.2411e-05 - f_pen: 9.8750e-07 - val_loss: 1.5228e-05 - val_mse: 1.4241e-05 - val_f_pen: 9.8722e-07\n",
      "Epoch 272/10000\n",
      "98/98 [==============================] - 4s 36ms/step - loss: 2.4484e-05 - mse: 2.3496e-05 - f_pen: 9.8781e-07 - val_loss: 1.7073e-05 - val_mse: 1.6083e-05 - val_f_pen: 9.8971e-07\n",
      "Epoch 273/10000\n",
      "98/98 [==============================] - 3s 36ms/step - loss: 2.0435e-05 - mse: 1.9448e-05 - f_pen: 9.8726e-07 - val_loss: 1.6664e-05 - val_mse: 1.5677e-05 - val_f_pen: 9.8661e-07\n",
      "Epoch 274/10000\n",
      "98/98 [==============================] - 3s 35ms/step - loss: 2.1329e-05 - mse: 2.0343e-05 - f_pen: 9.8616e-07 - val_loss: 2.2078e-05 - val_mse: 2.1095e-05 - val_f_pen: 9.8343e-07\n",
      "Epoch 275/10000\n",
      "98/98 [==============================] - 4s 37ms/step - loss: 2.4255e-05 - mse: 2.3268e-05 - f_pen: 9.8619e-07 - val_loss: 1.4119e-05 - val_mse: 1.3132e-05 - val_f_pen: 9.8694e-07\n",
      "Epoch 276/10000\n",
      "98/98 [==============================] - 4s 37ms/step - loss: 1.5122e-05 - mse: 1.4136e-05 - f_pen: 9.8630e-07 - val_loss: 1.5558e-05 - val_mse: 1.4572e-05 - val_f_pen: 9.8591e-07\n",
      "Epoch 277/10000\n",
      "98/98 [==============================] - 4s 36ms/step - loss: 3.4801e-05 - mse: 3.3814e-05 - f_pen: 9.8731e-07 - val_loss: 1.3489e-05 - val_mse: 1.2500e-05 - val_f_pen: 9.8901e-07\n",
      "Epoch 278/10000\n",
      "98/98 [==============================] - 4s 36ms/step - loss: 1.2912e-05 - mse: 1.1925e-05 - f_pen: 9.8715e-07 - val_loss: 1.7597e-05 - val_mse: 1.6611e-05 - val_f_pen: 9.8578e-07\n",
      "Epoch 279/10000\n",
      "98/98 [==============================] - 4s 36ms/step - loss: 1.9550e-05 - mse: 1.8564e-05 - f_pen: 9.8622e-07 - val_loss: 1.5197e-05 - val_mse: 1.4208e-05 - val_f_pen: 9.8889e-07\n",
      "Epoch 280/10000\n",
      "98/98 [==============================] - 4s 36ms/step - loss: 2.1637e-05 - mse: 2.0653e-05 - f_pen: 9.8491e-07 - val_loss: 1.7808e-05 - val_mse: 1.6821e-05 - val_f_pen: 9.8704e-07\n",
      "Epoch 281/10000\n",
      "98/98 [==============================] - 4s 36ms/step - loss: 2.0248e-05 - mse: 1.9263e-05 - f_pen: 9.8580e-07 - val_loss: 1.3386e-05 - val_mse: 1.2402e-05 - val_f_pen: 9.8431e-07\n",
      "Epoch 282/10000\n",
      "98/98 [==============================] - 4s 37ms/step - loss: 2.0951e-05 - mse: 1.9965e-05 - f_pen: 9.8579e-07 - val_loss: 1.6682e-05 - val_mse: 1.5696e-05 - val_f_pen: 9.8645e-07\n",
      "Epoch 283/10000\n",
      "98/98 [==============================] - 4s 37ms/step - loss: 2.0983e-05 - mse: 1.9997e-05 - f_pen: 9.8569e-07 - val_loss: 1.7753e-05 - val_mse: 1.6768e-05 - val_f_pen: 9.8488e-07\n",
      "Epoch 284/10000\n",
      "98/98 [==============================] - 4s 36ms/step - loss: 2.8384e-05 - mse: 2.7399e-05 - f_pen: 9.8515e-07 - val_loss: 1.5139e-05 - val_mse: 1.4151e-05 - val_f_pen: 9.8790e-07\n",
      "Epoch 285/10000\n",
      "98/98 [==============================] - 4s 37ms/step - loss: 1.9281e-05 - mse: 1.8295e-05 - f_pen: 9.8565e-07 - val_loss: 2.3514e-05 - val_mse: 2.2526e-05 - val_f_pen: 9.8719e-07\n",
      "Epoch 286/10000\n",
      "98/98 [==============================] - 4s 37ms/step - loss: 2.0547e-05 - mse: 1.9560e-05 - f_pen: 9.8614e-07 - val_loss: 1.8207e-05 - val_mse: 1.7216e-05 - val_f_pen: 9.9101e-07\n",
      "Epoch 287/10000\n",
      "98/98 [==============================] - 4s 38ms/step - loss: 2.1820e-05 - mse: 2.0835e-05 - f_pen: 9.8525e-07 - val_loss: 1.8416e-05 - val_mse: 1.7432e-05 - val_f_pen: 9.8389e-07\n",
      "Epoch 288/10000\n",
      "98/98 [==============================] - 4s 37ms/step - loss: 2.8379e-05 - mse: 2.7394e-05 - f_pen: 9.8454e-07 - val_loss: 1.4428e-05 - val_mse: 1.3441e-05 - val_f_pen: 9.8702e-07\n",
      "Epoch 289/10000\n",
      "98/98 [==============================] - 4s 36ms/step - loss: 1.2777e-05 - mse: 1.1791e-05 - f_pen: 9.8605e-07 - val_loss: 1.4278e-05 - val_mse: 1.3293e-05 - val_f_pen: 9.8505e-07\n",
      "Epoch 290/10000\n",
      "98/98 [==============================] - 4s 37ms/step - loss: 2.0878e-05 - mse: 1.9892e-05 - f_pen: 9.8560e-07 - val_loss: 1.7499e-05 - val_mse: 1.6515e-05 - val_f_pen: 9.8449e-07\n",
      "Epoch 291/10000\n",
      "98/98 [==============================] - 4s 36ms/step - loss: 2.5089e-05 - mse: 2.4104e-05 - f_pen: 9.8446e-07 - val_loss: 1.4539e-05 - val_mse: 1.3553e-05 - val_f_pen: 9.8584e-07\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 292/10000\n",
      "98/98 [==============================] - 4s 37ms/step - loss: 1.6651e-05 - mse: 1.5666e-05 - f_pen: 9.8425e-07 - val_loss: 1.7145e-05 - val_mse: 1.6158e-05 - val_f_pen: 9.8675e-07\n",
      "Epoch 293/10000\n",
      "98/98 [==============================] - 4s 37ms/step - loss: 2.2801e-05 - mse: 2.1816e-05 - f_pen: 9.8498e-07 - val_loss: 1.8182e-05 - val_mse: 1.7194e-05 - val_f_pen: 9.8736e-07\n",
      "Epoch 294/10000\n",
      "98/98 [==============================] - 4s 37ms/step - loss: 1.8791e-05 - mse: 1.7806e-05 - f_pen: 9.8495e-07 - val_loss: 1.6642e-05 - val_mse: 1.5656e-05 - val_f_pen: 9.8575e-07\n",
      "Epoch 295/10000\n",
      "98/98 [==============================] - 4s 36ms/step - loss: 2.4883e-05 - mse: 2.3899e-05 - f_pen: 9.8403e-07 - val_loss: 1.5171e-05 - val_mse: 1.4185e-05 - val_f_pen: 9.8566e-07\n",
      "Epoch 296/10000\n",
      "98/98 [==============================] - 4s 37ms/step - loss: 1.7784e-05 - mse: 1.6800e-05 - f_pen: 9.8411e-07 - val_loss: 1.4214e-05 - val_mse: 1.3228e-05 - val_f_pen: 9.8559e-07\n",
      "Epoch 297/10000\n",
      "98/98 [==============================] - 3s 36ms/step - loss: 2.0356e-05 - mse: 1.9371e-05 - f_pen: 9.8502e-07 - val_loss: 2.6658e-05 - val_mse: 2.5668e-05 - val_f_pen: 9.8969e-07\n",
      "Epoch 298/10000\n",
      "98/98 [==============================] - 4s 36ms/step - loss: 5.5256e-05 - mse: 5.4272e-05 - f_pen: 9.8439e-07 - val_loss: 1.3522e-05 - val_mse: 1.2537e-05 - val_f_pen: 9.8451e-07\n",
      "Epoch 299/10000\n",
      "98/98 [==============================] - 4s 36ms/step - loss: 9.1819e-06 - mse: 8.1971e-06 - f_pen: 9.8486e-07 - val_loss: 1.2445e-05 - val_mse: 1.1460e-05 - val_f_pen: 9.8556e-07\n",
      "Epoch 300/10000\n",
      "98/98 [==============================] - 4s 37ms/step - loss: 9.7219e-06 - mse: 8.7371e-06 - f_pen: 9.8479e-07 - val_loss: 1.4184e-05 - val_mse: 1.3196e-05 - val_f_pen: 9.8795e-07\n",
      "Epoch 301/10000\n",
      "98/98 [==============================] - 4s 36ms/step - loss: 1.3312e-05 - mse: 1.2328e-05 - f_pen: 9.8427e-07 - val_loss: 1.5576e-05 - val_mse: 1.4594e-05 - val_f_pen: 9.8242e-07\n",
      "Epoch 302/10000\n",
      "98/98 [==============================] - 4s 36ms/step - loss: 3.3117e-05 - mse: 3.2133e-05 - f_pen: 9.8369e-07 - val_loss: 1.2402e-05 - val_mse: 1.1418e-05 - val_f_pen: 9.8360e-07\n",
      "Epoch 303/10000\n",
      "98/98 [==============================] - 4s 36ms/step - loss: 1.2622e-05 - mse: 1.1638e-05 - f_pen: 9.8350e-07 - val_loss: 1.4507e-05 - val_mse: 1.3524e-05 - val_f_pen: 9.8368e-07\n",
      "Epoch 304/10000\n",
      "98/98 [==============================] - 4s 36ms/step - loss: 2.0764e-05 - mse: 1.9780e-05 - f_pen: 9.8459e-07 - val_loss: 1.4341e-05 - val_mse: 1.3358e-05 - val_f_pen: 9.8283e-07\n",
      "Epoch 305/10000\n",
      "98/98 [==============================] - 4s 37ms/step - loss: 1.9690e-05 - mse: 1.8706e-05 - f_pen: 9.8394e-07 - val_loss: 1.4495e-05 - val_mse: 1.3510e-05 - val_f_pen: 9.8453e-07\n",
      "Epoch 306/10000\n",
      "98/98 [==============================] - 4s 37ms/step - loss: 2.0622e-05 - mse: 1.9637e-05 - f_pen: 9.8472e-07 - val_loss: 1.4762e-05 - val_mse: 1.3778e-05 - val_f_pen: 9.8402e-07\n",
      "Epoch 307/10000\n",
      "98/98 [==============================] - 4s 37ms/step - loss: 1.8583e-05 - mse: 1.7599e-05 - f_pen: 9.8418e-07 - val_loss: 1.8347e-05 - val_mse: 1.7361e-05 - val_f_pen: 9.8576e-07\n",
      "Epoch 308/10000\n",
      "98/98 [==============================] - 4s 37ms/step - loss: 2.8800e-05 - mse: 2.7815e-05 - f_pen: 9.8495e-07 - val_loss: 1.3906e-05 - val_mse: 1.2922e-05 - val_f_pen: 9.8445e-07\n",
      "Epoch 309/10000\n",
      "98/98 [==============================] - 4s 36ms/step - loss: 1.3286e-05 - mse: 1.2302e-05 - f_pen: 9.8408e-07 - val_loss: 2.6042e-05 - val_mse: 2.5058e-05 - val_f_pen: 9.8476e-07\n",
      "Epoch 310/10000\n",
      "98/98 [==============================] - 4s 36ms/step - loss: 3.3874e-05 - mse: 3.2892e-05 - f_pen: 9.8162e-07 - val_loss: 1.2866e-05 - val_mse: 1.1883e-05 - val_f_pen: 9.8321e-07\n",
      "Epoch 311/10000\n",
      "98/98 [==============================] - 4s 36ms/step - loss: 1.2164e-05 - mse: 1.1181e-05 - f_pen: 9.8293e-07 - val_loss: 1.2162e-05 - val_mse: 1.1180e-05 - val_f_pen: 9.8258e-07\n",
      "Epoch 312/10000\n",
      "98/98 [==============================] - 4s 36ms/step - loss: 1.4773e-05 - mse: 1.3790e-05 - f_pen: 9.8287e-07 - val_loss: 1.5825e-05 - val_mse: 1.4843e-05 - val_f_pen: 9.8180e-07\n",
      "Epoch 313/10000\n",
      "98/98 [==============================] - 4s 36ms/step - loss: 2.9713e-05 - mse: 2.8730e-05 - f_pen: 9.8219e-07 - val_loss: 1.4592e-05 - val_mse: 1.3606e-05 - val_f_pen: 9.8618e-07\n",
      "Epoch 314/10000\n",
      "98/98 [==============================] - 4s 36ms/step - loss: 1.3822e-05 - mse: 1.2838e-05 - f_pen: 9.8380e-07 - val_loss: 1.5757e-05 - val_mse: 1.4775e-05 - val_f_pen: 9.8207e-07\n",
      "Epoch 315/10000\n",
      "98/98 [==============================] - 4s 37ms/step - loss: 2.9412e-05 - mse: 2.8427e-05 - f_pen: 9.8565e-07 - val_loss: 1.2951e-05 - val_mse: 1.1966e-05 - val_f_pen: 9.8459e-07\n",
      "Epoch 316/10000\n",
      "98/98 [==============================] - 4s 37ms/step - loss: 1.2654e-05 - mse: 1.1670e-05 - f_pen: 9.8422e-07 - val_loss: 1.3938e-05 - val_mse: 1.2957e-05 - val_f_pen: 9.8116e-07\n",
      "Epoch 317/10000\n",
      "98/98 [==============================] - 4s 36ms/step - loss: 1.9697e-05 - mse: 1.8713e-05 - f_pen: 9.8428e-07 - val_loss: 1.5770e-05 - val_mse: 1.4786e-05 - val_f_pen: 9.8420e-07\n",
      "Epoch 318/10000\n",
      "98/98 [==============================] - 4s 36ms/step - loss: 2.1443e-05 - mse: 2.0459e-05 - f_pen: 9.8382e-07 - val_loss: 1.3391e-05 - val_mse: 1.2408e-05 - val_f_pen: 9.8362e-07\n",
      "Epoch 319/10000\n",
      "98/98 [==============================] - 4s 36ms/step - loss: 1.8112e-05 - mse: 1.7128e-05 - f_pen: 9.8370e-07 - val_loss: 1.9636e-05 - val_mse: 1.8649e-05 - val_f_pen: 9.8761e-07\n",
      "Epoch 320/10000\n",
      "98/98 [==============================] - 4s 36ms/step - loss: 3.5535e-05 - mse: 3.4552e-05 - f_pen: 9.8303e-07 - val_loss: 1.2351e-05 - val_mse: 1.1367e-05 - val_f_pen: 9.8400e-07\n",
      "Epoch 321/10000\n",
      "98/98 [==============================] - 4s 36ms/step - loss: 1.0013e-05 - mse: 9.0299e-06 - f_pen: 9.8284e-07 - val_loss: 1.2503e-05 - val_mse: 1.1520e-05 - val_f_pen: 9.8216e-07\n",
      "Epoch 322/10000\n",
      "98/98 [==============================] - 4s 36ms/step - loss: 2.8748e-05 - mse: 2.7766e-05 - f_pen: 9.8200e-07 - val_loss: 2.8629e-05 - val_mse: 2.7645e-05 - val_f_pen: 9.8367e-07\n",
      "Epoch 323/10000\n",
      "98/98 [==============================] - 4s 36ms/step - loss: 1.6479e-05 - mse: 1.5495e-05 - f_pen: 9.8332e-07 - val_loss: 1.6629e-05 - val_mse: 1.5645e-05 - val_f_pen: 9.8435e-07\n",
      "Epoch 324/10000\n",
      "98/98 [==============================] - 4s 37ms/step - loss: 1.6016e-05 - mse: 1.5033e-05 - f_pen: 9.8271e-07 - val_loss: 1.1486e-05 - val_mse: 1.0502e-05 - val_f_pen: 9.8429e-07\n",
      "Epoch 325/10000\n",
      "98/98 [==============================] - 4s 37ms/step - loss: 2.1440e-05 - mse: 2.0458e-05 - f_pen: 9.8199e-07 - val_loss: 1.5561e-05 - val_mse: 1.4577e-05 - val_f_pen: 9.8426e-07\n",
      "Epoch 326/10000\n",
      "98/98 [==============================] - 4s 36ms/step - loss: 1.7577e-05 - mse: 1.6595e-05 - f_pen: 9.8213e-07 - val_loss: 2.3686e-05 - val_mse: 2.2703e-05 - val_f_pen: 9.8284e-07\n",
      "Epoch 327/10000\n",
      "98/98 [==============================] - 4s 36ms/step - loss: 2.8864e-05 - mse: 2.7881e-05 - f_pen: 9.8214e-07 - val_loss: 1.5911e-05 - val_mse: 1.4927e-05 - val_f_pen: 9.8458e-07\n",
      "Epoch 328/10000\n",
      "98/98 [==============================] - 4s 37ms/step - loss: 1.3527e-05 - mse: 1.2545e-05 - f_pen: 9.8228e-07 - val_loss: 1.6617e-05 - val_mse: 1.5632e-05 - val_f_pen: 9.8539e-07\n",
      "Epoch 329/10000\n",
      "98/98 [==============================] - 4s 37ms/step - loss: 3.3809e-05 - mse: 3.2826e-05 - f_pen: 9.8344e-07 - val_loss: 1.4224e-05 - val_mse: 1.3238e-05 - val_f_pen: 9.8648e-07\n",
      "Epoch 330/10000\n",
      "98/98 [==============================] - 4s 37ms/step - loss: 1.2168e-05 - mse: 1.1186e-05 - f_pen: 9.8222e-07 - val_loss: 1.3062e-05 - val_mse: 1.2081e-05 - val_f_pen: 9.8134e-07\n",
      "Epoch 331/10000\n",
      "98/98 [==============================] - 4s 36ms/step - loss: 1.3285e-05 - mse: 1.2303e-05 - f_pen: 9.8270e-07 - val_loss: 1.3925e-05 - val_mse: 1.2941e-05 - val_f_pen: 9.8471e-07\n",
      "Epoch 332/10000\n",
      "98/98 [==============================] - 4s 37ms/step - loss: 1.8764e-05 - mse: 1.7782e-05 - f_pen: 9.8185e-07 - val_loss: 1.3016e-05 - val_mse: 1.2034e-05 - val_f_pen: 9.8228e-07\n",
      "Epoch 333/10000\n",
      "98/98 [==============================] - 4s 36ms/step - loss: 1.8105e-05 - mse: 1.7123e-05 - f_pen: 9.8130e-07 - val_loss: 1.2358e-05 - val_mse: 1.1377e-05 - val_f_pen: 9.8176e-07\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 334/10000\n",
      "98/98 [==============================] - 4s 36ms/step - loss: 2.3466e-05 - mse: 2.2485e-05 - f_pen: 9.8133e-07 - val_loss: 1.3474e-05 - val_mse: 1.2490e-05 - val_f_pen: 9.8409e-07\n",
      "Epoch 335/10000\n",
      "98/98 [==============================] - 3s 36ms/step - loss: 1.6876e-05 - mse: 1.5895e-05 - f_pen: 9.8086e-07 - val_loss: 1.4687e-05 - val_mse: 1.3706e-05 - val_f_pen: 9.8169e-07\n",
      "Epoch 336/10000\n",
      "98/98 [==============================] - 4s 37ms/step - loss: 1.7979e-05 - mse: 1.6998e-05 - f_pen: 9.8078e-07 - val_loss: 1.3098e-05 - val_mse: 1.2117e-05 - val_f_pen: 9.8107e-07\n",
      "Epoch 337/10000\n",
      "98/98 [==============================] - 4s 37ms/step - loss: 1.8883e-05 - mse: 1.7901e-05 - f_pen: 9.8129e-07 - val_loss: 1.4137e-05 - val_mse: 1.3154e-05 - val_f_pen: 9.8215e-07\n",
      "Epoch 338/10000\n",
      "98/98 [==============================] - 4s 37ms/step - loss: 2.1862e-05 - mse: 2.0880e-05 - f_pen: 9.8119e-07 - val_loss: 1.5437e-05 - val_mse: 1.4455e-05 - val_f_pen: 9.8181e-07\n",
      "Epoch 339/10000\n",
      "98/98 [==============================] - 4s 37ms/step - loss: 2.3035e-05 - mse: 2.2054e-05 - f_pen: 9.8094e-07 - val_loss: 1.8946e-05 - val_mse: 1.7963e-05 - val_f_pen: 9.8282e-07\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00339: early stopping\n"
     ]
    }
   ],
   "source": [
    "history = dipole_model.train_model(\n",
    "    checkpoint_path=None,\n",
    "    batch_size=batch_size,\n",
    "    learning_rate=lr,\n",
    "    min_delta=min_delta\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18b66a5b",
   "metadata": {},
   "source": [
    "# Testing model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "471e1804",
   "metadata": {},
   "source": [
    "Predict on testing phase-space points generated earlier.\n",
    "\n",
    "This function will automatically calculate all the required dipoles and recoil factors to do the inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c88a40c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 1s 116ms/step\n"
     ]
    }
   ],
   "source": [
    "coefs, test_dipoles, y_preds = dipole_model.dipole_network_predictor(\n",
    "    model=dipole_model.model,\n",
    "    inputs=relevant_inputs,\n",
    "    momenta=X_test,\n",
    "    batch_size=2**16 # try increasing batch size to infer on more points at once\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "925368c4",
   "metadata": {},
   "source": [
    "Here we provide a simple plotting function that plots the error distributions in absolute percentage difference and as a prediction-to-truth ratio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f1a63409",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 648x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "utility_functions.plot_diffs(Y_test, {\"NN\": y_preds}, bins=np.linspace(-0.02, 0.02, 50));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e5fa931",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "test_fame",
   "language": "python",
   "name": "test_fame"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
