{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "da214ca6",
   "metadata": {},
   "source": [
    "# Factorisation-aware matrix element emulator\n",
    "\n",
    "This is a quickstart notebook that will take you through the necessary steps to reproduce the method described in the paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5b09063e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "24672827",
   "metadata": {},
   "outputs": [],
   "source": [
    "# comment this out if you want to use GPU\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '-1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ddc3fd07",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fame.phase_space import phasespace\n",
    "from fame.utilities import utility_functions, tests\n",
    "from fame.data_generation import cs_dipole, model_inputs\n",
    "from fame.model.dipole_model import DipoleModel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d04f3ef7",
   "metadata": {},
   "source": [
    "# Generate phase-space points using RAMBO with FastJet clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "738a5b1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "w = 1000 # sqrt(s)\n",
    "num_jets = 4 # number of final state jets\n",
    "train_points = 500000\n",
    "test_points = 100000\n",
    "num_points = train_points + test_points # number of phase-space points to generate\n",
    "y_global_cut = 0.001 # global phase-space cut\n",
    "num_cores = 16 # number of cores for parallel clustering of jets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "27aa759b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "66cbcd09eeae4c209e17addeeeabcf6d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Phase-space points:   0%|          | 0/600000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "######## Finished generating generic phase-space points #########\n"
     ]
    }
   ],
   "source": [
    "# clusters points using e+e- kt algorithm\n",
    "X = phasespace.generate_generic(\n",
    "    num_jets,\n",
    "    num_points,\n",
    "    w,\n",
    "    y_global_cut,\n",
    "    num_cores\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f445b6b",
   "metadata": {},
   "source": [
    "Phase-space points are structured in the following way:\n",
    "1. First and seconds rows are e+ and e-\n",
    "2. Third row is quark\n",
    "3. Fourth row is anti-quark\n",
    "4. Fifth and onwards are gluons\n",
    "\n",
    "This helps us to keep consistency between the inputs we calculate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1b6c3aef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 500.        ,    0.        ,    0.        ,  500.        ],\n",
       "       [ 500.        ,    0.        ,    0.        , -500.        ],\n",
       "       [ 351.17310502,  147.05877403,  -72.31030558,  310.5921544 ],\n",
       "       [ 364.99573015, -333.08502821,   36.63115041, -144.68726907],\n",
       "       [ 101.65025089,   20.47226992,   -1.4647162 ,  -99.55658831],\n",
       "       [ 182.18091394,  165.55398427,   37.14387138,  -66.34829702]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3add144b",
   "metadata": {},
   "source": [
    "# Evaluate phase-space points using matrix element provider\n",
    "## here we use NJet but others are available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5d463b83",
   "metadata": {},
   "outputs": [],
   "source": [
    "njet_data, njet_order = utility_functions.run_njet(num_jets)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c52759c5",
   "metadata": {},
   "source": [
    "Check 'njet_data' has the correct incoming and outgoing particles.\n",
    "\n",
    "'inc': [11, -11] -> e+ e-\n",
    "\n",
    "'out': [1, -1, 21, 21] -> ddxgg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "255d6ce0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'born': 0, 'inc': [11, -11], 'loop': 0, 'mcn': 1, 'name': 'eeddxGG', 'out': [1, -1, 21, 21]}]\n"
     ]
    }
   ],
   "source": [
    "print(njet_data[0][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "84b5adb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- channel eeddxGG -------- (600000 points)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a2c3521125b1439c8b0064bd6a83f6fd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/600000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "Y = utility_functions.generate_LO_njet([x.tolist() for x in X], njet_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37760ec3",
   "metadata": {},
   "source": [
    "# Split data into training and testing sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dd4a51bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test = X[:train_points], X[train_points:]\n",
    "Y_train, Y_test = Y[:train_points], Y[train_points:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "355c298a",
   "metadata": {},
   "source": [
    "Check magnitude of training and testing datasets are similar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dea862ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZEAAAEQCAYAAABxzUkqAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuNCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8QVMy6AAAACXBIWXMAAAsTAAALEwEAmpwYAAAeUklEQVR4nO3df7xVdZ3v8dfbgxxUCBvEucmRgCtqhySUE/ijEsYY8ZqhhgppWTEhqdl1Hl5/lBn5mB7pTBOTqWMU3JSmAQezcKTrZGpaMgKmXQHDjsjEMevS0Y4SoRz83D/2grbbvc/ZZ5299j77nPfz8TgP1/qu79rr82V7+PBd37W+X0UEZmZmaexX6wDMzKx+OYmYmVlqTiJmZpaak4iZmaXmJGJmZqk5iZiZWWqDah1ApRxyyCExZsyYWodhZlZXHn/88d9HxMi052eaRCTNBL4GNADfiogbCo43AncAk4F24LyI2CrpfOB/5VWdCBwXEU+WutaYMWNYv359hVtgZta/Sfqv3pyf2e0sSQ3ALcBpQDMwV1JzQbV5wEsRcQSwCLgRICL+JSImRcQk4CPAc10lEDMzq40sx0SmAK0RsSUiXgOWA7MK6swCbk+2VwKnSFJBnbnJuWZm1sdkmURGAdvy9tuSsqJ1IqIT6ABGFNQ5D/jXYheQNF/Seknrt2/fXpGgzcysfH16YF3SVGBnRGwodjwiFgOLAVpaWjwJmNkAtHv3btra2ti1a1etQ+nThgwZQlNTE/vvv39FPzfLJPI8cHjeflNSVqxOm6RBwHByA+x7zaFEL8TMDKCtrY1hw4YxZswY3nw33AAigvb2dtra2hg7dmxFPzvL21nrgPGSxkoaTC4hrCqoswq4MNmeDTwQybTCkvYDzsXjIWbWhV27djFixAgnkC5IYsSIEZn01jLriUREp6RLgfvIPeK7NCI2SroeWB8Rq4AlwDJJrcCL5BLNXu8DtkXElqxiNLP+wQmke1n9GWU6JhIRq4HVBWXX5W3vAs4pce5DwPFZxmdm1hvt7e2ccsopAPz2t7+loaGBkSNz7+2tXbuWwYMHlzx3/fr13HHHHdx0001dXuPEE0/k0UcfrVzQFdanB9atQh78culj06+pXhxmVbDoR89U9PMun3FkyWMjRozgySefBGDhwoUMHTqUK664Yt/xzs5OBg0q/tdsS0sLLS0t3V6/LycQ8NxZZmYV9bGPfYwFCxYwdepUrrzyStauXcsJJ5zAsccey4knnsjmzZsBeOihh/jABz4A5BLQJz7xCaZNm8a4cePe0DsZOnTovvrTpk1j9uzZHH300Zx//vnsXZl29erVHH300UyePJnLLrts3+dWg3siA8CaLe0lj50wvYqBmA0QbW1tPProozQ0NPDyyy/zyCOPMGjQIO6//34++9nPctddd73pnF/+8pc8+OCDvPLKKxx11FF86lOfetPjuE888QQbN27ksMMO46STTuJnP/sZLS0tXHTRRTz88MOMHTuWuXPnVquZgJNIv1KqG++BJbPqOuecc2hoaACgo6ODCy+8kF/96ldIYvfu3UXPOf3002lsbKSxsZFDDz2U3/3udzQ1Nb2hzpQpU/aVTZo0ia1btzJ06FDGjRu379HduXPnsnjx4gxb90ZOIv3I8b/u+f84pRJPV/eBzaxrBx100L7tz3/+80yfPp27776brVu3Mm3atKLnNDY27ttuaGigs7MzVZ1q85iImVmGOjo6GDUqN+PTt7/97Yp//lFHHcWWLVvYunUrACtWrKj4NbriJGJmlqErr7ySa665hmOPPTaTnsMBBxzArbfeysyZM5k8eTLDhg1j+PDhFb9OKdo7ul/vWlpaYqCvJ7JmyRXdVyrTCfO+UrHPMsvS008/zTve8Y5ah1FTO3bsYOjQoUQEl1xyCePHj+fyyy9/U71if1aSHo+I7p81LsE9ETOzOvfNb36TSZMmMWHCBDo6Orjooouqdm0PrJuZ1bnLL7+8aM+jGtwTMTOz1JxEzMwsNd/OsqL8/oiZlcNJxIoq/eKin9oysz9zEqlDnt7ErG/ozVTwkJtUcfDgwZx44okA3HbbbRx44IF89KMfzTbwCnISMbP+paulD9LoYrmE7qaC785DDz3E0KFD9yWRBQsW9CrUWvDAuplZBT3++OOcfPLJTJ48mVNPPZUXXngBgJtuuonm5mYmTpzInDlz2Lp1K7fddhuLFi1i0qRJPPLIIyxcuJCvfCV3y3jatGlcddVVTJkyhSOPPJJHHnkEgJ07d3LuuefS3NzMWWedxdSpU6nli9buiZiZVUhE8OlPf5of/OAHjBw5khUrVvC5z32OpUuXcsMNN/Dcc8/R2NjIH/7wBw4++GAWLFjwht7Lj3/84zd8XmdnJ2vXrmX16tV88Ytf5P777+fWW2/lrW99K5s2bWLDhg1MmjSpBi39MycRM7MKefXVV9mwYQMzZswAYM+ePbztbW8DYOLEiZx//vmceeaZnHnmmWV93tlnnw3A5MmT902w+NOf/pTPfOYzALzzne9k4sSJlW1EDzmJ1KE0U76bWfYiggkTJrBmzZo3Hbv33nt5+OGHueeee/jSl77EU0891e3n7Z36va9M+16Mx0TMzCqksbGR7du370siu3fvZuPGjbz++uts27aN6dOnc+ONN9LR0cGOHTsYNmwYr7zySo+ucdJJJ3HnnXcCsGnTprKSUZYyTSKSZkraLKlV0tVFjjdKWpEcf0zSmLxjEyWtkbRR0lOShmQZq5lZb+23336sXLmSq666ine9611MmjSJRx99lD179nDBBRdwzDHHcOyxx3LZZZdx8MEHc8YZZ3D33XfvG1gvx8UXX8z27dtpbm7m2muvZcKECVWd+r1QZlPBS2oAngFmAG3AOmBuRGzKq3MxMDEiFkiaA5wVEedJGgT8HPhIRPxC0gjgDxGxp9T1BtJU8JWc8r2nPEW89TUDbSr4PXv2sHv3boYMGcKzzz7L+9//fjZv3tztOymQzVTwWY6JTAFaI2ILgKTlwCxgU16dWcDCZHslcLMkAX8N/N+I+AVARLRnGKf1RKln8Lt4lt7MKmfnzp1Mnz6d3bt3ExHceuutZSWQrGSZREYB2/L224CppepERKekDmAEcCQQku4DRgLLI+LvM4zVyrRmS/F8fsL0KgdiNkANGzaspu+FFOqrT2cNAt4DvBvYCfw46XK94SFqSfOB+QCjR4+uepBmZgNdlgPrzwOH5+03JWVF6yTjIMOBdnK9locj4vcRsRNYDRxXeIGIWBwRLRHRsne+GjMbePrLMt9ZyurPKMsksg4YL2mspMHAHGBVQZ1VwIXJ9mzggci19D7gGEkHJsnlZN44lmJmBsCQIUNob293IulCRNDe3s6QIZV/yDWz21nJGMel5BJCA7A0IjZKuh5YHxGrgCXAMkmtwIvkEg0R8ZKkr5JLRAGsjoh7s4rVzOpXU1MTbW1tbN++vdah9GlDhgyhqamp4p+b6ZhIRKwmdysqv+y6vO1dwDklzv0O8J0s4zOz+rf//vszduzYWocxYPXVgXWjtu+DmJmVw9OemJlZak4iZmaWmpOImZml5iRiZmapOYmYmVlqTiJmZpaaH/G1iujqcWRPH2/Wf7knYmZmqTmJmJlZak4iZmaWmpOImZml5iRiZmapOYmYmVlqTiJmZpaak4iZmaXmJGJmZqk5iZiZWWpOImZmlpqTiJmZpeYkYmZmqWU6i6+kmcDXgAbgWxFxQ8HxRuAOYDLQDpwXEVsljQGeBjYnVf8zIhZkGatlZ9GPnilafvmMI6sciZlVWmZJRFIDcAswA2gD1klaFRGb8qrNA16KiCMkzQFuBM5Ljj0bEZOyis+q5/hfLy5xxFPEm9W7LG9nTQFaI2JLRLwGLAdmFdSZBdyebK8ETpGkDGMyM7MKyjKJjAK25e23JWVF60REJ9ABjEiOjZX0hKSfSHpvhnGamVlKfXVlwxeA0RHRLmky8H1JEyLi5fxKkuYD8wFGjx5dgzDNzAa2LJPI88DheftNSVmxOm2SBgHDgfaICOBVgIh4XNKzwJHA+vyTI2IxsBigpaUlsmhENZQaeD6+ynGYmfVUlrez1gHjJY2VNBiYA6wqqLMKuDDZng08EBEhaWQyMI+kccB4YEuGsZqZWQqZ9UQiolPSpcB95B7xXRoRGyVdD6yPiFXAEmCZpFbgRXKJBuB9wPWSdgOvAwsi4sWsYjUzs3QyHROJiNXA6oKy6/K2dwHnFDnvLuCuLGMzM7Pe66sD6wNK6fcozMz6Nk97YmZmqTmJmJlZak4iZmaWmpOImZml5oF1q5k1S64oWn7CPE/MaFYv3BMxM7PUnETMzCw1JxEzM0vNScTMzFJzEjEzs9ScRMzMLDUnETMzS81JxMzMUnMSMTOz1MpKIpKOyToQMzOrP+X2RG6VtFbSxZKGZxqRmZnVjbKSSES8FzgfOBx4XNJ3Jc3INDIzM+vzyh4TiYhfAdcCVwEnAzdJ+qWks7MKzszM+rZyx0QmSloEPA38FXBGRLwj2V6UYXxmZtaHlTsV/NeBbwGfjYg/7S2MiN9IujaTyMzMrM8rN4mcDvwpIvYASNoPGBIROyNiWWbR2YDkdUbM6ke5YyL3Awfk7R+YlHVJ0kxJmyW1Srq6yPFGSSuS449JGlNwfLSkHZKK/61iZmY1VW4SGRIRO/buJNsHdnWCpAbgFuA0oBmYK6m5oNo84KWIOILc2MqNBce/CvywzBjNzKzKyk0if5R03N4dSZOBP3VRH2AK0BoRWyLiNWA5MKugzizg9mR7JXCKJCXXOBN4DthYZoxmZlZl5Y6J/E/g3yT9BhDw34DzujlnFLAtb78NmFqqTkR0SuoARkjaRe5R4hlAyVtZkuYD8wFGjx5dZlPMzKxSykoiEbFO0tHAUUnR5ojYnV1YLAQWRcSOpGNSKq7FwGKAlpaWyDAeMzMrotyeCMC7gTHJOcdJIiLu6KL+8+TecN+rKSkrVqdN0iBgONBOrscyW9LfAwcDr0vaFRE39yBeMzPLWFlJRNIy4L8DTwJ7kuIAukoi64DxksaSSxZzgA8X1FkFXAisAWYDD0REAO/Nu/ZCYIcTiJlZ31NuT6QFaE7+gi9LMsZxKXAf0AAsjYiNkq4H1kfEKmAJsExSK/AiuUTTPz345VpHYGZWceUmkQ3kBtNf6MmHR8RqYHVB2XV527uAc7r5jIU9uaaZmVVPuUnkEGCTpLXAq3sLI+KDmURlZmZ1odwksjDLIAaCNVvaax2CmVnFlfuI708kvR0YHxH3SzqQ3DiHmZkNYOU+nfVJci/1/QW5p7RGAbcBp2QXmlmBUg8nTL+munGY2T7l3s66hNw0Jo9BboEqSYdmFpVZEaVuCZ4wvcqBmNk+5c6d9Woy/xUAyYuBfkPczGyAKzeJ/ETSZ4EDkrXV/w24J7uwzMysHpSbRK4GtgNPAReRe/fDKxqamQ1w5T6d9TrwzeTHzMwMKP/prOcoMgYSEeMqHpGZmdWNnsydtdcQclOV/EXlwzEzs3pS1phIRLTn/TwfEf8EnJ5taGZm1teVezvruLzd/cj1THqyFomZmfVD5SaCf8zb7gS2AudWPBozM6sr5T6d5XeCzczsTcq9nfW3XR2PiK9WJhwzM6snPXk6693klrMFOANYC/wqi6DMzKw+lJtEmoDjIuIV2Lfu+b0RcUFWgZmZWd9X7rQnfwm8lrf/WlJmZmYDWLk9kTuAtZLuTvbPBG7PJCKzniq1zgh4rRGzjJX7dNaXJP0QeG9S9PGIeCK7sMzK19XSw15rxCxb5d7OAjgQeDkivga0SRrb3QmSZkraLKlV0tVFjjdKWpEcf0zSmKR8iqQnk59fSDqrB3GamVmVlJVEJH0BuArYe29gf+A73ZzTANwCnAY0A3MlNRdUmwe8FBFHAIuAG5PyDUBLREwCZgLfSBbCMjOzPqTcnshZwAeBPwJExG+AYd2cMwVojYgtyaqIy4FZBXVm8eexlZXAKZIUETsjojMpH4JXUTQz65PKTSKvRUSQ/GUu6aAyzhkFbMvbb0vKitZJkkYHMCK5xlRJG8kthLUgL6mYmVkfUW4SuVPSN4CDJX0SuJ+MF6iKiMciYgK5lxyvkTSksI6k+ZLWS1q/ffv2LMMxM7Miuk0ikgSsIHe76S7gKOC6iPh6N6c+Dxyet9+UlBWtk4x5DAfe8KhNRDwN7ADeWXiBiFgcES0R0TJy5MjummJmZhXW7WB1RISk1RFxDPCjHnz2OmB88hTX88Ac4MMFdVYBFwJrgNnAA8n1xgLbIqJT0tuBo8nNHGxmZn1IuU88/VzSuyNiXbkfnCSAS4H7gAZgaURslHQ9sD4iVgFLgGWSWoEXySUagPcAV0vaDbwOXBwRvy/32mb7lHoR0S8hmlVEuUlkKnCBpK3kntASuU7KxK5OiojVwOqCsuvytneRW2q38LxlwLIyYzMzsxrpMolIGh0RvwZOrVI8ZmZWR7rriXyf3Oy9/yXproj4UBViMjOzOtFdElHe9rgsAzHLQql5tTynlllldPeIb5TYNjMz67Yn8i5JL5PrkRyQbMOfB9bfkml0ZmbWp3WZRCKioVqBmJlZ/enJVPBmZmZv4OnVK62rVfbMzPoZJ5EK62qVPTOz/sa3s8zMLDUnETMzS81JxMzMUnMSMTOz1JxEzMwsNT+dZQPSmiVXFC0/Yd5XqhyJWX1zT8TMzFJzEjEzs9ScRMzMLDUnETMzS81JxMzMUnMSMTOz1JxEzMwstUyTiKSZkjZLapV0dZHjjZJWJMcfkzQmKZ8h6XFJTyX//ass4zQzs3QySyKSGoBbgNOAZmCupOaCavOAlyLiCGARcGNS/nvgjIg4BrgQWJZVnGZmll6Wb6xPAVojYguApOXALGBTXp1ZwMJkeyVwsyRFxBN5dTaSW9+9MSJezTBes9KLik2/prpxmNWJLJPIKGBb3n4bMLVUnYjolNQBjCDXE9nrQ8DPiyUQSfOB+QCjR4+uXOQ2YJVaVOyE6VUOxKxO9OmBdUkTyN3iuqjY8YhYHBEtEdEycuTI6gZnZmaZJpHngcPz9puSsqJ1JA0ChgPtyX4TcDfw0Yh4NsM4zcwspSyTyDpgvKSxkgYDc4BVBXVWkRs4B5gNPBARIelg4F7g6oj4WYYxmplZL2SWRCKiE7gUuA94GrgzIjZKul7SB5NqS4ARklqBvwX2PgZ8KXAEcJ2kJ5OfQ7OK1czM0sl0PZGIWA2sLii7Lm97F3BOkfP+Dvi7LGMzM7Pe69MD62Zm1rc5iZiZWWpOImZmlprXWDcrh99kNyvKScSsDH6T3aw4384yM7PUnETMzCw1JxEzM0vNScTMzFJzEjEzs9ScRMzMLDUnETMzS83viZj1wpolV5Q8dsK8r1QxErPacBJJadGPnilafnyV4zAzqyUnkZSO//XiWodgZlZzHhMxM7PUnETMzCw1JxEzM0vNYyJmWfH08TYAuCdiZmapuSdilhGvQWIDQaY9EUkzJW2W1Crp6iLHGyWtSI4/JmlMUj5C0oOSdki6OcsYzcwsvcySiKQG4BbgNKAZmCupuaDaPOCliDgCWATcmJTvAj4PlH4d2MzMai7LnsgUoDUitkTEa8ByYFZBnVnA7cn2SuAUSYqIP0bET8klEzMz66OyTCKjgG15+21JWdE6EdEJdAAjMozJzMwqqK6fzpI0X9J6Seu3b99e63DMzAacLJ/Oeh44PG+/KSkrVqdN0iBgOFD8kZYiImIxsBigpaUlehWtWZWUmvnXs/5aPcqyJ7IOGC9prKTBwBxgVUGdVcCFyfZs4IGIcDIwM6sTmfVEIqJT0qXAfUADsDQiNkq6HlgfEauAJcAySa3Ai+QSDQCStgJvAQZLOhP464jYlFW8ZmbWc5m+bBgRq4HVBWXX5W3vAs4pce6YLGMzM7Peq+uBdTMzqy1Pe2LWV3jCRqtDTiJmfYTn2rJ65NtZZmaWmpOImZml5iRiZmapeUzErI9b9KNnipZfPuPIKkdi9mbuiZiZWWruiZj1ccf/enGJI55ry2rPPREzM0vNScTMzFLz7SyzOlVqSnnwtPJWPe6JmJlZau6JdKOrf+2ZmQ10TiJm/ZDfLbFqcRIx64dKPhb84Iji5Z4p2FJyEjEbQDxTsFWaB9bNzCw1JxEzM0vNt7PMrORTiH7fxLrjJGJmJZV6ygv8pJflOImYWUmlJ3+ENUuKl7v3MrBkmkQkzQS+BjQA34qIGwqONwJ3AJOBduC8iNiaHLsGmAfsAS6LiPuyjNXMKsO3xgaWzJKIpAbgFmAG0Aask7QqIjblVZsHvBQRR0iaA9wInCepGZgDTAAOA+6XdGRE7MkqXjPLVsnkMs7vrtSzLHsiU4DWiNgCIGk5MAvITyKzgIXJ9krgZklKypdHxKvAc5Jak89bk1Wwnt7ErDZKvbvCFk8wWQ+yTCKjgG15+23A1FJ1IqJTUgcwIin/z4JzRxVeQNJ8YH6yu0PSZmA40FEknmLlhWWl9vPL924fAvy+yHXKVSrOcur0tC3dbfemLdVsR+F+4ffj76TrGMupU+nvBLJoy9/8Y/d1+s93UuxYJb+Tt3cTV9ciIpMfYDa5cZC9+x8Bbi6oswFoytt/NmnczcAFeeVLgNllXndxueWFZaX288vzytb38s+naJxZtKW77d60pZrt6CJ+fyd99Dupp7b01e+kp99B1t9J4U+WLxs+Dxyet9+UlBWtI2kQuUzZXua5pdzTg/LCslL793RRJ61yPqdSbSlnO61qtqNwv9j30xv+TvyddLedVm/aUexYLb+TN1CSlSr/wbmk8AxwCrkEsA74cERszKtzCXBMRCxIBtbPjohzJU0AvktuHOQw4MfA+OhDA+uS1kdES63jqIT+0pb+0g5wW/qi/tIOqGxbMhsTidwYx6XAfeQe8V0aERslXU+uK7WK3G2qZcnA+YvknsgiqXcnuUH4TuCSvpRAEqUfoK8//aUt/aUd4Lb0Rf2lHVDBtmTWEzEzs/7PEzCamVlqTiJmZpaak4iZmaXmJFJBksZJWiJpZVdlfV2Jdhwk6XZJ35R0fi3jS0NSs6Q7Jf2zpNm1jqc3JI2W9H1JSyVdXet40pL0Xkm3SfqWpEdrHU9vSNpP0pckfV3ShbWOJy1J0yQ9knwv08o5x0kkkfxC/j9JGwrKZ0raLKm1u1/YiNgSEfO6K8tSVu0AzgZWRsQngQ9WOOwuVaJNwGnA1yPiU8BHMwu2GxVqyzHkvotPAMdmFmwXKvT/2SMRsQD4d+D2LOPtSoW+k1nk3mfbTW6GjaqrUDsC2AEModx2VOqtxXr/Ad4HHAdsyCtrIPcW/ThgMPALoJncL/G/F/wcmnfeyiKf/6ayemoHcA0wKdn+br19N8nPLcA/AD+r5//PyE0N9CDwAPDxem1H3nl3AsPq/Du5GrgoObcqv+sZtWO/5Ly/BP6lnOt6PZFERDwsaUxBcdFJJCPiy8AHqhxiWTJsRxu5f2k9SZV7sBVs0yXJ7NLfyyzYblSiLZKuAL6QfNZK4H9nHPabVOo7kTQa6IiIV7KMtysV+k7agNeS3Zq801bh3/2XgMZyruvbWV0rNonkmyaC3EvSCEm3Accqtx5K0bIa6HU7yP3F+yFJ/0xG0yf0UE/bNEbSYnLr1/xDxrH1VI/aAvwf4LLkO9qaYVw91dN2QG45iKonwTL0tC3fA06V9HXg4SwD66Ge/p6cLekbwDJycxh2yz2RCoqIdmBBd2V9XYl2/BH4eG0i6r3ILXY2v7t69SAiNpCb4LTuRcQXah1DJUTETnIJsa5FxPfoYU/dPZGu9WYiyL6kv7QjX39qU39pS39pB/SftmTeDieRrq0DxksaK2kwubm9VtU4pjT6Szvy9ac29Ze29Jd2QP9pS/btqNUTEX3tB/hX4AX+/IjevKT8f5CbjfhZ4HO1jnOgtKO/tqm/tKW/tKM/taVW7fAEjGZmlppvZ5mZWWpOImZmlpqTiJmZpeYkYmZmqTmJmJlZak4iZmaWmpOImZml5iRiZmapeQJGsxqTdCZwOvAWYElE/EdtIzIrn3siZr0g6aEiazj06NyI+H7kVoxcAJxX0QDNMuYkYlZhyq1H/5YSxw7r4tRrya2+aFY3nETMKm8aRdahl3Qy8Pki5ZJ0I/DDiPh59uGZVY6TiFnltQDT8wskHQF8FdhepP6ngfcDsyXV1QJmZh5YN6u8Q4AZe3ckvRU4FdjIG5cqBSAibgJuqlp0ZhXknohZBUlqBFqBPZLeI2l/4JPAbeRWlWurZXxmleaeiFllvR+4HxgDfAKYAnwjIvZIchKxfsc9EbMKSHodU4HjImIjsJjcinLfj4iOpNoo4CVJ/seb9RtOImaV0QT8E/BdgIh4Gjg+Irbk1VkKfAo4qOrRmWXE/yIyq4CIWA4sLyjbWrD/6WrGZFYN7omY9c63gT/U4FyzPkERUesYzMysTrknYmZmqTmJmJlZak4iZmaWmpOImZml5iRiZmapOYmYmVlqTiJmZpaak4iZmaX2/wHY+DwksfWRpgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "bins = np.logspace(np.log10(np.min(Y_train)), np.log10(np.max(Y_train)), 50)\n",
    "plt.figure()\n",
    "plt.hist(Y_train, bins, weights=np.ones(train_points) / train_points, alpha=0.5, label='Training')\n",
    "plt.hist(Y_test, bins, weights=np.ones(test_points) / test_points, alpha=0.5, label='Testing')\n",
    "plt.legend()\n",
    "plt.xscale(\"log\")\n",
    "plt.xlabel(r\"$|\\mathcal{M}|^{2}$\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21cb5e86",
   "metadata": {},
   "source": [
    "# Generate neural network inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18277946",
   "metadata": {},
   "source": [
    "## Calculate dipoles and recoil factors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdeb97b5",
   "metadata": {},
   "source": [
    "First need to get relevant permutations where each permutation is in the form $(i, j, k)$\n",
    "\n",
    "$i$ = emitter,\n",
    "$j$ = emitted,\n",
    "$k$ = spectator\n",
    "\n",
    "Partons are numbered from $(1, ..., n_{j})$ where 1 = $q$, 2 = $\\bar{q}$, and everything 3+ = $g$, like the phase-space points.\n",
    "\n",
    "For example permutation $(2, 4, 1)$ would mean the anti-quark emits a gluon and the quark is the spectator.\n",
    "\n",
    "**For each permutation we have a corresponding dipole and recoil factor.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "06033d1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "relevant_permutations = model_inputs.get_relevant_permutations(num_jets)\n",
    "tests.check_relevant_permutations(relevant_permutations, num_jets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1b701f68",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, 3, 2),\n",
       " (1, 3, 4),\n",
       " (1, 4, 2),\n",
       " (1, 4, 3),\n",
       " (2, 3, 1),\n",
       " (2, 3, 4),\n",
       " (2, 4, 1),\n",
       " (2, 4, 3),\n",
       " (3, 4, 1),\n",
       " (3, 4, 2)]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "relevant_permutations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "db8fd33a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialise classes that will generate inputs\n",
    "CS = cs_dipole.CS_dipole(mode='gluon')\n",
    "relevant_inputs = model_inputs.ModelInputsGenerator(relevant_permutations, CS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "790e65ea",
   "metadata": {},
   "source": [
    "If there are two or more gluons in the final state we need to account for spin-correlation effects that have been averaged out in the spin-averaged Catani-Seymour dipoles.\n",
    "\n",
    "**For each pair of gluons in the final state, $(i, j)$, there are $sin(2\\phi_{ij})$ and $cos(2\\phi_{ij})$ terms.**\n",
    "\n",
    "**The total set of dipoles is therefore the Catani-Seymour dipoles and the spin-correlation terms.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f28cd241",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate model inputs\n",
    "if num_jets > 3:\n",
    "    # extra spin-correlation terms\n",
    "    train_phi_terms = model_inputs.calculate_cs_phis(p=X_train, num_jets=num_jets, cast=False)\n",
    "    tests.check_phi_terms(train_phi_terms, num_jets)\n",
    "    \n",
    "    # Catani-Seymour dipoles and recoil factors\n",
    "    # concatenate phi terms with Catani-Seymour dipoles\n",
    "    train_dipoles, train_ys = relevant_inputs.calculate_inputs(\n",
    "        p_array=X_train,\n",
    "        to_concat=[*train_phi_terms]\n",
    "    )\n",
    "    \n",
    "    tests.check_recoil_factors(train_ys, num_jets)\n",
    "    tests.check_all_dipoles(train_dipoles, num_jets)\n",
    "else:\n",
    "    # no phi terms for 3-jet case\n",
    "    train_dipoles, train_ys = relevant_inputs.calculate_inputs(p_array=X_train)\n",
    "    tests.check_all_dipoles(train_dipoles, num_jets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "58bbdccf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set scales of problem\n",
    "pred_scale = np.min(Y_train)\n",
    "dipole_scale = np.mean(train_dipoles)\n",
    "coef_scale = pred_scale / dipole_scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0fdba134",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X shape = (500000, 6, 4), Y shape = (500000,), dipoles shape = (500000, 12), ys shape = (500000, 10)\n",
      "pred_scale = 5.5714033998247135e-12, dipole_scale = 0.0006503360422056045, coef_scale = 8.566960829864797e-09\n"
     ]
    }
   ],
   "source": [
    "print(f\"X shape = {X_train.shape}, Y shape = {Y_train.shape}, dipoles shape = {train_dipoles.shape}, ys shape = {train_ys.shape}\")\n",
    "print(f\"pred_scale = {pred_scale}, dipole_scale = {dipole_scale}, coef_scale = {coef_scale}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0489c3bb",
   "metadata": {},
   "source": [
    "Now we have all the inputs generated we feed it to our emulator."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f31aa27",
   "metadata": {},
   "source": [
    "# Constructing neural network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7c33a25",
   "metadata": {},
   "source": [
    "## Define neural network hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8ab5040e",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 4096\n",
    "lr = 0.001\n",
    "# min_delta for EarlyStopping, should go smaller for lower multiplicity as higher accuracy there\n",
    "min_delta = 1E-6\n",
    "# J is tuned manually such that f_pen << mse\n",
    "J = 1E6"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e18dabc1",
   "metadata": {},
   "source": [
    "## Initialise class for building dipole NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2acf1b2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "dipole_model = DipoleModel(\n",
    "    num_jets=num_jets,\n",
    "    permutations=relevant_permutations,\n",
    "    X=X_train,\n",
    "    Y=Y_train,\n",
    "    recoil_factors=train_ys,\n",
    "    dipoles=train_dipoles,\n",
    "    pred_scale=pred_scale,\n",
    "    coef_scale=coef_scale,\n",
    "    J=J\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26b2ff96",
   "metadata": {},
   "source": [
    "## Preprocesssing inputs involves standardising inputs and outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d0f7000d",
   "metadata": {},
   "outputs": [],
   "source": [
    "dipole_model.preprocess_inputs()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04dc269b",
   "metadata": {},
   "source": [
    "All scalers can be accessed through the DipoleModel class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c3f3be33",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Variable 'mean:0' shape=(1,) dtype=float32, numpy=array([4.2759204], dtype=float32)>,\n",
       " <tf.Variable 'variance:0' shape=(1,) dtype=float32, numpy=array([3.0926394], dtype=float32)>,\n",
       " <tf.Variable 'count:0' shape=() dtype=int64, numpy=500000>]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dipole_model.y_scaler.variables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f2790f2",
   "metadata": {},
   "source": [
    "## Building model: create densely connected neural network with custom loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "80dfac09",
   "metadata": {},
   "outputs": [],
   "source": [
    "dipole_model.build_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d3216a9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"dipole_4_jets\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, 4, 4)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "normalization (Normalization)   (None, 4, 4)         9           input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            [(None, 10)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "flatten (Flatten)               (None, 16)           0           normalization[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "normalization_1 (Normalization) (None, 10)           21          input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "concatenate (Concatenate)       (None, 26)           0           flatten[0][0]                    \n",
      "                                                                 normalization_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 64)           1728        concatenate[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 128)          8320        dense[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 256)          33024       dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 512)          131584      dense_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_4 (Dense)                 (None, 768)          393984      dense_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_5 (Dense)                 (None, 386)          296834      dense_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_6 (Dense)                 (None, 128)          49536       dense_5[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_7 (Dense)                 (None, 64)           8256        dense_6[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_8 (Dense)                 (None, 12)           780         dense_7[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "tf.math.sinh (TFOpLambda)       (None, 12)           0           dense_8[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "input_4 (InputLayer)            [(None, 12)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_3 (InputLayer)            [(None, 1)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "tf.math.multiply (TFOpLambda)   (None, 12)           0           tf.math.sinh[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "tf.math.multiply_1 (TFOpLambda) (None, 12)           0           tf.math.multiply[0][0]           \n",
      "                                                                 input_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "tf.math.reduce_sum (TFOpLambda) (None,)              0           tf.math.multiply_1[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "tf.math.truediv (TFOpLambda)    (None,)              0           tf.math.reduce_sum[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "tf.math.multiply_2 (TFOpLambda) (None, 12)           0           tf.math.multiply[0][0]           \n",
      "                                                                 input_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "tf.math.asinh (TFOpLambda)      (None,)              0           tf.math.truediv[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "tf.math.abs (TFOpLambda)        (None, 12)           0           tf.math.multiply_2[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "tf.math.pow (TFOpLambda)        (None, 12)           0           input_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "normalization_2 (Normalization) (None, 1)            3           tf.math.asinh[0][0]              \n",
      "                                                                 tf.math.asinh_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "tf.math.multiply_3 (TFOpLambda) (None, 12)           0           tf.math.abs[0][0]                \n",
      "                                                                 tf.math.pow[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "tf.convert_to_tensor (TFOpLambd (None, 1)            0           normalization_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "tf.cast (TFOpLambda)            (None, 1)            0           input_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "tf.math.reduce_sum_2 (TFOpLambd (None,)              0           tf.math.multiply_3[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "tf.math.reduce_sum_1 (TFOpLambd (None,)              0           tf.math.pow[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "tf.math.squared_difference (TFO (None, 1)            0           tf.convert_to_tensor[0][0]       \n",
      "                                                                 tf.cast[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "tf.math.truediv_1 (TFOpLambda)  (None,)              0           tf.math.reduce_sum_2[0][0]       \n",
      "                                                                 tf.math.reduce_sum_1[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "tf.math.reduce_mean (TFOpLambda (None,)              0           tf.math.squared_difference[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "tf.math.multiply_4 (TFOpLambda) (None,)              0           tf.math.truediv_1[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "tf.__operators__.add (TFOpLambd (None,)              0           tf.math.reduce_mean[0][0]        \n",
      "                                                                 tf.math.multiply_4[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "add_loss (AddLoss)              (None,)              0           tf.__operators__.add[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "tf.math.multiply_5 (TFOpLambda) (None, 12)           0           tf.math.multiply[0][0]           \n",
      "                                                                 input_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "tf.math.reduce_sum_3 (TFOpLambd (None,)              0           tf.math.multiply_5[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "tf.math.truediv_2 (TFOpLambda)  (None,)              0           tf.math.reduce_sum_3[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "tf.math.asinh_1 (TFOpLambda)    (None,)              0           tf.math.truediv_2[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "tf.convert_to_tensor_1 (TFOpLam (None, 1)            0           normalization_2[1][0]            \n",
      "__________________________________________________________________________________________________\n",
      "tf.cast_1 (TFOpLambda)          (None, 1)            0           input_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "tf.math.squared_difference_1 (T (None, 1)            0           tf.convert_to_tensor_1[0][0]     \n",
      "                                                                 tf.cast_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "tf.math.reduce_mean_1 (TFOpLamb (None,)              0           tf.math.squared_difference_1[0][0\n",
      "__________________________________________________________________________________________________\n",
      "add_metric (AddMetric)          (None,)              0           tf.math.reduce_mean_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "tf.math.multiply_6 (TFOpLambda) (None, 12)           0           tf.math.multiply[0][0]           \n",
      "                                                                 input_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "tf.math.abs_1 (TFOpLambda)      (None, 12)           0           tf.math.multiply_6[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "tf.math.pow_1 (TFOpLambda)      (None, 12)           0           input_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "tf.math.multiply_7 (TFOpLambda) (None, 12)           0           tf.math.abs_1[0][0]              \n",
      "                                                                 tf.math.pow_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "tf.math.reduce_sum_5 (TFOpLambd (None,)              0           tf.math.multiply_7[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "tf.math.reduce_sum_4 (TFOpLambd (None,)              0           tf.math.pow_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "tf.math.truediv_3 (TFOpLambda)  (None,)              0           tf.math.reduce_sum_5[0][0]       \n",
      "                                                                 tf.math.reduce_sum_4[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "tf.math.multiply_8 (TFOpLambda) (None,)              0           tf.math.truediv_3[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "add_metric_1 (AddMetric)        (None,)              0           tf.math.multiply_8[0][0]         \n",
      "==================================================================================================\n",
      "Total params: 924,079\n",
      "Trainable params: 924,046\n",
      "Non-trainable params: 33\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "dipole_model.model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c8cc61b",
   "metadata": {},
   "source": [
    "# Train model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b557f3e4",
   "metadata": {},
   "source": [
    "The following wrapper functions will train the network until the desired min_delta is reached in the validation loss.\n",
    "\n",
    "It is possible to terminate training prematurely by interrupting the Jupyter kernel. The model will have the most up-to-date weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f9905192",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10000\n",
      "98/98 [==============================] - 5s 42ms/step - loss: 0.3307 - mse: 0.3307 - f_pen: 7.0067e-07 - val_loss: 0.0653 - val_mse: 0.0653 - val_f_pen: 5.9105e-07\n",
      "Epoch 2/10000\n",
      "98/98 [==============================] - 4s 38ms/step - loss: 0.0362 - mse: 0.0362 - f_pen: 5.3295e-07 - val_loss: 0.0192 - val_mse: 0.0192 - val_f_pen: 5.1451e-07\n",
      "Epoch 3/10000\n",
      "98/98 [==============================] - 4s 39ms/step - loss: 0.0131 - mse: 0.0131 - f_pen: 5.0905e-07 - val_loss: 0.0093 - val_mse: 0.0093 - val_f_pen: 5.0474e-07\n",
      "Epoch 4/10000\n",
      "98/98 [==============================] - 4s 38ms/step - loss: 0.0074 - mse: 0.0074 - f_pen: 5.0117e-07 - val_loss: 0.0060 - val_mse: 0.0060 - val_f_pen: 4.9823e-07\n",
      "Epoch 5/10000\n",
      "98/98 [==============================] - 4s 39ms/step - loss: 0.0051 - mse: 0.0051 - f_pen: 5.0083e-07 - val_loss: 0.0045 - val_mse: 0.0045 - val_f_pen: 5.0004e-07\n",
      "Epoch 6/10000\n",
      "98/98 [==============================] - 4s 38ms/step - loss: 0.0039 - mse: 0.0039 - f_pen: 5.0298e-07 - val_loss: 0.0034 - val_mse: 0.0034 - val_f_pen: 5.0481e-07\n",
      "Epoch 7/10000\n",
      "98/98 [==============================] - 4s 37ms/step - loss: 0.0033 - mse: 0.0033 - f_pen: 5.0742e-07 - val_loss: 0.0029 - val_mse: 0.0029 - val_f_pen: 5.1135e-07\n",
      "Epoch 8/10000\n",
      "98/98 [==============================] - 4s 37ms/step - loss: 0.0025 - mse: 0.0025 - f_pen: 5.1112e-07 - val_loss: 0.0024 - val_mse: 0.0024 - val_f_pen: 5.1821e-07\n",
      "Epoch 9/10000\n",
      "98/98 [==============================] - 4s 37ms/step - loss: 0.0024 - mse: 0.0024 - f_pen: 5.1661e-07 - val_loss: 0.0019 - val_mse: 0.0019 - val_f_pen: 5.2048e-07\n",
      "Epoch 10/10000\n",
      "98/98 [==============================] - 4s 38ms/step - loss: 0.0019 - mse: 0.0019 - f_pen: 5.2192e-07 - val_loss: 0.0017 - val_mse: 0.0017 - val_f_pen: 5.2365e-07\n",
      "Epoch 11/10000\n",
      "98/98 [==============================] - 4s 37ms/step - loss: 0.0017 - mse: 0.0017 - f_pen: 5.2683e-07 - val_loss: 0.0015 - val_mse: 0.0015 - val_f_pen: 5.3216e-07\n",
      "Epoch 12/10000\n",
      "98/98 [==============================] - 4s 37ms/step - loss: 0.0016 - mse: 0.0016 - f_pen: 5.3131e-07 - val_loss: 0.0013 - val_mse: 0.0013 - val_f_pen: 5.3515e-07\n",
      "Epoch 13/10000\n",
      "98/98 [==============================] - 4s 37ms/step - loss: 0.0014 - mse: 0.0014 - f_pen: 5.3596e-07 - val_loss: 0.0013 - val_mse: 0.0013 - val_f_pen: 5.3795e-07\n",
      "Epoch 14/10000\n",
      "98/98 [==============================] - 4s 37ms/step - loss: 0.0012 - mse: 0.0012 - f_pen: 5.3963e-07 - val_loss: 0.0011 - val_mse: 0.0011 - val_f_pen: 5.4595e-07\n",
      "Epoch 15/10000\n",
      "98/98 [==============================] - 4s 37ms/step - loss: 0.0012 - mse: 0.0012 - f_pen: 5.4528e-07 - val_loss: 9.3644e-04 - val_mse: 9.3589e-04 - val_f_pen: 5.4926e-07\n",
      "Epoch 16/10000\n",
      "98/98 [==============================] - 4s 37ms/step - loss: 0.0012 - mse: 0.0011 - f_pen: 5.4967e-07 - val_loss: 8.7454e-04 - val_mse: 8.7399e-04 - val_f_pen: 5.4988e-07\n",
      "Epoch 17/10000\n",
      "98/98 [==============================] - 4s 36ms/step - loss: 9.3773e-04 - mse: 9.3718e-04 - f_pen: 5.5344e-07 - val_loss: 8.3829e-04 - val_mse: 8.3774e-04 - val_f_pen: 5.5329e-07\n",
      "Epoch 18/10000\n",
      "98/98 [==============================] - 4s 36ms/step - loss: 0.0010 - mse: 0.0010 - f_pen: 5.5832e-07 - val_loss: 9.6455e-04 - val_mse: 9.6398e-04 - val_f_pen: 5.7298e-07\n",
      "Epoch 19/10000\n",
      "98/98 [==============================] - 4s 36ms/step - loss: 9.3039e-04 - mse: 9.2983e-04 - f_pen: 5.6292e-07 - val_loss: 7.0446e-04 - val_mse: 7.0389e-04 - val_f_pen: 5.6651e-07\n",
      "Epoch 20/10000\n",
      "98/98 [==============================] - 4s 37ms/step - loss: 7.7520e-04 - mse: 7.7463e-04 - f_pen: 5.6684e-07 - val_loss: 6.5816e-04 - val_mse: 6.5759e-04 - val_f_pen: 5.7152e-07\n",
      "Epoch 21/10000\n",
      "98/98 [==============================] - 4s 37ms/step - loss: 8.6487e-04 - mse: 8.6430e-04 - f_pen: 5.7003e-07 - val_loss: 0.0010 - val_mse: 0.0010 - val_f_pen: 5.7020e-07\n",
      "Epoch 22/10000\n",
      "98/98 [==============================] - 4s 37ms/step - loss: 0.0012 - mse: 0.0012 - f_pen: 5.7243e-07 - val_loss: 6.2313e-04 - val_mse: 6.2255e-04 - val_f_pen: 5.7758e-07\n",
      "Epoch 23/10000\n",
      "98/98 [==============================] - 4s 37ms/step - loss: 6.1192e-04 - mse: 6.1134e-04 - f_pen: 5.7601e-07 - val_loss: 6.7386e-04 - val_mse: 6.7327e-04 - val_f_pen: 5.8895e-07\n",
      "Epoch 24/10000\n",
      "98/98 [==============================] - 4s 38ms/step - loss: 6.2239e-04 - mse: 6.2180e-04 - f_pen: 5.8285e-07 - val_loss: 5.9419e-04 - val_mse: 5.9360e-04 - val_f_pen: 5.8437e-07\n",
      "Epoch 25/10000\n",
      "98/98 [==============================] - 4s 37ms/step - loss: 5.8265e-04 - mse: 5.8206e-04 - f_pen: 5.8868e-07 - val_loss: 4.8519e-04 - val_mse: 4.8459e-04 - val_f_pen: 6.0007e-07\n",
      "Epoch 26/10000\n",
      "98/98 [==============================] - 4s 37ms/step - loss: 7.0295e-04 - mse: 7.0236e-04 - f_pen: 5.9325e-07 - val_loss: 5.3839e-04 - val_mse: 5.3780e-04 - val_f_pen: 5.9255e-07\n",
      "Epoch 27/10000\n",
      "98/98 [==============================] - 4s 38ms/step - loss: 6.1134e-04 - mse: 6.1074e-04 - f_pen: 5.9753e-07 - val_loss: 4.5004e-04 - val_mse: 4.4944e-04 - val_f_pen: 5.9925e-07\n",
      "Epoch 28/10000\n",
      "98/98 [==============================] - 4s 37ms/step - loss: 6.3932e-04 - mse: 6.3872e-04 - f_pen: 6.0345e-07 - val_loss: 4.2252e-04 - val_mse: 4.2191e-04 - val_f_pen: 6.1073e-07\n",
      "Epoch 29/10000\n",
      "98/98 [==============================] - 4s 38ms/step - loss: 8.5017e-04 - mse: 8.4956e-04 - f_pen: 6.0751e-07 - val_loss: 5.2710e-04 - val_mse: 5.2650e-04 - val_f_pen: 6.0272e-07\n",
      "Epoch 30/10000\n",
      "98/98 [==============================] - 4s 36ms/step - loss: 4.0073e-04 - mse: 4.0012e-04 - f_pen: 6.0921e-07 - val_loss: 3.5593e-04 - val_mse: 3.5531e-04 - val_f_pen: 6.1679e-07\n",
      "Epoch 31/10000\n",
      "98/98 [==============================] - 4s 37ms/step - loss: 5.0414e-04 - mse: 5.0352e-04 - f_pen: 6.1630e-07 - val_loss: 3.5312e-04 - val_mse: 3.5250e-04 - val_f_pen: 6.2259e-07\n",
      "Epoch 32/10000\n",
      "98/98 [==============================] - 4s 36ms/step - loss: 5.6310e-04 - mse: 5.6248e-04 - f_pen: 6.1927e-07 - val_loss: 3.5331e-04 - val_mse: 3.5269e-04 - val_f_pen: 6.2260e-07\n",
      "Epoch 33/10000\n",
      "98/98 [==============================] - 4s 36ms/step - loss: 6.0815e-04 - mse: 6.0753e-04 - f_pen: 6.2341e-07 - val_loss: 4.3577e-04 - val_mse: 4.3514e-04 - val_f_pen: 6.2388e-07\n",
      "Epoch 34/10000\n",
      "98/98 [==============================] - 4s 38ms/step - loss: 4.1737e-04 - mse: 4.1675e-04 - f_pen: 6.2692e-07 - val_loss: 3.2168e-04 - val_mse: 3.2105e-04 - val_f_pen: 6.3494e-07\n",
      "Epoch 35/10000\n",
      "98/98 [==============================] - 4s 38ms/step - loss: 5.2296e-04 - mse: 5.2233e-04 - f_pen: 6.3156e-07 - val_loss: 3.1212e-04 - val_mse: 3.1148e-04 - val_f_pen: 6.3929e-07\n",
      "Epoch 36/10000\n",
      "98/98 [==============================] - 4s 38ms/step - loss: 5.2418e-04 - mse: 5.2355e-04 - f_pen: 6.3529e-07 - val_loss: 3.1119e-04 - val_mse: 3.1055e-04 - val_f_pen: 6.3773e-07\n",
      "Epoch 37/10000\n",
      "98/98 [==============================] - 4s 37ms/step - loss: 4.9265e-04 - mse: 4.9201e-04 - f_pen: 6.3903e-07 - val_loss: 4.9147e-04 - val_mse: 4.9082e-04 - val_f_pen: 6.4232e-07\n",
      "Epoch 38/10000\n",
      "98/98 [==============================] - 4s 37ms/step - loss: 4.1665e-04 - mse: 4.1600e-04 - f_pen: 6.4381e-07 - val_loss: 2.6939e-04 - val_mse: 2.6874e-04 - val_f_pen: 6.5116e-07\n",
      "Epoch 39/10000\n",
      "98/98 [==============================] - 4s 37ms/step - loss: 4.5510e-04 - mse: 4.5445e-04 - f_pen: 6.4723e-07 - val_loss: 2.9379e-04 - val_mse: 2.9314e-04 - val_f_pen: 6.4688e-07\n",
      "Epoch 40/10000\n",
      "98/98 [==============================] - 4s 37ms/step - loss: 5.0809e-04 - mse: 5.0744e-04 - f_pen: 6.4986e-07 - val_loss: 2.4393e-04 - val_mse: 2.4327e-04 - val_f_pen: 6.6003e-07\n",
      "Epoch 41/10000\n",
      "98/98 [==============================] - 4s 37ms/step - loss: 4.7790e-04 - mse: 4.7725e-04 - f_pen: 6.5455e-07 - val_loss: 3.0803e-04 - val_mse: 3.0738e-04 - val_f_pen: 6.5701e-07\n",
      "Epoch 42/10000\n",
      "98/98 [==============================] - 4s 38ms/step - loss: 4.6120e-04 - mse: 4.6054e-04 - f_pen: 6.5824e-07 - val_loss: 3.6660e-04 - val_mse: 3.6593e-04 - val_f_pen: 6.6956e-07\n",
      "Epoch 43/10000\n",
      "98/98 [==============================] - 4s 37ms/step - loss: 3.3969e-04 - mse: 3.3903e-04 - f_pen: 6.5988e-07 - val_loss: 2.3363e-04 - val_mse: 2.3297e-04 - val_f_pen: 6.6470e-07\n",
      "Epoch 44/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "98/98 [==============================] - 4s 38ms/step - loss: 5.4647e-04 - mse: 5.4580e-04 - f_pen: 6.6515e-07 - val_loss: 2.3194e-04 - val_mse: 2.3127e-04 - val_f_pen: 6.7479e-07\n",
      "Epoch 45/10000\n",
      "98/98 [==============================] - 4s 37ms/step - loss: 3.5979e-04 - mse: 3.5912e-04 - f_pen: 6.7103e-07 - val_loss: 2.5975e-04 - val_mse: 2.5907e-04 - val_f_pen: 6.7920e-07\n",
      "Epoch 46/10000\n",
      "98/98 [==============================] - 4s 37ms/step - loss: 4.2194e-04 - mse: 4.2127e-04 - f_pen: 6.7597e-07 - val_loss: 2.5337e-04 - val_mse: 2.5270e-04 - val_f_pen: 6.7551e-07\n",
      "Epoch 47/10000\n",
      "98/98 [==============================] - 4s 37ms/step - loss: 3.8097e-04 - mse: 3.8029e-04 - f_pen: 6.7990e-07 - val_loss: 6.4405e-04 - val_mse: 6.4335e-04 - val_f_pen: 6.9850e-07\n",
      "Epoch 48/10000\n",
      "98/98 [==============================] - 4s 37ms/step - loss: 9.1292e-04 - mse: 9.1223e-04 - f_pen: 6.8792e-07 - val_loss: 2.1735e-04 - val_mse: 2.1665e-04 - val_f_pen: 6.9731e-07\n",
      "Epoch 49/10000\n",
      "98/98 [==============================] - 4s 37ms/step - loss: 2.0996e-04 - mse: 2.0926e-04 - f_pen: 6.9457e-07 - val_loss: 2.3990e-04 - val_mse: 2.3920e-04 - val_f_pen: 7.0097e-07\n",
      "Epoch 50/10000\n",
      "98/98 [==============================] - 4s 37ms/step - loss: 2.7114e-04 - mse: 2.7044e-04 - f_pen: 6.9846e-07 - val_loss: 1.7351e-04 - val_mse: 1.7280e-04 - val_f_pen: 7.0667e-07\n",
      "Epoch 51/10000\n",
      "98/98 [==============================] - 4s 36ms/step - loss: 4.5806e-04 - mse: 4.5735e-04 - f_pen: 7.0129e-07 - val_loss: 2.5810e-04 - val_mse: 2.5738e-04 - val_f_pen: 7.1460e-07\n",
      "Epoch 52/10000\n",
      "98/98 [==============================] - 4s 37ms/step - loss: 2.7264e-04 - mse: 2.7193e-04 - f_pen: 7.0671e-07 - val_loss: 1.9763e-04 - val_mse: 1.9691e-04 - val_f_pen: 7.1408e-07\n",
      "Epoch 53/10000\n",
      "98/98 [==============================] - 4s 38ms/step - loss: 4.3898e-04 - mse: 4.3827e-04 - f_pen: 7.1007e-07 - val_loss: 2.0582e-04 - val_mse: 2.0510e-04 - val_f_pen: 7.2246e-07\n",
      "Epoch 54/10000\n",
      "98/98 [==============================] - 4s 37ms/step - loss: 2.9528e-04 - mse: 2.9457e-04 - f_pen: 7.1465e-07 - val_loss: 1.6354e-04 - val_mse: 1.6282e-04 - val_f_pen: 7.2249e-07\n",
      "Epoch 55/10000\n",
      "98/98 [==============================] - 4s 39ms/step - loss: 4.4855e-04 - mse: 4.4783e-04 - f_pen: 7.1994e-07 - val_loss: 1.9482e-04 - val_mse: 1.9409e-04 - val_f_pen: 7.2780e-07\n",
      "Epoch 56/10000\n",
      "98/98 [==============================] - 4s 37ms/step - loss: 4.2635e-04 - mse: 4.2563e-04 - f_pen: 7.2544e-07 - val_loss: 4.7678e-04 - val_mse: 4.7606e-04 - val_f_pen: 7.2098e-07\n",
      "Epoch 57/10000\n",
      "98/98 [==============================] - 4s 37ms/step - loss: 2.7908e-04 - mse: 2.7834e-04 - f_pen: 7.3435e-07 - val_loss: 1.6276e-04 - val_mse: 1.6202e-04 - val_f_pen: 7.4150e-07\n",
      "Epoch 58/10000\n",
      "98/98 [==============================] - 4s 37ms/step - loss: 3.5128e-04 - mse: 3.5054e-04 - f_pen: 7.3950e-07 - val_loss: 1.7305e-04 - val_mse: 1.7230e-04 - val_f_pen: 7.4943e-07\n",
      "Epoch 59/10000\n",
      "98/98 [==============================] - 4s 36ms/step - loss: 3.6092e-04 - mse: 3.6017e-04 - f_pen: 7.4356e-07 - val_loss: 1.6304e-04 - val_mse: 1.6229e-04 - val_f_pen: 7.5251e-07\n",
      "Epoch 60/10000\n",
      "98/98 [==============================] - 4s 37ms/step - loss: 3.7206e-04 - mse: 3.7131e-04 - f_pen: 7.4806e-07 - val_loss: 1.4607e-04 - val_mse: 1.4531e-04 - val_f_pen: 7.5592e-07\n",
      "Epoch 61/10000\n",
      "98/98 [==============================] - 4s 37ms/step - loss: 3.5732e-04 - mse: 3.5657e-04 - f_pen: 7.5385e-07 - val_loss: 2.1731e-04 - val_mse: 2.1654e-04 - val_f_pen: 7.6762e-07\n",
      "Epoch 62/10000\n",
      "98/98 [==============================] - 4s 38ms/step - loss: 3.6058e-04 - mse: 3.5982e-04 - f_pen: 7.5858e-07 - val_loss: 3.6464e-04 - val_mse: 3.6388e-04 - val_f_pen: 7.5850e-07\n",
      "Epoch 63/10000\n",
      "98/98 [==============================] - 4s 38ms/step - loss: 4.3884e-04 - mse: 4.3807e-04 - f_pen: 7.7045e-07 - val_loss: 1.6190e-04 - val_mse: 1.6112e-04 - val_f_pen: 7.8116e-07\n",
      "Epoch 64/10000\n",
      "98/98 [==============================] - 4s 38ms/step - loss: 2.6828e-04 - mse: 2.6750e-04 - f_pen: 7.7455e-07 - val_loss: 3.6886e-04 - val_mse: 3.6809e-04 - val_f_pen: 7.7033e-07\n",
      "Epoch 65/10000\n",
      "98/98 [==============================] - 4s 38ms/step - loss: 3.6196e-04 - mse: 3.6118e-04 - f_pen: 7.8007e-07 - val_loss: 1.4574e-04 - val_mse: 1.4494e-04 - val_f_pen: 7.9687e-07\n",
      "Epoch 66/10000\n",
      "98/98 [==============================] - 4s 38ms/step - loss: 3.7355e-04 - mse: 3.7276e-04 - f_pen: 7.8241e-07 - val_loss: 3.3661e-04 - val_mse: 3.3582e-04 - val_f_pen: 7.8306e-07\n",
      "Epoch 67/10000\n",
      "98/98 [==============================] - 4s 38ms/step - loss: 3.2166e-04 - mse: 3.2087e-04 - f_pen: 7.9164e-07 - val_loss: 1.9554e-04 - val_mse: 1.9474e-04 - val_f_pen: 7.9934e-07\n",
      "Epoch 68/10000\n",
      "98/98 [==============================] - 4s 36ms/step - loss: 2.8409e-04 - mse: 2.8329e-04 - f_pen: 7.9907e-07 - val_loss: 2.1731e-04 - val_mse: 2.1649e-04 - val_f_pen: 8.1496e-07\n",
      "Epoch 69/10000\n",
      "98/98 [==============================] - 4s 37ms/step - loss: 7.2543e-04 - mse: 7.2462e-04 - f_pen: 8.0758e-07 - val_loss: 1.3931e-04 - val_mse: 1.3849e-04 - val_f_pen: 8.1619e-07\n",
      "Epoch 70/10000\n",
      "98/98 [==============================] - 4s 37ms/step - loss: 1.4919e-04 - mse: 1.4838e-04 - f_pen: 8.1246e-07 - val_loss: 1.2119e-04 - val_mse: 1.2037e-04 - val_f_pen: 8.1936e-07\n",
      "Epoch 71/10000\n",
      "98/98 [==============================] - 4s 37ms/step - loss: 2.7031e-04 - mse: 2.6950e-04 - f_pen: 8.1387e-07 - val_loss: 3.7008e-04 - val_mse: 3.6927e-04 - val_f_pen: 8.0224e-07\n",
      "Epoch 72/10000\n",
      "98/98 [==============================] - 4s 36ms/step - loss: 8.3476e-04 - mse: 8.3393e-04 - f_pen: 8.3100e-07 - val_loss: 1.3886e-04 - val_mse: 1.3801e-04 - val_f_pen: 8.4686e-07\n",
      "Epoch 73/10000\n",
      "98/98 [==============================] - 4s 37ms/step - loss: 1.2676e-04 - mse: 1.2592e-04 - f_pen: 8.3551e-07 - val_loss: 1.1305e-04 - val_mse: 1.1221e-04 - val_f_pen: 8.4103e-07\n",
      "Epoch 74/10000\n",
      "98/98 [==============================] - 4s 37ms/step - loss: 1.9642e-04 - mse: 1.9559e-04 - f_pen: 8.3745e-07 - val_loss: 1.6199e-04 - val_mse: 1.6115e-04 - val_f_pen: 8.4108e-07\n",
      "Epoch 75/10000\n",
      "98/98 [==============================] - 4s 37ms/step - loss: 3.1386e-04 - mse: 3.1302e-04 - f_pen: 8.4417e-07 - val_loss: 1.5300e-04 - val_mse: 1.5215e-04 - val_f_pen: 8.5039e-07\n",
      "Epoch 76/10000\n",
      "98/98 [==============================] - 4s 37ms/step - loss: 4.6465e-04 - mse: 4.6379e-04 - f_pen: 8.5687e-07 - val_loss: 1.2580e-04 - val_mse: 1.2493e-04 - val_f_pen: 8.6445e-07\n",
      "Epoch 77/10000\n",
      "98/98 [==============================] - 4s 38ms/step - loss: 1.8475e-04 - mse: 1.8389e-04 - f_pen: 8.5818e-07 - val_loss: 3.2355e-04 - val_mse: 3.2270e-04 - val_f_pen: 8.5197e-07\n",
      "Epoch 78/10000\n",
      "98/98 [==============================] - 4s 38ms/step - loss: 2.8597e-04 - mse: 2.8511e-04 - f_pen: 8.6130e-07 - val_loss: 1.2684e-04 - val_mse: 1.2597e-04 - val_f_pen: 8.7186e-07\n",
      "Epoch 79/10000\n",
      "98/98 [==============================] - 4s 37ms/step - loss: 2.9196e-04 - mse: 2.9110e-04 - f_pen: 8.6410e-07 - val_loss: 2.7053e-04 - val_mse: 2.6967e-04 - val_f_pen: 8.5807e-07\n",
      "Epoch 80/10000\n",
      "98/98 [==============================] - 4s 37ms/step - loss: 2.7852e-04 - mse: 2.7765e-04 - f_pen: 8.6883e-07 - val_loss: 3.0453e-04 - val_mse: 3.0367e-04 - val_f_pen: 8.5941e-07\n",
      "Epoch 81/10000\n",
      "98/98 [==============================] - 4s 37ms/step - loss: 6.1933e-04 - mse: 6.1845e-04 - f_pen: 8.8375e-07 - val_loss: 1.3279e-04 - val_mse: 1.3189e-04 - val_f_pen: 8.9571e-07\n",
      "Epoch 82/10000\n",
      "98/98 [==============================] - 4s 36ms/step - loss: 1.3538e-04 - mse: 1.3449e-04 - f_pen: 8.9037e-07 - val_loss: 1.2842e-04 - val_mse: 1.2752e-04 - val_f_pen: 9.0061e-07\n",
      "Epoch 83/10000\n",
      "98/98 [==============================] - 4s 36ms/step - loss: 5.3184e-04 - mse: 5.3095e-04 - f_pen: 8.9299e-07 - val_loss: 2.3715e-04 - val_mse: 2.3623e-04 - val_f_pen: 9.1120e-07\n",
      "Epoch 84/10000\n",
      "98/98 [==============================] - 4s 36ms/step - loss: 1.7214e-04 - mse: 1.7123e-04 - f_pen: 9.0562e-07 - val_loss: 1.5242e-04 - val_mse: 1.5151e-04 - val_f_pen: 9.1142e-07\n",
      "Epoch 85/10000\n",
      "98/98 [==============================] - 4s 37ms/step - loss: 2.8525e-04 - mse: 2.8434e-04 - f_pen: 9.0795e-07 - val_loss: 1.9220e-04 - val_mse: 1.9128e-04 - val_f_pen: 9.1936e-07\n",
      "Epoch 86/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "98/98 [==============================] - 4s 37ms/step - loss: 1.8634e-04 - mse: 1.8542e-04 - f_pen: 9.1311e-07 - val_loss: 1.0823e-04 - val_mse: 1.0731e-04 - val_f_pen: 9.2320e-07\n",
      "Epoch 87/10000\n",
      "98/98 [==============================] - 4s 37ms/step - loss: 3.7206e-04 - mse: 3.7114e-04 - f_pen: 9.2029e-07 - val_loss: 9.8471e-05 - val_mse: 9.7542e-05 - val_f_pen: 9.2840e-07\n",
      "Epoch 88/10000\n",
      "98/98 [==============================] - 4s 38ms/step - loss: 2.1699e-04 - mse: 2.1607e-04 - f_pen: 9.2392e-07 - val_loss: 1.9440e-04 - val_mse: 1.9348e-04 - val_f_pen: 9.2390e-07\n",
      "Epoch 89/10000\n",
      "98/98 [==============================] - 4s 37ms/step - loss: 4.7279e-04 - mse: 4.7185e-04 - f_pen: 9.3545e-07 - val_loss: 1.3128e-04 - val_mse: 1.3034e-04 - val_f_pen: 9.4565e-07\n",
      "Epoch 90/10000\n",
      "98/98 [==============================] - 4s 38ms/step - loss: 2.2926e-04 - mse: 2.2832e-04 - f_pen: 9.3803e-07 - val_loss: 1.9425e-04 - val_mse: 1.9330e-04 - val_f_pen: 9.4984e-07\n",
      "Epoch 91/10000\n",
      "98/98 [==============================] - 4s 37ms/step - loss: 1.8969e-04 - mse: 1.8874e-04 - f_pen: 9.4318e-07 - val_loss: 2.0401e-04 - val_mse: 2.0306e-04 - val_f_pen: 9.4938e-07\n",
      "Epoch 92/10000\n",
      "98/98 [==============================] - 4s 38ms/step - loss: 3.2694e-04 - mse: 3.2599e-04 - f_pen: 9.5063e-07 - val_loss: 2.4156e-04 - val_mse: 2.4061e-04 - val_f_pen: 9.5089e-07\n",
      "Epoch 93/10000\n",
      "98/98 [==============================] - 4s 37ms/step - loss: 2.8807e-04 - mse: 2.8712e-04 - f_pen: 9.5613e-07 - val_loss: 1.0751e-04 - val_mse: 1.0655e-04 - val_f_pen: 9.6502e-07\n",
      "Epoch 94/10000\n",
      "98/98 [==============================] - 4s 38ms/step - loss: 2.6071e-04 - mse: 2.5974e-04 - f_pen: 9.6201e-07 - val_loss: 1.9542e-04 - val_mse: 1.9445e-04 - val_f_pen: 9.6984e-07\n",
      "Epoch 95/10000\n",
      "98/98 [==============================] - 4s 37ms/step - loss: 2.3745e-04 - mse: 2.3649e-04 - f_pen: 9.6727e-07 - val_loss: 1.8256e-04 - val_mse: 1.8158e-04 - val_f_pen: 9.8278e-07\n",
      "Epoch 96/10000\n",
      "98/98 [==============================] - 4s 37ms/step - loss: 3.3474e-04 - mse: 3.3376e-04 - f_pen: 9.7606e-07 - val_loss: 1.1381e-04 - val_mse: 1.1281e-04 - val_f_pen: 9.9236e-07\n",
      "Epoch 97/10000\n",
      "98/98 [==============================] - 4s 38ms/step - loss: 2.5407e-04 - mse: 2.5309e-04 - f_pen: 9.8082e-07 - val_loss: 8.3288e-05 - val_mse: 8.2298e-05 - val_f_pen: 9.9015e-07\n",
      "Epoch 98/10000\n",
      "98/98 [==============================] - 4s 37ms/step - loss: 3.2354e-04 - mse: 3.2256e-04 - f_pen: 9.8217e-07 - val_loss: 1.5900e-04 - val_mse: 1.5800e-04 - val_f_pen: 9.9732e-07\n",
      "Epoch 99/10000\n",
      "98/98 [==============================] - 4s 38ms/step - loss: 2.0030e-04 - mse: 1.9931e-04 - f_pen: 9.8886e-07 - val_loss: 1.0827e-04 - val_mse: 1.0727e-04 - val_f_pen: 1.0000e-06\n",
      "Epoch 100/10000\n",
      "98/98 [==============================] - 4s 38ms/step - loss: 2.7768e-04 - mse: 2.7668e-04 - f_pen: 9.9714e-07 - val_loss: 9.5854e-05 - val_mse: 9.4842e-05 - val_f_pen: 1.0127e-06\n",
      "Epoch 101/10000\n",
      "98/98 [==============================] - 4s 36ms/step - loss: 2.7108e-04 - mse: 2.7008e-04 - f_pen: 1.0028e-06 - val_loss: 1.1492e-04 - val_mse: 1.1390e-04 - val_f_pen: 1.0164e-06\n",
      "Epoch 102/10000\n",
      "98/98 [==============================] - 4s 38ms/step - loss: 2.1724e-04 - mse: 2.1623e-04 - f_pen: 1.0087e-06 - val_loss: 1.2787e-04 - val_mse: 1.2685e-04 - val_f_pen: 1.0220e-06\n",
      "Epoch 103/10000\n",
      "98/98 [==============================] - 4s 37ms/step - loss: 2.8303e-04 - mse: 2.8202e-04 - f_pen: 1.0127e-06 - val_loss: 1.2128e-04 - val_mse: 1.2026e-04 - val_f_pen: 1.0209e-06\n",
      "Epoch 104/10000\n",
      "98/98 [==============================] - 4s 36ms/step - loss: 2.4478e-04 - mse: 2.4376e-04 - f_pen: 1.0211e-06 - val_loss: 1.3502e-04 - val_mse: 1.3398e-04 - val_f_pen: 1.0365e-06\n",
      "Epoch 105/10000\n",
      "98/98 [==============================] - 4s 37ms/step - loss: 4.4333e-04 - mse: 4.4229e-04 - f_pen: 1.0376e-06 - val_loss: 1.0447e-04 - val_mse: 1.0342e-04 - val_f_pen: 1.0468e-06\n",
      "Epoch 106/10000\n",
      "98/98 [==============================] - 3s 35ms/step - loss: 1.3818e-04 - mse: 1.3714e-04 - f_pen: 1.0386e-06 - val_loss: 1.4423e-04 - val_mse: 1.4318e-04 - val_f_pen: 1.0563e-06\n",
      "Epoch 107/10000\n",
      "98/98 [==============================] - 4s 36ms/step - loss: 2.9031e-04 - mse: 2.8926e-04 - f_pen: 1.0438e-06 - val_loss: 1.0362e-04 - val_mse: 1.0256e-04 - val_f_pen: 1.0569e-06\n",
      "Epoch 108/10000\n",
      "98/98 [==============================] - 4s 37ms/step - loss: 2.5552e-04 - mse: 2.5448e-04 - f_pen: 1.0448e-06 - val_loss: 3.4578e-04 - val_mse: 3.4474e-04 - val_f_pen: 1.0391e-06\n",
      "Epoch 109/10000\n",
      "98/98 [==============================] - 4s 37ms/step - loss: 3.2685e-04 - mse: 3.2579e-04 - f_pen: 1.0560e-06 - val_loss: 1.0013e-04 - val_mse: 9.9056e-05 - val_f_pen: 1.0720e-06\n",
      "Epoch 110/10000\n",
      "98/98 [==============================] - 4s 37ms/step - loss: 1.6695e-04 - mse: 1.6590e-04 - f_pen: 1.0574e-06 - val_loss: 2.0740e-04 - val_mse: 2.0633e-04 - val_f_pen: 1.0732e-06\n",
      "Epoch 111/10000\n",
      "98/98 [==============================] - 4s 37ms/step - loss: 2.1551e-04 - mse: 2.1445e-04 - f_pen: 1.0624e-06 - val_loss: 9.7044e-05 - val_mse: 9.5965e-05 - val_f_pen: 1.0789e-06\n",
      "Epoch 112/10000\n",
      "98/98 [==============================] - 4s 37ms/step - loss: 2.0791e-04 - mse: 2.0684e-04 - f_pen: 1.0641e-06 - val_loss: 8.7750e-05 - val_mse: 8.6677e-05 - val_f_pen: 1.0734e-06\n",
      "Epoch 113/10000\n",
      "98/98 [==============================] - 4s 37ms/step - loss: 3.5032e-04 - mse: 3.4925e-04 - f_pen: 1.0720e-06 - val_loss: 1.1648e-04 - val_mse: 1.1540e-04 - val_f_pen: 1.0804e-06\n",
      "Epoch 114/10000\n",
      "98/98 [==============================] - 4s 37ms/step - loss: 2.0559e-04 - mse: 2.0451e-04 - f_pen: 1.0793e-06 - val_loss: 8.1902e-05 - val_mse: 8.0805e-05 - val_f_pen: 1.0973e-06\n",
      "Epoch 115/10000\n",
      "98/98 [==============================] - 4s 37ms/step - loss: 2.3986e-04 - mse: 2.3878e-04 - f_pen: 1.0818e-06 - val_loss: 3.0657e-04 - val_mse: 3.0549e-04 - val_f_pen: 1.0798e-06\n",
      "Epoch 116/10000\n",
      "98/98 [==============================] - 4s 37ms/step - loss: 2.3254e-04 - mse: 2.3145e-04 - f_pen: 1.0872e-06 - val_loss: 1.0901e-04 - val_mse: 1.0790e-04 - val_f_pen: 1.1038e-06\n",
      "Epoch 117/10000\n",
      "98/98 [==============================] - 4s 37ms/step - loss: 1.9208e-04 - mse: 1.9098e-04 - f_pen: 1.0960e-06 - val_loss: 1.6232e-04 - val_mse: 1.6121e-04 - val_f_pen: 1.1075e-06\n",
      "Epoch 118/10000\n",
      "98/98 [==============================] - 4s 38ms/step - loss: 2.1281e-04 - mse: 2.1171e-04 - f_pen: 1.0988e-06 - val_loss: 8.7627e-05 - val_mse: 8.6522e-05 - val_f_pen: 1.1053e-06\n",
      "Epoch 119/10000\n",
      "98/98 [==============================] - 4s 38ms/step - loss: 2.0763e-04 - mse: 2.0653e-04 - f_pen: 1.0988e-06 - val_loss: 7.5729e-05 - val_mse: 7.4610e-05 - val_f_pen: 1.1189e-06\n",
      "Epoch 120/10000\n",
      "98/98 [==============================] - 4s 37ms/step - loss: 2.6738e-04 - mse: 2.6627e-04 - f_pen: 1.1054e-06 - val_loss: 1.7327e-04 - val_mse: 1.7214e-04 - val_f_pen: 1.1284e-06\n",
      "Epoch 121/10000\n",
      "98/98 [==============================] - 4s 37ms/step - loss: 2.2024e-04 - mse: 2.1912e-04 - f_pen: 1.1162e-06 - val_loss: 7.9093e-05 - val_mse: 7.7963e-05 - val_f_pen: 1.1297e-06\n",
      "Epoch 122/10000\n",
      "98/98 [==============================] - 4s 37ms/step - loss: 2.0016e-04 - mse: 1.9904e-04 - f_pen: 1.1198e-06 - val_loss: 1.2496e-04 - val_mse: 1.2383e-04 - val_f_pen: 1.1302e-06\n",
      "Epoch 123/10000\n",
      "98/98 [==============================] - 4s 37ms/step - loss: 3.0145e-04 - mse: 3.0032e-04 - f_pen: 1.1294e-06 - val_loss: 1.6689e-04 - val_mse: 1.6573e-04 - val_f_pen: 1.1529e-06\n",
      "Epoch 124/10000\n",
      "98/98 [==============================] - 4s 37ms/step - loss: 2.1346e-04 - mse: 2.1233e-04 - f_pen: 1.1344e-06 - val_loss: 2.1784e-04 - val_mse: 2.1669e-04 - val_f_pen: 1.1546e-06\n",
      "Epoch 125/10000\n",
      "98/98 [==============================] - 4s 36ms/step - loss: 2.4631e-04 - mse: 2.4517e-04 - f_pen: 1.1411e-06 - val_loss: 7.8983e-05 - val_mse: 7.7829e-05 - val_f_pen: 1.1538e-06\n",
      "Epoch 126/10000\n",
      "98/98 [==============================] - 4s 38ms/step - loss: 1.7278e-04 - mse: 1.7164e-04 - f_pen: 1.1424e-06 - val_loss: 9.3222e-05 - val_mse: 9.2070e-05 - val_f_pen: 1.1521e-06\n",
      "Epoch 127/10000\n",
      "98/98 [==============================] - 4s 37ms/step - loss: 1.6496e-04 - mse: 1.6381e-04 - f_pen: 1.1454e-06 - val_loss: 7.3634e-05 - val_mse: 7.2475e-05 - val_f_pen: 1.1588e-06\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 128/10000\n",
      "98/98 [==============================] - 4s 36ms/step - loss: 2.8130e-04 - mse: 2.8015e-04 - f_pen: 1.1471e-06 - val_loss: 7.5996e-05 - val_mse: 7.4835e-05 - val_f_pen: 1.1612e-06\n",
      "Epoch 129/10000\n",
      "98/98 [==============================] - 4s 38ms/step - loss: 1.5767e-04 - mse: 1.5652e-04 - f_pen: 1.1547e-06 - val_loss: 1.0222e-04 - val_mse: 1.0106e-04 - val_f_pen: 1.1611e-06\n",
      "Epoch 130/10000\n",
      "98/98 [==============================] - 4s 36ms/step - loss: 2.9453e-04 - mse: 2.9337e-04 - f_pen: 1.1596e-06 - val_loss: 7.5743e-05 - val_mse: 7.4569e-05 - val_f_pen: 1.1745e-06\n",
      "Epoch 131/10000\n",
      "98/98 [==============================] - 4s 37ms/step - loss: 1.6025e-04 - mse: 1.5909e-04 - f_pen: 1.1638e-06 - val_loss: 1.0814e-04 - val_mse: 1.0696e-04 - val_f_pen: 1.1749e-06\n",
      "Epoch 132/10000\n",
      "98/98 [==============================] - 4s 37ms/step - loss: 1.5081e-04 - mse: 1.4964e-04 - f_pen: 1.1680e-06 - val_loss: 9.7980e-05 - val_mse: 9.6807e-05 - val_f_pen: 1.1735e-06\n",
      "Epoch 133/10000\n",
      "98/98 [==============================] - 4s 38ms/step - loss: 7.5401e-04 - mse: 7.5282e-04 - f_pen: 1.1901e-06 - val_loss: 9.3627e-05 - val_mse: 9.2408e-05 - val_f_pen: 1.2187e-06\n",
      "Epoch 134/10000\n",
      "98/98 [==============================] - 4s 36ms/step - loss: 9.0212e-05 - mse: 8.9014e-05 - f_pen: 1.1983e-06 - val_loss: 1.7222e-04 - val_mse: 1.7101e-04 - val_f_pen: 1.2155e-06\n",
      "Epoch 135/10000\n",
      "98/98 [==============================] - 4s 37ms/step - loss: 2.6372e-04 - mse: 2.6252e-04 - f_pen: 1.1952e-06 - val_loss: 6.6909e-05 - val_mse: 6.5699e-05 - val_f_pen: 1.2108e-06\n",
      "Epoch 136/10000\n",
      "98/98 [==============================] - 4s 37ms/step - loss: 6.4601e-05 - mse: 6.3405e-05 - f_pen: 1.1966e-06 - val_loss: 6.7016e-05 - val_mse: 6.5808e-05 - val_f_pen: 1.2074e-06\n",
      "Epoch 137/10000\n",
      "98/98 [==============================] - 4s 38ms/step - loss: 1.4629e-04 - mse: 1.4510e-04 - f_pen: 1.1916e-06 - val_loss: 8.1256e-05 - val_mse: 8.0054e-05 - val_f_pen: 1.2025e-06\n",
      "Epoch 138/10000\n",
      "98/98 [==============================] - 4s 37ms/step - loss: 2.6672e-04 - mse: 2.6553e-04 - f_pen: 1.1940e-06 - val_loss: 2.0684e-04 - val_mse: 2.0563e-04 - val_f_pen: 1.2102e-06\n",
      "Epoch 139/10000\n",
      "98/98 [==============================] - 4s 36ms/step - loss: 1.1167e-04 - mse: 1.1047e-04 - f_pen: 1.2010e-06 - val_loss: 9.1260e-05 - val_mse: 9.0041e-05 - val_f_pen: 1.2183e-06\n",
      "Epoch 140/10000\n",
      "98/98 [==============================] - 4s 37ms/step - loss: 2.1505e-04 - mse: 2.1385e-04 - f_pen: 1.2072e-06 - val_loss: 8.0781e-05 - val_mse: 7.9565e-05 - val_f_pen: 1.2161e-06\n",
      "Epoch 141/10000\n",
      "98/98 [==============================] - 4s 37ms/step - loss: 1.6183e-04 - mse: 1.6062e-04 - f_pen: 1.2093e-06 - val_loss: 8.8147e-05 - val_mse: 8.6921e-05 - val_f_pen: 1.2260e-06\n",
      "Epoch 142/10000\n",
      "98/98 [==============================] - 4s 36ms/step - loss: 1.6192e-04 - mse: 1.6071e-04 - f_pen: 1.2103e-06 - val_loss: 1.3833e-04 - val_mse: 1.3710e-04 - val_f_pen: 1.2286e-06\n",
      "Epoch 143/10000\n",
      "98/98 [==============================] - 4s 38ms/step - loss: 2.7379e-04 - mse: 2.7257e-04 - f_pen: 1.2258e-06 - val_loss: 1.0329e-04 - val_mse: 1.0206e-04 - val_f_pen: 1.2272e-06\n",
      "Epoch 144/10000\n",
      "98/98 [==============================] - 4s 37ms/step - loss: 1.0873e-04 - mse: 1.0750e-04 - f_pen: 1.2240e-06 - val_loss: 6.4617e-05 - val_mse: 6.3382e-05 - val_f_pen: 1.2348e-06\n",
      "Epoch 145/10000\n",
      "98/98 [==============================] - 4s 38ms/step - loss: 1.9474e-04 - mse: 1.9351e-04 - f_pen: 1.2243e-06 - val_loss: 8.7254e-05 - val_mse: 8.6010e-05 - val_f_pen: 1.2436e-06\n",
      "Epoch 146/10000\n",
      "98/98 [==============================] - 4s 37ms/step - loss: 1.7369e-04 - mse: 1.7246e-04 - f_pen: 1.2278e-06 - val_loss: 1.5432e-04 - val_mse: 1.5308e-04 - val_f_pen: 1.2422e-06\n",
      "Epoch 147/10000\n",
      "98/98 [==============================] - 4s 36ms/step - loss: 1.8589e-04 - mse: 1.8465e-04 - f_pen: 1.2329e-06 - val_loss: 8.2774e-05 - val_mse: 8.1525e-05 - val_f_pen: 1.2492e-06\n",
      "Epoch 148/10000\n",
      "98/98 [==============================] - 4s 38ms/step - loss: 1.7951e-04 - mse: 1.7827e-04 - f_pen: 1.2383e-06 - val_loss: 7.9663e-05 - val_mse: 7.8412e-05 - val_f_pen: 1.2515e-06\n",
      "Epoch 149/10000\n",
      "98/98 [==============================] - 4s 37ms/step - loss: 2.3853e-04 - mse: 2.3729e-04 - f_pen: 1.2401e-06 - val_loss: 2.4664e-04 - val_mse: 2.4540e-04 - val_f_pen: 1.2447e-06\n",
      "Epoch 150/10000\n",
      "98/98 [==============================] - 4s 37ms/step - loss: 1.3846e-04 - mse: 1.3721e-04 - f_pen: 1.2509e-06 - val_loss: 7.1353e-05 - val_mse: 7.0083e-05 - val_f_pen: 1.2694e-06\n",
      "Epoch 151/10000\n",
      "98/98 [==============================] - 4s 36ms/step - loss: 1.8303e-04 - mse: 1.8178e-04 - f_pen: 1.2523e-06 - val_loss: 9.0228e-05 - val_mse: 8.8957e-05 - val_f_pen: 1.2704e-06\n",
      "Epoch 152/10000\n",
      "98/98 [==============================] - 4s 37ms/step - loss: 1.6841e-04 - mse: 1.6715e-04 - f_pen: 1.2585e-06 - val_loss: 1.0983e-04 - val_mse: 1.0856e-04 - val_f_pen: 1.2700e-06\n",
      "Epoch 153/10000\n",
      "98/98 [==============================] - 4s 36ms/step - loss: 1.7204e-04 - mse: 1.7078e-04 - f_pen: 1.2614e-06 - val_loss: 1.2516e-04 - val_mse: 1.2388e-04 - val_f_pen: 1.2735e-06\n",
      "Epoch 154/10000\n",
      "98/98 [==============================] - 4s 37ms/step - loss: 1.6064e-04 - mse: 1.5938e-04 - f_pen: 1.2627e-06 - val_loss: 9.1836e-05 - val_mse: 9.0561e-05 - val_f_pen: 1.2752e-06\n",
      "Epoch 155/10000\n",
      "98/98 [==============================] - 4s 37ms/step - loss: 2.3761e-04 - mse: 2.3634e-04 - f_pen: 1.2722e-06 - val_loss: 7.2820e-05 - val_mse: 7.1539e-05 - val_f_pen: 1.2809e-06\n",
      "Epoch 156/10000\n",
      "98/98 [==============================] - 4s 38ms/step - loss: 1.7908e-04 - mse: 1.7781e-04 - f_pen: 1.2715e-06 - val_loss: 1.1874e-04 - val_mse: 1.1746e-04 - val_f_pen: 1.2786e-06\n",
      "Epoch 157/10000\n",
      "98/98 [==============================] - 4s 37ms/step - loss: 1.1223e-04 - mse: 1.1096e-04 - f_pen: 1.2754e-06 - val_loss: 6.6199e-05 - val_mse: 6.4915e-05 - val_f_pen: 1.2842e-06\n",
      "Epoch 158/10000\n",
      "98/98 [==============================] - 4s 36ms/step - loss: 1.9364e-04 - mse: 1.9236e-04 - f_pen: 1.2755e-06 - val_loss: 5.7433e-05 - val_mse: 5.6140e-05 - val_f_pen: 1.2923e-06\n",
      "Epoch 159/10000\n",
      "98/98 [==============================] - 4s 37ms/step - loss: 1.5414e-04 - mse: 1.5286e-04 - f_pen: 1.2777e-06 - val_loss: 6.9806e-05 - val_mse: 6.8502e-05 - val_f_pen: 1.3042e-06\n",
      "Epoch 160/10000\n",
      "98/98 [==============================] - 4s 37ms/step - loss: 1.2224e-04 - mse: 1.2096e-04 - f_pen: 1.2819e-06 - val_loss: 8.9205e-05 - val_mse: 8.7912e-05 - val_f_pen: 1.2929e-06\n",
      "Epoch 161/10000\n",
      "98/98 [==============================] - 4s 37ms/step - loss: 2.0150e-04 - mse: 2.0022e-04 - f_pen: 1.2855e-06 - val_loss: 8.2400e-05 - val_mse: 8.1101e-05 - val_f_pen: 1.2994e-06\n",
      "Epoch 162/10000\n",
      "98/98 [==============================] - 4s 37ms/step - loss: 1.3224e-04 - mse: 1.3096e-04 - f_pen: 1.2846e-06 - val_loss: 8.2084e-05 - val_mse: 8.0783e-05 - val_f_pen: 1.3009e-06\n",
      "Epoch 163/10000\n",
      "98/98 [==============================] - 4s 38ms/step - loss: 2.6750e-04 - mse: 2.6620e-04 - f_pen: 1.2923e-06 - val_loss: 7.8404e-05 - val_mse: 7.7101e-05 - val_f_pen: 1.3036e-06\n",
      "Epoch 164/10000\n",
      "98/98 [==============================] - 4s 37ms/step - loss: 1.0683e-04 - mse: 1.0554e-04 - f_pen: 1.2951e-06 - val_loss: 8.4296e-05 - val_mse: 8.2993e-05 - val_f_pen: 1.3025e-06\n",
      "Epoch 165/10000\n",
      "98/98 [==============================] - 4s 37ms/step - loss: 1.4786e-04 - mse: 1.4656e-04 - f_pen: 1.2962e-06 - val_loss: 9.8709e-05 - val_mse: 9.7406e-05 - val_f_pen: 1.3027e-06\n",
      "Epoch 166/10000\n",
      "98/98 [==============================] - 4s 37ms/step - loss: 1.4701e-04 - mse: 1.4571e-04 - f_pen: 1.3001e-06 - val_loss: 9.1975e-05 - val_mse: 9.0650e-05 - val_f_pen: 1.3253e-06\n",
      "Epoch 167/10000\n",
      "98/98 [==============================] - 4s 37ms/step - loss: 1.7090e-04 - mse: 1.6960e-04 - f_pen: 1.3023e-06 - val_loss: 6.6021e-05 - val_mse: 6.4700e-05 - val_f_pen: 1.3211e-06\n",
      "Epoch 168/10000\n",
      "98/98 [==============================] - 4s 37ms/step - loss: 1.9725e-04 - mse: 1.9594e-04 - f_pen: 1.3105e-06 - val_loss: 8.3287e-05 - val_mse: 8.1953e-05 - val_f_pen: 1.3335e-06\n",
      "Epoch 169/10000\n",
      "98/98 [==============================] - 4s 37ms/step - loss: 1.7373e-04 - mse: 1.7242e-04 - f_pen: 1.3120e-06 - val_loss: 3.1088e-04 - val_mse: 3.0954e-04 - val_f_pen: 1.3461e-06\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 170/10000\n",
      "98/98 [==============================] - 4s 37ms/step - loss: 2.0456e-04 - mse: 2.0324e-04 - f_pen: 1.3218e-06 - val_loss: 8.2164e-05 - val_mse: 8.0819e-05 - val_f_pen: 1.3443e-06\n",
      "Epoch 171/10000\n",
      "98/98 [==============================] - 4s 37ms/step - loss: 8.0841e-05 - mse: 7.9519e-05 - f_pen: 1.3226e-06 - val_loss: 6.0820e-05 - val_mse: 5.9487e-05 - val_f_pen: 1.3329e-06\n",
      "Epoch 172/10000\n",
      "98/98 [==============================] - 4s 37ms/step - loss: 9.9669e-05 - mse: 9.8349e-05 - f_pen: 1.3201e-06 - val_loss: 7.8806e-05 - val_mse: 7.7472e-05 - val_f_pen: 1.3340e-06\n",
      "Epoch 173/10000\n",
      "98/98 [==============================] - 4s 37ms/step - loss: 2.0194e-04 - mse: 2.0062e-04 - f_pen: 1.3204e-06 - val_loss: 1.0881e-04 - val_mse: 1.0747e-04 - val_f_pen: 1.3435e-06\n",
      "Epoch 174/10000\n",
      "98/98 [==============================] - 4s 37ms/step - loss: 1.9905e-04 - mse: 1.9771e-04 - f_pen: 1.3312e-06 - val_loss: 7.5728e-05 - val_mse: 7.4376e-05 - val_f_pen: 1.3526e-06\n",
      "Epoch 175/10000\n",
      "98/98 [==============================] - 4s 37ms/step - loss: 9.5542e-05 - mse: 9.4212e-05 - f_pen: 1.3301e-06 - val_loss: 5.7944e-05 - val_mse: 5.6600e-05 - val_f_pen: 1.3445e-06\n",
      "Epoch 176/10000\n",
      "98/98 [==============================] - 4s 36ms/step - loss: 1.5331e-04 - mse: 1.5197e-04 - f_pen: 1.3315e-06 - val_loss: 1.0213e-04 - val_mse: 1.0077e-04 - val_f_pen: 1.3556e-06\n",
      "Epoch 177/10000\n",
      "98/98 [==============================] - 4s 37ms/step - loss: 2.9866e-04 - mse: 2.9732e-04 - f_pen: 1.3440e-06 - val_loss: 6.9934e-05 - val_mse: 6.8566e-05 - val_f_pen: 1.3675e-06\n",
      "Epoch 178/10000\n",
      "98/98 [==============================] - 4s 37ms/step - loss: 7.9055e-05 - mse: 7.7709e-05 - f_pen: 1.3456e-06 - val_loss: 5.7905e-05 - val_mse: 5.6539e-05 - val_f_pen: 1.3657e-06\n",
      "\n",
      "Epoch 00178: ReduceLROnPlateau reducing learning rate to 0.0007000000332482159.\n",
      "Epoch 179/10000\n",
      "98/98 [==============================] - 4s 37ms/step - loss: 3.2331e-05 - mse: 3.0988e-05 - f_pen: 1.3428e-06 - val_loss: 3.8067e-05 - val_mse: 3.6712e-05 - val_f_pen: 1.3547e-06\n",
      "Epoch 180/10000\n",
      "98/98 [==============================] - 4s 37ms/step - loss: 2.9000e-05 - mse: 2.7660e-05 - f_pen: 1.3401e-06 - val_loss: 3.6143e-05 - val_mse: 3.4786e-05 - val_f_pen: 1.3573e-06\n",
      "Epoch 181/10000\n",
      "98/98 [==============================] - 4s 38ms/step - loss: 2.8533e-05 - mse: 2.7195e-05 - f_pen: 1.3381e-06 - val_loss: 3.4051e-05 - val_mse: 3.2702e-05 - val_f_pen: 1.3487e-06\n",
      "Epoch 182/10000\n",
      "98/98 [==============================] - 4s 37ms/step - loss: 2.7775e-05 - mse: 2.6439e-05 - f_pen: 1.3361e-06 - val_loss: 3.6854e-05 - val_mse: 3.5508e-05 - val_f_pen: 1.3461e-06\n",
      "Epoch 183/10000\n",
      "98/98 [==============================] - 4s 37ms/step - loss: 3.2719e-05 - mse: 3.1385e-05 - f_pen: 1.3336e-06 - val_loss: 3.3557e-05 - val_mse: 3.2211e-05 - val_f_pen: 1.3452e-06\n",
      "Epoch 184/10000\n",
      "98/98 [==============================] - 4s 37ms/step - loss: 4.1288e-05 - mse: 3.9954e-05 - f_pen: 1.3335e-06 - val_loss: 4.2151e-05 - val_mse: 4.0801e-05 - val_f_pen: 1.3507e-06\n",
      "Epoch 185/10000\n",
      "98/98 [==============================] - 4s 37ms/step - loss: 6.4602e-05 - mse: 6.3269e-05 - f_pen: 1.3327e-06 - val_loss: 1.0979e-04 - val_mse: 1.0844e-04 - val_f_pen: 1.3492e-06\n",
      "Epoch 186/10000\n",
      "98/98 [==============================] - 4s 37ms/step - loss: 1.7625e-04 - mse: 1.7492e-04 - f_pen: 1.3396e-06 - val_loss: 3.7513e-05 - val_mse: 3.6161e-05 - val_f_pen: 1.3519e-06\n",
      "Epoch 187/10000\n",
      "98/98 [==============================] - 4s 37ms/step - loss: 3.5390e-05 - mse: 3.4052e-05 - f_pen: 1.3378e-06 - val_loss: 3.3297e-05 - val_mse: 3.1942e-05 - val_f_pen: 1.3554e-06\n",
      "Epoch 188/10000\n",
      "98/98 [==============================] - 4s 36ms/step - loss: 4.6488e-05 - mse: 4.5152e-05 - f_pen: 1.3358e-06 - val_loss: 4.0937e-05 - val_mse: 3.9591e-05 - val_f_pen: 1.3462e-06\n",
      "Epoch 189/10000\n",
      "98/98 [==============================] - 4s 37ms/step - loss: 8.9874e-05 - mse: 8.8538e-05 - f_pen: 1.3355e-06 - val_loss: 4.3090e-05 - val_mse: 4.1738e-05 - val_f_pen: 1.3516e-06\n",
      "Epoch 190/10000\n",
      "98/98 [==============================] - 4s 37ms/step - loss: 8.3200e-05 - mse: 8.1865e-05 - f_pen: 1.3350e-06 - val_loss: 1.0761e-04 - val_mse: 1.0626e-04 - val_f_pen: 1.3470e-06\n",
      "Epoch 191/10000\n",
      "98/98 [==============================] - 4s 37ms/step - loss: 8.8973e-05 - mse: 8.7633e-05 - f_pen: 1.3403e-06 - val_loss: 5.9037e-05 - val_mse: 5.7682e-05 - val_f_pen: 1.3551e-06\n",
      "Epoch 192/10000\n",
      "98/98 [==============================] - 4s 37ms/step - loss: 1.1731e-04 - mse: 1.1597e-04 - f_pen: 1.3396e-06 - val_loss: 4.0150e-05 - val_mse: 3.8795e-05 - val_f_pen: 1.3556e-06\n",
      "Epoch 193/10000\n",
      "98/98 [==============================] - 4s 37ms/step - loss: 6.8083e-05 - mse: 6.6743e-05 - f_pen: 1.3395e-06 - val_loss: 5.8240e-05 - val_mse: 5.6882e-05 - val_f_pen: 1.3586e-06\n",
      "Epoch 194/10000\n",
      "98/98 [==============================] - 4s 36ms/step - loss: 6.9383e-05 - mse: 6.8043e-05 - f_pen: 1.3401e-06 - val_loss: 1.1387e-04 - val_mse: 1.1251e-04 - val_f_pen: 1.3617e-06\n",
      "Epoch 195/10000\n",
      "98/98 [==============================] - 4s 37ms/step - loss: 2.7952e-04 - mse: 2.7817e-04 - f_pen: 1.3521e-06 - val_loss: 4.2351e-05 - val_mse: 4.0985e-05 - val_f_pen: 1.3666e-06\n",
      "Epoch 196/10000\n",
      "98/98 [==============================] - 4s 37ms/step - loss: 3.4185e-05 - mse: 3.2835e-05 - f_pen: 1.3505e-06 - val_loss: 3.7509e-05 - val_mse: 3.6141e-05 - val_f_pen: 1.3672e-06\n",
      "Epoch 197/10000\n",
      "98/98 [==============================] - 4s 38ms/step - loss: 4.0500e-05 - mse: 3.9152e-05 - f_pen: 1.3481e-06 - val_loss: 5.0370e-05 - val_mse: 4.9010e-05 - val_f_pen: 1.3596e-06\n",
      "Epoch 198/10000\n",
      "98/98 [==============================] - 4s 37ms/step - loss: 5.8436e-05 - mse: 5.7089e-05 - f_pen: 1.3477e-06 - val_loss: 5.6181e-05 - val_mse: 5.4812e-05 - val_f_pen: 1.3692e-06\n",
      "Epoch 199/10000\n",
      "98/98 [==============================] - 4s 36ms/step - loss: 7.4519e-05 - mse: 7.3171e-05 - f_pen: 1.3480e-06 - val_loss: 4.2150e-05 - val_mse: 4.0784e-05 - val_f_pen: 1.3655e-06\n",
      "Epoch 200/10000\n",
      "98/98 [==============================] - 4s 38ms/step - loss: 9.2894e-05 - mse: 9.1546e-05 - f_pen: 1.3472e-06 - val_loss: 8.3403e-05 - val_mse: 8.2038e-05 - val_f_pen: 1.3656e-06\n",
      "Epoch 201/10000\n",
      "98/98 [==============================] - 4s 36ms/step - loss: 8.6955e-05 - mse: 8.5605e-05 - f_pen: 1.3495e-06 - val_loss: 7.2076e-05 - val_mse: 7.0723e-05 - val_f_pen: 1.3530e-06\n",
      "Epoch 202/10000\n",
      "98/98 [==============================] - 4s 37ms/step - loss: 8.0364e-05 - mse: 7.9013e-05 - f_pen: 1.3505e-06 - val_loss: 7.1781e-05 - val_mse: 7.0410e-05 - val_f_pen: 1.3711e-06\n",
      "Epoch 203/10000\n",
      "98/98 [==============================] - 4s 37ms/step - loss: 8.3927e-05 - mse: 8.2575e-05 - f_pen: 1.3526e-06 - val_loss: 3.7648e-05 - val_mse: 3.6284e-05 - val_f_pen: 1.3633e-06\n",
      "Epoch 204/10000\n",
      "98/98 [==============================] - 4s 38ms/step - loss: 5.7308e-05 - mse: 5.5958e-05 - f_pen: 1.3499e-06 - val_loss: 7.7219e-05 - val_mse: 7.5855e-05 - val_f_pen: 1.3642e-06\n",
      "Epoch 205/10000\n",
      "98/98 [==============================] - 4s 37ms/step - loss: 2.4480e-04 - mse: 2.4344e-04 - f_pen: 1.3568e-06 - val_loss: 4.2027e-05 - val_mse: 4.0643e-05 - val_f_pen: 1.3843e-06\n",
      "Epoch 206/10000\n",
      "98/98 [==============================] - 4s 37ms/step - loss: 3.0649e-05 - mse: 2.9288e-05 - f_pen: 1.3604e-06 - val_loss: 3.4019e-05 - val_mse: 3.2641e-05 - val_f_pen: 1.3778e-06\n",
      "Epoch 207/10000\n",
      "98/98 [==============================] - 4s 37ms/step - loss: 3.7452e-05 - mse: 3.6094e-05 - f_pen: 1.3582e-06 - val_loss: 3.6308e-05 - val_mse: 3.4929e-05 - val_f_pen: 1.3792e-06\n",
      "\n",
      "Epoch 00207: ReduceLROnPlateau reducing learning rate to 0.0004900000232737511.\n",
      "Epoch 208/10000\n",
      "98/98 [==============================] - 4s 37ms/step - loss: 2.0131e-05 - mse: 1.8774e-05 - f_pen: 1.3570e-06 - val_loss: 2.5501e-05 - val_mse: 2.4131e-05 - val_f_pen: 1.3704e-06\n",
      "Epoch 209/10000\n",
      "98/98 [==============================] - 4s 37ms/step - loss: 1.8642e-05 - mse: 1.7286e-05 - f_pen: 1.3554e-06 - val_loss: 2.4779e-05 - val_mse: 2.3411e-05 - val_f_pen: 1.3679e-06\n",
      "Epoch 210/10000\n",
      "98/98 [==============================] - 4s 37ms/step - loss: 1.8406e-05 - mse: 1.7052e-05 - f_pen: 1.3540e-06 - val_loss: 2.3649e-05 - val_mse: 2.2281e-05 - val_f_pen: 1.3675e-06\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 211/10000\n",
      "98/98 [==============================] - 4s 36ms/step - loss: 1.7959e-05 - mse: 1.6606e-05 - f_pen: 1.3527e-06 - val_loss: 2.5116e-05 - val_mse: 2.3750e-05 - val_f_pen: 1.3664e-06\n",
      "Epoch 212/10000\n",
      "98/98 [==============================] - 4s 37ms/step - loss: 1.9612e-05 - mse: 1.8261e-05 - f_pen: 1.3516e-06 - val_loss: 2.4098e-05 - val_mse: 2.2731e-05 - val_f_pen: 1.3666e-06\n",
      "Epoch 213/10000\n",
      "98/98 [==============================] - 4s 37ms/step - loss: 2.3234e-05 - mse: 2.1884e-05 - f_pen: 1.3499e-06 - val_loss: 2.5425e-05 - val_mse: 2.4060e-05 - val_f_pen: 1.3648e-06\n",
      "Epoch 214/10000\n",
      "98/98 [==============================] - 4s 37ms/step - loss: 2.4760e-05 - mse: 2.3410e-05 - f_pen: 1.3493e-06 - val_loss: 2.5124e-05 - val_mse: 2.3758e-05 - val_f_pen: 1.3658e-06\n",
      "Epoch 215/10000\n",
      "98/98 [==============================] - 4s 37ms/step - loss: 6.4565e-05 - mse: 6.3218e-05 - f_pen: 1.3466e-06 - val_loss: 4.3271e-05 - val_mse: 4.1905e-05 - val_f_pen: 1.3655e-06\n",
      "Epoch 216/10000\n",
      "98/98 [==============================] - 4s 38ms/step - loss: 3.3761e-05 - mse: 3.2411e-05 - f_pen: 1.3497e-06 - val_loss: 2.8275e-05 - val_mse: 2.6915e-05 - val_f_pen: 1.3600e-06\n",
      "Epoch 217/10000\n",
      "98/98 [==============================] - 4s 37ms/step - loss: 3.7552e-05 - mse: 3.6203e-05 - f_pen: 1.3493e-06 - val_loss: 2.6136e-05 - val_mse: 2.4770e-05 - val_f_pen: 1.3664e-06\n",
      "Epoch 218/10000\n",
      "98/98 [==============================] - 4s 37ms/step - loss: 4.5032e-05 - mse: 4.3683e-05 - f_pen: 1.3487e-06 - val_loss: 4.5520e-05 - val_mse: 4.4157e-05 - val_f_pen: 1.3627e-06\n",
      "Epoch 219/10000\n",
      "98/98 [==============================] - 4s 37ms/step - loss: 6.5260e-05 - mse: 6.3911e-05 - f_pen: 1.3496e-06 - val_loss: 2.4160e-05 - val_mse: 2.2793e-05 - val_f_pen: 1.3670e-06\n",
      "Epoch 220/10000\n",
      "98/98 [==============================] - 4s 37ms/step - loss: 3.1252e-05 - mse: 2.9903e-05 - f_pen: 1.3489e-06 - val_loss: 2.7851e-05 - val_mse: 2.6486e-05 - val_f_pen: 1.3654e-06\n",
      "Epoch 221/10000\n",
      "98/98 [==============================] - 4s 38ms/step - loss: 5.1142e-05 - mse: 4.9793e-05 - f_pen: 1.3491e-06 - val_loss: 3.2326e-05 - val_mse: 3.0966e-05 - val_f_pen: 1.3601e-06\n",
      "Epoch 222/10000\n",
      "98/98 [==============================] - 4s 36ms/step - loss: 4.5504e-05 - mse: 4.4155e-05 - f_pen: 1.3487e-06 - val_loss: 5.3503e-05 - val_mse: 5.2137e-05 - val_f_pen: 1.3659e-06\n",
      "Epoch 223/10000\n",
      "98/98 [==============================] - 4s 37ms/step - loss: 5.2078e-05 - mse: 5.0729e-05 - f_pen: 1.3497e-06 - val_loss: 2.6969e-05 - val_mse: 2.5606e-05 - val_f_pen: 1.3627e-06\n",
      "Epoch 224/10000\n",
      "98/98 [==============================] - 4s 38ms/step - loss: 4.0873e-05 - mse: 3.9524e-05 - f_pen: 1.3491e-06 - val_loss: 5.7847e-05 - val_mse: 5.6478e-05 - val_f_pen: 1.3695e-06\n",
      "Epoch 225/10000\n",
      "98/98 [==============================] - 4s 37ms/step - loss: 1.0392e-04 - mse: 1.0257e-04 - f_pen: 1.3510e-06 - val_loss: 2.4507e-05 - val_mse: 2.3136e-05 - val_f_pen: 1.3709e-06\n",
      "Epoch 226/10000\n",
      "98/98 [==============================] - 4s 37ms/step - loss: 2.0668e-05 - mse: 1.9316e-05 - f_pen: 1.3523e-06 - val_loss: 2.5533e-05 - val_mse: 2.4163e-05 - val_f_pen: 1.3697e-06\n",
      "Epoch 227/10000\n",
      "98/98 [==============================] - 4s 37ms/step - loss: 2.8821e-05 - mse: 2.7471e-05 - f_pen: 1.3497e-06 - val_loss: 2.8158e-05 - val_mse: 2.6790e-05 - val_f_pen: 1.3676e-06\n",
      "Epoch 228/10000\n",
      "98/98 [==============================] - 4s 36ms/step - loss: 5.2259e-05 - mse: 5.0909e-05 - f_pen: 1.3498e-06 - val_loss: 4.1105e-05 - val_mse: 3.9738e-05 - val_f_pen: 1.3670e-06\n",
      "Epoch 229/10000\n",
      "98/98 [==============================] - 4s 37ms/step - loss: 4.5948e-05 - mse: 4.4597e-05 - f_pen: 1.3515e-06 - val_loss: 3.3078e-05 - val_mse: 3.1706e-05 - val_f_pen: 1.3722e-06\n",
      "Epoch 230/10000\n",
      "98/98 [==============================] - 4s 37ms/step - loss: 4.1203e-05 - mse: 3.9853e-05 - f_pen: 1.3503e-06 - val_loss: 2.5428e-05 - val_mse: 2.4063e-05 - val_f_pen: 1.3650e-06\n",
      "\n",
      "Epoch 00230: ReduceLROnPlateau reducing learning rate to 0.00034300000406801696.\n",
      "Epoch 231/10000\n",
      "98/98 [==============================] - 4s 36ms/step - loss: 1.4423e-05 - mse: 1.3073e-05 - f_pen: 1.3499e-06 - val_loss: 1.8599e-05 - val_mse: 1.7235e-05 - val_f_pen: 1.3643e-06\n",
      "Epoch 232/10000\n",
      "98/98 [==============================] - 4s 37ms/step - loss: 1.3959e-05 - mse: 1.2610e-05 - f_pen: 1.3489e-06 - val_loss: 1.8616e-05 - val_mse: 1.7251e-05 - val_f_pen: 1.3648e-06\n",
      "Epoch 233/10000\n",
      "98/98 [==============================] - 4s 37ms/step - loss: 1.3294e-05 - mse: 1.1947e-05 - f_pen: 1.3473e-06 - val_loss: 1.7331e-05 - val_mse: 1.5968e-05 - val_f_pen: 1.3630e-06\n",
      "Epoch 234/10000\n",
      "98/98 [==============================] - 4s 37ms/step - loss: 1.3421e-05 - mse: 1.2075e-05 - f_pen: 1.3459e-06 - val_loss: 1.7617e-05 - val_mse: 1.6257e-05 - val_f_pen: 1.3600e-06\n",
      "Epoch 235/10000\n",
      "98/98 [==============================] - 4s 36ms/step - loss: 1.3671e-05 - mse: 1.2326e-05 - f_pen: 1.3451e-06 - val_loss: 1.7133e-05 - val_mse: 1.5772e-05 - val_f_pen: 1.3611e-06\n",
      "Epoch 236/10000\n",
      "98/98 [==============================] - 4s 36ms/step - loss: 1.4365e-05 - mse: 1.3021e-05 - f_pen: 1.3445e-06 - val_loss: 2.1927e-05 - val_mse: 2.0569e-05 - val_f_pen: 1.3581e-06\n",
      "Epoch 237/10000\n",
      "98/98 [==============================] - 4s 37ms/step - loss: 2.0108e-05 - mse: 1.8764e-05 - f_pen: 1.3439e-06 - val_loss: 1.9345e-05 - val_mse: 1.7983e-05 - val_f_pen: 1.3617e-06\n",
      "Epoch 238/10000\n",
      "98/98 [==============================] - 4s 37ms/step - loss: 1.8995e-05 - mse: 1.7652e-05 - f_pen: 1.3429e-06 - val_loss: 1.8688e-05 - val_mse: 1.7331e-05 - val_f_pen: 1.3572e-06\n",
      "Epoch 239/10000\n",
      "98/98 [==============================] - 4s 37ms/step - loss: 3.3621e-05 - mse: 3.2279e-05 - f_pen: 1.3420e-06 - val_loss: 2.2459e-05 - val_mse: 2.1101e-05 - val_f_pen: 1.3580e-06\n",
      "Epoch 240/10000\n",
      "98/98 [==============================] - 4s 36ms/step - loss: 2.6147e-05 - mse: 2.4805e-05 - f_pen: 1.3422e-06 - val_loss: 2.0726e-05 - val_mse: 1.9370e-05 - val_f_pen: 1.3559e-06\n",
      "Epoch 241/10000\n",
      "98/98 [==============================] - 4s 37ms/step - loss: 2.5804e-05 - mse: 2.4462e-05 - f_pen: 1.3420e-06 - val_loss: 2.5956e-05 - val_mse: 2.4597e-05 - val_f_pen: 1.3584e-06\n",
      "Epoch 242/10000\n",
      "98/98 [==============================] - 4s 37ms/step - loss: 2.9214e-05 - mse: 2.7872e-05 - f_pen: 1.3416e-06 - val_loss: 2.4400e-05 - val_mse: 2.3045e-05 - val_f_pen: 1.3552e-06\n",
      "Epoch 243/10000\n",
      "98/98 [==============================] - 4s 37ms/step - loss: 2.5824e-05 - mse: 2.4483e-05 - f_pen: 1.3413e-06 - val_loss: 2.2892e-05 - val_mse: 2.1535e-05 - val_f_pen: 1.3572e-06\n",
      "Epoch 244/10000\n",
      "98/98 [==============================] - 4s 38ms/step - loss: 3.1713e-05 - mse: 3.0372e-05 - f_pen: 1.3414e-06 - val_loss: 2.5952e-05 - val_mse: 2.4598e-05 - val_f_pen: 1.3547e-06\n",
      "Epoch 245/10000\n",
      "98/98 [==============================] - 4s 37ms/step - loss: 3.0489e-05 - mse: 2.9148e-05 - f_pen: 1.3403e-06 - val_loss: 2.6179e-05 - val_mse: 2.4825e-05 - val_f_pen: 1.3539e-06\n",
      "Epoch 246/10000\n",
      "98/98 [==============================] - 4s 36ms/step - loss: 2.2194e-05 - mse: 2.0854e-05 - f_pen: 1.3404e-06 - val_loss: 2.1092e-05 - val_mse: 1.9735e-05 - val_f_pen: 1.3569e-06\n",
      "Epoch 247/10000\n",
      "98/98 [==============================] - 4s 36ms/step - loss: 2.8871e-05 - mse: 2.7532e-05 - f_pen: 1.3391e-06 - val_loss: 1.9723e-05 - val_mse: 1.8369e-05 - val_f_pen: 1.3541e-06\n",
      "Epoch 248/10000\n",
      "98/98 [==============================] - 4s 37ms/step - loss: 2.7146e-05 - mse: 2.5806e-05 - f_pen: 1.3396e-06 - val_loss: 2.6752e-05 - val_mse: 2.5399e-05 - val_f_pen: 1.3536e-06\n",
      "Epoch 249/10000\n",
      "98/98 [==============================] - 4s 37ms/step - loss: 3.3216e-05 - mse: 3.1876e-05 - f_pen: 1.3402e-06 - val_loss: 1.9391e-05 - val_mse: 1.8038e-05 - val_f_pen: 1.3532e-06\n",
      "Epoch 250/10000\n",
      "98/98 [==============================] - 4s 37ms/step - loss: 1.9886e-05 - mse: 1.8546e-05 - f_pen: 1.3404e-06 - val_loss: 1.9442e-05 - val_mse: 1.8088e-05 - val_f_pen: 1.3542e-06\n",
      "Epoch 251/10000\n",
      "98/98 [==============================] - 4s 37ms/step - loss: 2.6709e-05 - mse: 2.5370e-05 - f_pen: 1.3388e-06 - val_loss: 1.8749e-05 - val_mse: 1.7391e-05 - val_f_pen: 1.3579e-06\n",
      "Epoch 252/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "98/98 [==============================] - 4s 37ms/step - loss: 2.8066e-05 - mse: 2.6727e-05 - f_pen: 1.3387e-06 - val_loss: 2.3448e-05 - val_mse: 2.2089e-05 - val_f_pen: 1.3584e-06\n",
      "Epoch 253/10000\n",
      "98/98 [==============================] - 4s 37ms/step - loss: 2.3431e-05 - mse: 2.2093e-05 - f_pen: 1.3384e-06 - val_loss: 2.1362e-05 - val_mse: 2.0009e-05 - val_f_pen: 1.3533e-06\n",
      "Epoch 254/10000\n",
      "98/98 [==============================] - 4s 37ms/step - loss: 3.3070e-05 - mse: 3.1732e-05 - f_pen: 1.3387e-06 - val_loss: 1.8047e-05 - val_mse: 1.6695e-05 - val_f_pen: 1.3519e-06\n",
      "Epoch 255/10000\n",
      "98/98 [==============================] - 4s 37ms/step - loss: 2.2534e-05 - mse: 2.1197e-05 - f_pen: 1.3376e-06 - val_loss: 2.2783e-05 - val_mse: 2.1434e-05 - val_f_pen: 1.3498e-06\n",
      "\n",
      "Epoch 00255: ReduceLROnPlateau reducing learning rate to 0.00024009999469853935.\n",
      "Epoch 256/10000\n",
      "98/98 [==============================] - 4s 37ms/step - loss: 1.1247e-05 - mse: 9.9101e-06 - f_pen: 1.3369e-06 - val_loss: 1.3104e-05 - val_mse: 1.1753e-05 - val_f_pen: 1.3514e-06\n",
      "Epoch 257/10000\n",
      "98/98 [==============================] - 4s 38ms/step - loss: 9.7418e-06 - mse: 8.4063e-06 - f_pen: 1.3355e-06 - val_loss: 1.4275e-05 - val_mse: 1.2924e-05 - val_f_pen: 1.3516e-06\n",
      "Epoch 258/10000\n",
      "98/98 [==============================] - 4s 37ms/step - loss: 9.7677e-06 - mse: 8.4333e-06 - f_pen: 1.3344e-06 - val_loss: 1.4636e-05 - val_mse: 1.3289e-05 - val_f_pen: 1.3468e-06\n",
      "Epoch 259/10000\n",
      "98/98 [==============================] - 4s 36ms/step - loss: 1.0001e-05 - mse: 8.6674e-06 - f_pen: 1.3334e-06 - val_loss: 1.4618e-05 - val_mse: 1.3269e-05 - val_f_pen: 1.3488e-06\n",
      "Epoch 260/10000\n",
      "98/98 [==============================] - 4s 37ms/step - loss: 1.0109e-05 - mse: 8.7773e-06 - f_pen: 1.3321e-06 - val_loss: 1.5449e-05 - val_mse: 1.4102e-05 - val_f_pen: 1.3477e-06\n",
      "Epoch 261/10000\n",
      "98/98 [==============================] - 4s 36ms/step - loss: 1.0672e-05 - mse: 9.3407e-06 - f_pen: 1.3316e-06 - val_loss: 1.5139e-05 - val_mse: 1.3792e-05 - val_f_pen: 1.3473e-06\n",
      "Epoch 262/10000\n",
      "98/98 [==============================] - 4s 37ms/step - loss: 1.1267e-05 - mse: 9.9362e-06 - f_pen: 1.3304e-06 - val_loss: 1.3996e-05 - val_mse: 1.2649e-05 - val_f_pen: 1.3466e-06\n",
      "Epoch 263/10000\n",
      "98/98 [==============================] - 4s 37ms/step - loss: 1.2331e-05 - mse: 1.1001e-05 - f_pen: 1.3296e-06 - val_loss: 1.6003e-05 - val_mse: 1.4660e-05 - val_f_pen: 1.3432e-06\n",
      "Epoch 264/10000\n",
      "98/98 [==============================] - 4s 37ms/step - loss: 1.9917e-05 - mse: 1.8589e-05 - f_pen: 1.3283e-06 - val_loss: 1.7821e-05 - val_mse: 1.6478e-05 - val_f_pen: 1.3426e-06\n",
      "Epoch 265/10000\n",
      "98/98 [==============================] - 4s 37ms/step - loss: 1.6151e-05 - mse: 1.4823e-05 - f_pen: 1.3283e-06 - val_loss: 2.2031e-05 - val_mse: 2.0690e-05 - val_f_pen: 1.3413e-06\n",
      "Epoch 266/10000\n",
      "98/98 [==============================] - 4s 38ms/step - loss: 1.8326e-05 - mse: 1.6997e-05 - f_pen: 1.3287e-06 - val_loss: 1.8045e-05 - val_mse: 1.6697e-05 - val_f_pen: 1.3480e-06\n",
      "Epoch 267/10000\n",
      "98/98 [==============================] - 4s 37ms/step - loss: 1.4988e-05 - mse: 1.3660e-05 - f_pen: 1.3283e-06 - val_loss: 1.5413e-05 - val_mse: 1.4069e-05 - val_f_pen: 1.3434e-06\n",
      "Epoch 268/10000\n",
      "98/98 [==============================] - 4s 36ms/step - loss: 1.7868e-05 - mse: 1.6540e-05 - f_pen: 1.3274e-06 - val_loss: 1.8426e-05 - val_mse: 1.7080e-05 - val_f_pen: 1.3453e-06\n",
      "Epoch 269/10000\n",
      "98/98 [==============================] - 4s 36ms/step - loss: 2.2793e-05 - mse: 2.1466e-05 - f_pen: 1.3269e-06 - val_loss: 1.5639e-05 - val_mse: 1.4297e-05 - val_f_pen: 1.3424e-06\n",
      "Epoch 270/10000\n",
      "98/98 [==============================] - 4s 38ms/step - loss: 1.3620e-05 - mse: 1.2294e-05 - f_pen: 1.3260e-06 - val_loss: 1.6163e-05 - val_mse: 1.4822e-05 - val_f_pen: 1.3409e-06\n",
      "Epoch 271/10000\n",
      "98/98 [==============================] - 4s 37ms/step - loss: 1.3060e-05 - mse: 1.1735e-05 - f_pen: 1.3256e-06 - val_loss: 1.4759e-05 - val_mse: 1.3420e-05 - val_f_pen: 1.3394e-06\n",
      "Epoch 272/10000\n",
      "98/98 [==============================] - 4s 36ms/step - loss: 1.8485e-05 - mse: 1.7160e-05 - f_pen: 1.3253e-06 - val_loss: 2.2439e-05 - val_mse: 2.1094e-05 - val_f_pen: 1.3449e-06\n",
      "Epoch 273/10000\n",
      "98/98 [==============================] - 4s 36ms/step - loss: 2.4273e-05 - mse: 2.2947e-05 - f_pen: 1.3253e-06 - val_loss: 1.4996e-05 - val_mse: 1.3656e-05 - val_f_pen: 1.3394e-06\n",
      "Epoch 274/10000\n",
      "98/98 [==============================] - 4s 37ms/step - loss: 1.2370e-05 - mse: 1.1045e-05 - f_pen: 1.3246e-06 - val_loss: 1.3042e-05 - val_mse: 1.1702e-05 - val_f_pen: 1.3407e-06\n",
      "Epoch 275/10000\n",
      "98/98 [==============================] - 4s 36ms/step - loss: 1.3251e-05 - mse: 1.1928e-05 - f_pen: 1.3232e-06 - val_loss: 1.9263e-05 - val_mse: 1.7923e-05 - val_f_pen: 1.3402e-06\n",
      "Epoch 276/10000\n",
      "98/98 [==============================] - 4s 37ms/step - loss: 1.4693e-05 - mse: 1.3370e-05 - f_pen: 1.3235e-06 - val_loss: 1.3878e-05 - val_mse: 1.2539e-05 - val_f_pen: 1.3391e-06\n",
      "\n",
      "Epoch 00276: ReduceLROnPlateau reducing learning rate to 0.00016806999628897755.\n",
      "Epoch 277/10000\n",
      "98/98 [==============================] - 4s 37ms/step - loss: 8.2457e-06 - mse: 6.9242e-06 - f_pen: 1.3216e-06 - val_loss: 1.1178e-05 - val_mse: 9.8420e-06 - val_f_pen: 1.3359e-06\n",
      "Epoch 278/10000\n",
      "98/98 [==============================] - 4s 36ms/step - loss: 7.7375e-06 - mse: 6.4163e-06 - f_pen: 1.3213e-06 - val_loss: 1.1584e-05 - val_mse: 1.0248e-05 - val_f_pen: 1.3354e-06\n",
      "Epoch 279/10000\n",
      "98/98 [==============================] - 4s 36ms/step - loss: 7.8573e-06 - mse: 6.5371e-06 - f_pen: 1.3202e-06 - val_loss: 1.1412e-05 - val_mse: 1.0077e-05 - val_f_pen: 1.3352e-06\n",
      "Epoch 280/10000\n",
      "98/98 [==============================] - 4s 37ms/step - loss: 8.1341e-06 - mse: 6.8145e-06 - f_pen: 1.3195e-06 - val_loss: 1.1951e-05 - val_mse: 1.0616e-05 - val_f_pen: 1.3357e-06\n",
      "Epoch 281/10000\n",
      "98/98 [==============================] - 4s 36ms/step - loss: 8.4122e-06 - mse: 7.0936e-06 - f_pen: 1.3186e-06 - val_loss: 1.1293e-05 - val_mse: 9.9597e-06 - val_f_pen: 1.3336e-06\n",
      "Epoch 282/10000\n",
      "98/98 [==============================] - 4s 37ms/step - loss: 8.4364e-06 - mse: 7.1187e-06 - f_pen: 1.3177e-06 - val_loss: 1.1963e-05 - val_mse: 1.0630e-05 - val_f_pen: 1.3335e-06\n",
      "Epoch 283/10000\n",
      "98/98 [==============================] - 4s 36ms/step - loss: 9.6814e-06 - mse: 8.3642e-06 - f_pen: 1.3172e-06 - val_loss: 1.2728e-05 - val_mse: 1.1397e-05 - val_f_pen: 1.3308e-06\n",
      "Epoch 284/10000\n",
      "98/98 [==============================] - 4s 37ms/step - loss: 9.4344e-06 - mse: 8.1183e-06 - f_pen: 1.3161e-06 - val_loss: 1.2377e-05 - val_mse: 1.1046e-05 - val_f_pen: 1.3301e-06\n",
      "Epoch 285/10000\n",
      "98/98 [==============================] - 4s 37ms/step - loss: 1.1270e-05 - mse: 9.9544e-06 - f_pen: 1.3159e-06 - val_loss: 1.1697e-05 - val_mse: 1.0366e-05 - val_f_pen: 1.3312e-06\n",
      "Epoch 286/10000\n",
      "98/98 [==============================] - 4s 37ms/step - loss: 9.7862e-06 - mse: 8.4710e-06 - f_pen: 1.3151e-06 - val_loss: 1.2815e-05 - val_mse: 1.1486e-05 - val_f_pen: 1.3290e-06\n",
      "Epoch 287/10000\n",
      "98/98 [==============================] - 4s 36ms/step - loss: 1.2671e-05 - mse: 1.1356e-05 - f_pen: 1.3148e-06 - val_loss: 1.2020e-05 - val_mse: 1.0691e-05 - val_f_pen: 1.3288e-06\n",
      "Epoch 288/10000\n",
      "98/98 [==============================] - 4s 37ms/step - loss: 1.0697e-05 - mse: 9.3829e-06 - f_pen: 1.3136e-06 - val_loss: 1.2655e-05 - val_mse: 1.1325e-05 - val_f_pen: 1.3293e-06\n",
      "Epoch 289/10000\n",
      "98/98 [==============================] - 4s 37ms/step - loss: 1.0972e-05 - mse: 9.6595e-06 - f_pen: 1.3128e-06 - val_loss: 1.2531e-05 - val_mse: 1.1200e-05 - val_f_pen: 1.3303e-06\n",
      "Epoch 290/10000\n",
      "98/98 [==============================] - 4s 37ms/step - loss: 1.3819e-05 - mse: 1.2507e-05 - f_pen: 1.3123e-06 - val_loss: 1.3016e-05 - val_mse: 1.1688e-05 - val_f_pen: 1.3275e-06\n",
      "Epoch 291/10000\n",
      "98/98 [==============================] - 4s 37ms/step - loss: 1.2233e-05 - mse: 1.0921e-05 - f_pen: 1.3118e-06 - val_loss: 1.5444e-05 - val_mse: 1.4116e-05 - val_f_pen: 1.3280e-06\n",
      "Epoch 292/10000\n",
      "98/98 [==============================] - 4s 36ms/step - loss: 9.8412e-06 - mse: 8.5302e-06 - f_pen: 1.3110e-06 - val_loss: 1.1165e-05 - val_mse: 9.8391e-06 - val_f_pen: 1.3254e-06\n",
      "Epoch 293/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "98/98 [==============================] - 4s 36ms/step - loss: 1.0349e-05 - mse: 9.0386e-06 - f_pen: 1.3105e-06 - val_loss: 1.4383e-05 - val_mse: 1.3059e-05 - val_f_pen: 1.3240e-06\n",
      "Epoch 294/10000\n",
      "98/98 [==============================] - 4s 36ms/step - loss: 1.5535e-05 - mse: 1.4225e-05 - f_pen: 1.3100e-06 - val_loss: 1.2266e-05 - val_mse: 1.0939e-05 - val_f_pen: 1.3269e-06\n",
      "Epoch 295/10000\n",
      "98/98 [==============================] - 4s 37ms/step - loss: 8.4528e-06 - mse: 7.1433e-06 - f_pen: 1.3094e-06 - val_loss: 1.1639e-05 - val_mse: 1.0317e-05 - val_f_pen: 1.3226e-06\n",
      "Epoch 296/10000\n",
      "98/98 [==============================] - 4s 37ms/step - loss: 9.3210e-06 - mse: 8.0122e-06 - f_pen: 1.3088e-06 - val_loss: 1.2485e-05 - val_mse: 1.1158e-05 - val_f_pen: 1.3263e-06\n",
      "Epoch 297/10000\n",
      "98/98 [==============================] - 4s 37ms/step - loss: 1.2968e-05 - mse: 1.1660e-05 - f_pen: 1.3080e-06 - val_loss: 1.1268e-05 - val_mse: 9.9459e-06 - val_f_pen: 1.3219e-06\n",
      "\n",
      "Epoch 00297: ReduceLROnPlateau reducing learning rate to 0.00011764899536501615.\n",
      "Epoch 298/10000\n",
      "98/98 [==============================] - 4s 37ms/step - loss: 6.6964e-06 - mse: 5.3890e-06 - f_pen: 1.3074e-06 - val_loss: 9.7824e-06 - val_mse: 8.4594e-06 - val_f_pen: 1.3229e-06\n",
      "Epoch 299/10000\n",
      "98/98 [==============================] - 4s 36ms/step - loss: 6.4491e-06 - mse: 5.1422e-06 - f_pen: 1.3069e-06 - val_loss: 9.6389e-06 - val_mse: 8.3165e-06 - val_f_pen: 1.3224e-06\n",
      "Epoch 300/10000\n",
      "98/98 [==============================] - 4s 37ms/step - loss: 6.4612e-06 - mse: 5.1552e-06 - f_pen: 1.3059e-06 - val_loss: 9.5337e-06 - val_mse: 8.2131e-06 - val_f_pen: 1.3207e-06\n",
      "Epoch 301/10000\n",
      "98/98 [==============================] - 4s 36ms/step - loss: 6.5997e-06 - mse: 5.2942e-06 - f_pen: 1.3056e-06 - val_loss: 9.4400e-06 - val_mse: 8.1211e-06 - val_f_pen: 1.3189e-06\n",
      "Epoch 302/10000\n",
      "98/98 [==============================] - 4s 37ms/step - loss: 6.5504e-06 - mse: 5.2458e-06 - f_pen: 1.3045e-06 - val_loss: 9.8514e-06 - val_mse: 8.5328e-06 - val_f_pen: 1.3185e-06\n",
      "Epoch 303/10000\n",
      "98/98 [==============================] - 4s 37ms/step - loss: 6.9455e-06 - mse: 5.6413e-06 - f_pen: 1.3041e-06 - val_loss: 1.0585e-05 - val_mse: 9.2678e-06 - val_f_pen: 1.3173e-06\n",
      "Epoch 304/10000\n",
      "98/98 [==============================] - 4s 36ms/step - loss: 7.2340e-06 - mse: 5.9305e-06 - f_pen: 1.3035e-06 - val_loss: 1.0246e-05 - val_mse: 8.9275e-06 - val_f_pen: 1.3185e-06\n",
      "Epoch 305/10000\n",
      "98/98 [==============================] - 4s 37ms/step - loss: 7.2634e-06 - mse: 5.9604e-06 - f_pen: 1.3031e-06 - val_loss: 1.0249e-05 - val_mse: 8.9312e-06 - val_f_pen: 1.3182e-06\n",
      "Epoch 306/10000\n",
      "98/98 [==============================] - 4s 37ms/step - loss: 7.3653e-06 - mse: 6.0631e-06 - f_pen: 1.3022e-06 - val_loss: 1.0459e-05 - val_mse: 9.1432e-06 - val_f_pen: 1.3154e-06\n",
      "Epoch 307/10000\n",
      "98/98 [==============================] - 4s 37ms/step - loss: 8.0975e-06 - mse: 6.7963e-06 - f_pen: 1.3012e-06 - val_loss: 1.1381e-05 - val_mse: 1.0064e-05 - val_f_pen: 1.3168e-06\n",
      "Epoch 308/10000\n",
      "98/98 [==============================] - 4s 37ms/step - loss: 7.9492e-06 - mse: 6.6482e-06 - f_pen: 1.3009e-06 - val_loss: 9.8058e-06 - val_mse: 8.4902e-06 - val_f_pen: 1.3156e-06\n",
      "Epoch 309/10000\n",
      "98/98 [==============================] - 4s 38ms/step - loss: 7.9372e-06 - mse: 6.6373e-06 - f_pen: 1.2999e-06 - val_loss: 9.9023e-06 - val_mse: 8.5870e-06 - val_f_pen: 1.3153e-06\n",
      "Epoch 310/10000\n",
      "98/98 [==============================] - 4s 37ms/step - loss: 7.7252e-06 - mse: 6.4259e-06 - f_pen: 1.2993e-06 - val_loss: 1.1441e-05 - val_mse: 1.0127e-05 - val_f_pen: 1.3140e-06\n",
      "Epoch 311/10000\n",
      "98/98 [==============================] - 4s 38ms/step - loss: 8.0116e-06 - mse: 6.7132e-06 - f_pen: 1.2985e-06 - val_loss: 9.3703e-06 - val_mse: 8.0580e-06 - val_f_pen: 1.3123e-06\n",
      "Epoch 312/10000\n",
      "98/98 [==============================] - 4s 38ms/step - loss: 8.5661e-06 - mse: 7.2686e-06 - f_pen: 1.2976e-06 - val_loss: 9.8738e-06 - val_mse: 8.5625e-06 - val_f_pen: 1.3113e-06\n",
      "Epoch 313/10000\n",
      "98/98 [==============================] - 4s 37ms/step - loss: 7.2637e-06 - mse: 5.9664e-06 - f_pen: 1.2973e-06 - val_loss: 9.5480e-06 - val_mse: 8.2376e-06 - val_f_pen: 1.3104e-06\n",
      "Epoch 314/10000\n",
      "98/98 [==============================] - 4s 37ms/step - loss: 8.7740e-06 - mse: 7.4773e-06 - f_pen: 1.2967e-06 - val_loss: 9.4831e-06 - val_mse: 8.1718e-06 - val_f_pen: 1.3113e-06\n",
      "Epoch 315/10000\n",
      "98/98 [==============================] - 4s 37ms/step - loss: 7.3966e-06 - mse: 6.1006e-06 - f_pen: 1.2960e-06 - val_loss: 9.4712e-06 - val_mse: 8.1603e-06 - val_f_pen: 1.3109e-06\n",
      "Epoch 316/10000\n",
      "98/98 [==============================] - 4s 37ms/step - loss: 8.1919e-06 - mse: 6.8965e-06 - f_pen: 1.2953e-06 - val_loss: 1.0033e-05 - val_mse: 8.7239e-06 - val_f_pen: 1.3095e-06\n",
      "Epoch 317/10000\n",
      "98/98 [==============================] - 4s 37ms/step - loss: 7.9150e-06 - mse: 6.6205e-06 - f_pen: 1.2945e-06 - val_loss: 9.9027e-06 - val_mse: 8.5934e-06 - val_f_pen: 1.3093e-06\n",
      "Epoch 318/10000\n",
      "98/98 [==============================] - 4s 37ms/step - loss: 8.6371e-06 - mse: 7.3431e-06 - f_pen: 1.2940e-06 - val_loss: 9.8065e-06 - val_mse: 8.4998e-06 - val_f_pen: 1.3067e-06\n",
      "Epoch 319/10000\n",
      "98/98 [==============================] - 4s 37ms/step - loss: 7.8989e-06 - mse: 6.6056e-06 - f_pen: 1.2933e-06 - val_loss: 9.7523e-06 - val_mse: 8.4451e-06 - val_f_pen: 1.3072e-06\n",
      "Epoch 320/10000\n",
      "98/98 [==============================] - 4s 37ms/step - loss: 8.3584e-06 - mse: 7.0656e-06 - f_pen: 1.2928e-06 - val_loss: 1.0307e-05 - val_mse: 8.9990e-06 - val_f_pen: 1.3085e-06\n",
      "Epoch 321/10000\n",
      "98/98 [==============================] - 4s 38ms/step - loss: 8.6689e-06 - mse: 7.3770e-06 - f_pen: 1.2919e-06 - val_loss: 1.0420e-05 - val_mse: 9.1134e-06 - val_f_pen: 1.3062e-06\n",
      "Epoch 322/10000\n",
      "98/98 [==============================] - 4s 38ms/step - loss: 7.1679e-06 - mse: 5.8766e-06 - f_pen: 1.2913e-06 - val_loss: 9.7463e-06 - val_mse: 8.4408e-06 - val_f_pen: 1.3055e-06\n",
      "Epoch 323/10000\n",
      "98/98 [==============================] - 4s 37ms/step - loss: 7.1581e-06 - mse: 5.8677e-06 - f_pen: 1.2904e-06 - val_loss: 1.0560e-05 - val_mse: 9.2560e-06 - val_f_pen: 1.3042e-06\n",
      "Epoch 324/10000\n",
      "98/98 [==============================] - 3s 36ms/step - loss: 8.6997e-06 - mse: 7.4094e-06 - f_pen: 1.2904e-06 - val_loss: 1.0274e-05 - val_mse: 8.9690e-06 - val_f_pen: 1.3054e-06\n",
      "Epoch 325/10000\n",
      "98/98 [==============================] - 4s 36ms/step - loss: 7.8839e-06 - mse: 6.5939e-06 - f_pen: 1.2900e-06 - val_loss: 8.8028e-06 - val_mse: 7.4979e-06 - val_f_pen: 1.3049e-06\n",
      "Epoch 326/10000\n",
      "98/98 [==============================] - 4s 37ms/step - loss: 6.1363e-06 - mse: 4.8475e-06 - f_pen: 1.2889e-06 - val_loss: 9.1239e-06 - val_mse: 7.8216e-06 - val_f_pen: 1.3022e-06\n",
      "Epoch 327/10000\n",
      "98/98 [==============================] - 4s 38ms/step - loss: 8.7256e-06 - mse: 7.4373e-06 - f_pen: 1.2882e-06 - val_loss: 9.7923e-06 - val_mse: 8.4893e-06 - val_f_pen: 1.3030e-06\n",
      "Epoch 328/10000\n",
      "98/98 [==============================] - 4s 37ms/step - loss: 7.1256e-06 - mse: 5.8382e-06 - f_pen: 1.2874e-06 - val_loss: 9.7433e-06 - val_mse: 8.4438e-06 - val_f_pen: 1.2995e-06\n",
      "Epoch 329/10000\n",
      "98/98 [==============================] - 4s 37ms/step - loss: 8.5009e-06 - mse: 7.2140e-06 - f_pen: 1.2869e-06 - val_loss: 9.2360e-06 - val_mse: 7.9336e-06 - val_f_pen: 1.3023e-06\n",
      "Epoch 330/10000\n",
      "98/98 [==============================] - 4s 37ms/step - loss: 7.5157e-06 - mse: 6.2295e-06 - f_pen: 1.2861e-06 - val_loss: 9.1714e-06 - val_mse: 7.8694e-06 - val_f_pen: 1.3019e-06\n",
      "Epoch 331/10000\n",
      "98/98 [==============================] - 4s 36ms/step - loss: 6.9012e-06 - mse: 5.6155e-06 - f_pen: 1.2857e-06 - val_loss: 1.0012e-05 - val_mse: 8.7111e-06 - val_f_pen: 1.3005e-06\n",
      "Epoch 332/10000\n",
      "98/98 [==============================] - 4s 37ms/step - loss: 7.9003e-06 - mse: 6.6152e-06 - f_pen: 1.2851e-06 - val_loss: 9.4571e-06 - val_mse: 8.1590e-06 - val_f_pen: 1.2981e-06\n",
      "Epoch 333/10000\n",
      "98/98 [==============================] - 4s 37ms/step - loss: 7.0036e-06 - mse: 5.7192e-06 - f_pen: 1.2845e-06 - val_loss: 9.0893e-06 - val_mse: 7.7918e-06 - val_f_pen: 1.2974e-06\n",
      "Epoch 334/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "98/98 [==============================] - 4s 36ms/step - loss: 7.5125e-06 - mse: 6.2293e-06 - f_pen: 1.2832e-06 - val_loss: 9.6902e-06 - val_mse: 8.3911e-06 - val_f_pen: 1.2991e-06\n",
      "Epoch 335/10000\n",
      "98/98 [==============================] - 4s 38ms/step - loss: 9.4040e-06 - mse: 8.1211e-06 - f_pen: 1.2829e-06 - val_loss: 9.0065e-06 - val_mse: 7.7098e-06 - val_f_pen: 1.2967e-06\n",
      "Epoch 336/10000\n",
      "98/98 [==============================] - 4s 37ms/step - loss: 7.1128e-06 - mse: 5.8303e-06 - f_pen: 1.2825e-06 - val_loss: 9.4775e-06 - val_mse: 8.1788e-06 - val_f_pen: 1.2987e-06\n",
      "Epoch 337/10000\n",
      "98/98 [==============================] - 4s 37ms/step - loss: 6.2952e-06 - mse: 5.0130e-06 - f_pen: 1.2822e-06 - val_loss: 8.6823e-06 - val_mse: 7.3858e-06 - val_f_pen: 1.2965e-06\n",
      "Epoch 338/10000\n",
      "98/98 [==============================] - 4s 37ms/step - loss: 7.3036e-06 - mse: 6.0221e-06 - f_pen: 1.2815e-06 - val_loss: 9.2612e-06 - val_mse: 7.9652e-06 - val_f_pen: 1.2960e-06\n",
      "Epoch 339/10000\n",
      "98/98 [==============================] - 4s 37ms/step - loss: 8.4304e-06 - mse: 7.1501e-06 - f_pen: 1.2803e-06 - val_loss: 9.3454e-06 - val_mse: 8.0501e-06 - val_f_pen: 1.2954e-06\n",
      "Epoch 340/10000\n",
      "98/98 [==============================] - 4s 37ms/step - loss: 7.1978e-06 - mse: 5.9176e-06 - f_pen: 1.2802e-06 - val_loss: 9.2090e-06 - val_mse: 7.9140e-06 - val_f_pen: 1.2950e-06\n",
      "Epoch 341/10000\n",
      "98/98 [==============================] - 4s 36ms/step - loss: 6.6444e-06 - mse: 5.3651e-06 - f_pen: 1.2793e-06 - val_loss: 8.4812e-06 - val_mse: 7.1868e-06 - val_f_pen: 1.2944e-06\n",
      "Epoch 342/10000\n",
      "98/98 [==============================] - 4s 37ms/step - loss: 8.1467e-06 - mse: 6.8680e-06 - f_pen: 1.2787e-06 - val_loss: 9.2368e-06 - val_mse: 7.9430e-06 - val_f_pen: 1.2938e-06\n",
      "Epoch 343/10000\n",
      "98/98 [==============================] - 4s 38ms/step - loss: 7.5789e-06 - mse: 6.3006e-06 - f_pen: 1.2784e-06 - val_loss: 1.0507e-05 - val_mse: 9.2138e-06 - val_f_pen: 1.2931e-06\n",
      "Epoch 344/10000\n",
      "98/98 [==============================] - 4s 37ms/step - loss: 7.1064e-06 - mse: 5.8289e-06 - f_pen: 1.2775e-06 - val_loss: 8.1765e-06 - val_mse: 6.8840e-06 - val_f_pen: 1.2925e-06\n",
      "Epoch 345/10000\n",
      "98/98 [==============================] - 4s 37ms/step - loss: 6.8272e-06 - mse: 5.5502e-06 - f_pen: 1.2770e-06 - val_loss: 8.9894e-06 - val_mse: 7.6988e-06 - val_f_pen: 1.2906e-06\n",
      "Epoch 346/10000\n",
      "98/98 [==============================] - 4s 37ms/step - loss: 7.0581e-06 - mse: 5.7819e-06 - f_pen: 1.2762e-06 - val_loss: 1.0238e-05 - val_mse: 8.9482e-06 - val_f_pen: 1.2897e-06\n",
      "Epoch 347/10000\n",
      "98/98 [==============================] - 4s 37ms/step - loss: 7.9651e-06 - mse: 6.6895e-06 - f_pen: 1.2756e-06 - val_loss: 8.9215e-06 - val_mse: 7.6303e-06 - val_f_pen: 1.2912e-06\n",
      "Epoch 348/10000\n",
      "98/98 [==============================] - 4s 37ms/step - loss: 6.9358e-06 - mse: 5.6602e-06 - f_pen: 1.2755e-06 - val_loss: 9.0727e-06 - val_mse: 7.7865e-06 - val_f_pen: 1.2863e-06\n",
      "Epoch 349/10000\n",
      "98/98 [==============================] - 4s 36ms/step - loss: 8.3959e-06 - mse: 7.1214e-06 - f_pen: 1.2745e-06 - val_loss: 1.0628e-05 - val_mse: 9.3409e-06 - val_f_pen: 1.2873e-06\n",
      "Epoch 350/10000\n",
      "98/98 [==============================] - 4s 36ms/step - loss: 8.5573e-06 - mse: 7.2831e-06 - f_pen: 1.2743e-06 - val_loss: 8.2090e-06 - val_mse: 6.9212e-06 - val_f_pen: 1.2878e-06\n",
      "Epoch 351/10000\n",
      "98/98 [==============================] - 4s 36ms/step - loss: 5.7660e-06 - mse: 4.4922e-06 - f_pen: 1.2738e-06 - val_loss: 8.0947e-06 - val_mse: 6.8088e-06 - val_f_pen: 1.2859e-06\n",
      "Epoch 352/10000\n",
      "98/98 [==============================] - 4s 36ms/step - loss: 6.0938e-06 - mse: 4.8211e-06 - f_pen: 1.2727e-06 - val_loss: 7.7999e-06 - val_mse: 6.5140e-06 - val_f_pen: 1.2859e-06\n",
      "Epoch 353/10000\n",
      "98/98 [==============================] - 4s 37ms/step - loss: 6.8287e-06 - mse: 5.5567e-06 - f_pen: 1.2720e-06 - val_loss: 8.7463e-06 - val_mse: 7.4608e-06 - val_f_pen: 1.2855e-06\n",
      "Epoch 354/10000\n",
      "98/98 [==============================] - 4s 37ms/step - loss: 6.9857e-06 - mse: 5.7140e-06 - f_pen: 1.2717e-06 - val_loss: 8.4128e-06 - val_mse: 7.1273e-06 - val_f_pen: 1.2855e-06\n",
      "Epoch 355/10000\n",
      "98/98 [==============================] - 4s 37ms/step - loss: 6.9822e-06 - mse: 5.7113e-06 - f_pen: 1.2709e-06 - val_loss: 7.8715e-06 - val_mse: 6.5875e-06 - val_f_pen: 1.2839e-06\n",
      "Epoch 356/10000\n",
      "98/98 [==============================] - 4s 36ms/step - loss: 7.3168e-06 - mse: 6.0461e-06 - f_pen: 1.2707e-06 - val_loss: 8.0680e-06 - val_mse: 6.7833e-06 - val_f_pen: 1.2847e-06\n",
      "Epoch 357/10000\n",
      "98/98 [==============================] - 4s 37ms/step - loss: 6.8312e-06 - mse: 5.5613e-06 - f_pen: 1.2699e-06 - val_loss: 8.2756e-06 - val_mse: 6.9910e-06 - val_f_pen: 1.2845e-06\n",
      "Epoch 358/10000\n",
      "98/98 [==============================] - 4s 36ms/step - loss: 7.0080e-06 - mse: 5.7386e-06 - f_pen: 1.2694e-06 - val_loss: 7.8921e-06 - val_mse: 6.6085e-06 - val_f_pen: 1.2836e-06\n",
      "Epoch 359/10000\n",
      "98/98 [==============================] - 4s 37ms/step - loss: 7.0595e-06 - mse: 5.7909e-06 - f_pen: 1.2686e-06 - val_loss: 9.3579e-06 - val_mse: 8.0782e-06 - val_f_pen: 1.2797e-06\n",
      "Epoch 360/10000\n",
      "98/98 [==============================] - 4s 38ms/step - loss: 7.8192e-06 - mse: 6.5510e-06 - f_pen: 1.2682e-06 - val_loss: 8.3109e-06 - val_mse: 7.0294e-06 - val_f_pen: 1.2815e-06\n",
      "Epoch 361/10000\n",
      "98/98 [==============================] - 4s 37ms/step - loss: 6.7805e-06 - mse: 5.5132e-06 - f_pen: 1.2674e-06 - val_loss: 8.2278e-06 - val_mse: 6.9457e-06 - val_f_pen: 1.2822e-06\n",
      "Epoch 362/10000\n",
      "98/98 [==============================] - 4s 37ms/step - loss: 7.1175e-06 - mse: 5.8505e-06 - f_pen: 1.2671e-06 - val_loss: 7.9679e-06 - val_mse: 6.6859e-06 - val_f_pen: 1.2819e-06\n",
      "Epoch 363/10000\n",
      "98/98 [==============================] - 4s 36ms/step - loss: 7.1187e-06 - mse: 5.8519e-06 - f_pen: 1.2668e-06 - val_loss: 8.9001e-06 - val_mse: 7.6208e-06 - val_f_pen: 1.2794e-06\n",
      "Epoch 364/10000\n",
      "98/98 [==============================] - 4s 37ms/step - loss: 6.5327e-06 - mse: 5.2666e-06 - f_pen: 1.2661e-06 - val_loss: 1.0051e-05 - val_mse: 8.7722e-06 - val_f_pen: 1.2783e-06\n",
      "Epoch 365/10000\n",
      "98/98 [==============================] - 4s 36ms/step - loss: 7.2451e-06 - mse: 5.9796e-06 - f_pen: 1.2655e-06 - val_loss: 8.7065e-06 - val_mse: 7.4282e-06 - val_f_pen: 1.2783e-06\n",
      "Epoch 366/10000\n",
      "98/98 [==============================] - 4s 38ms/step - loss: 7.3122e-06 - mse: 6.0470e-06 - f_pen: 1.2651e-06 - val_loss: 8.0866e-06 - val_mse: 6.8073e-06 - val_f_pen: 1.2793e-06\n",
      "Epoch 367/10000\n",
      "98/98 [==============================] - 4s 37ms/step - loss: 7.2383e-06 - mse: 5.9735e-06 - f_pen: 1.2647e-06 - val_loss: 8.6797e-06 - val_mse: 7.4010e-06 - val_f_pen: 1.2787e-06\n",
      "Epoch 368/10000\n",
      "98/98 [==============================] - 4s 36ms/step - loss: 7.1114e-06 - mse: 5.8471e-06 - f_pen: 1.2642e-06 - val_loss: 8.1958e-06 - val_mse: 6.9190e-06 - val_f_pen: 1.2768e-06\n",
      "Epoch 369/10000\n",
      "98/98 [==============================] - 4s 37ms/step - loss: 6.4082e-06 - mse: 5.1445e-06 - f_pen: 1.2637e-06 - val_loss: 8.1816e-06 - val_mse: 6.9061e-06 - val_f_pen: 1.2755e-06\n",
      "Epoch 370/10000\n",
      "98/98 [==============================] - 4s 37ms/step - loss: 6.3628e-06 - mse: 5.0997e-06 - f_pen: 1.2631e-06 - val_loss: 8.2031e-06 - val_mse: 6.9264e-06 - val_f_pen: 1.2767e-06\n",
      "Epoch 371/10000\n",
      "98/98 [==============================] - 4s 37ms/step - loss: 7.4675e-06 - mse: 6.2052e-06 - f_pen: 1.2623e-06 - val_loss: 8.6449e-06 - val_mse: 7.3706e-06 - val_f_pen: 1.2742e-06\n",
      "Epoch 372/10000\n",
      "98/98 [==============================] - 4s 37ms/step - loss: 8.8090e-06 - mse: 7.5473e-06 - f_pen: 1.2617e-06 - val_loss: 7.5780e-06 - val_mse: 6.3026e-06 - val_f_pen: 1.2754e-06\n",
      "Epoch 373/10000\n",
      "98/98 [==============================] - 4s 37ms/step - loss: 6.5593e-06 - mse: 5.2981e-06 - f_pen: 1.2612e-06 - val_loss: 7.4973e-06 - val_mse: 6.2215e-06 - val_f_pen: 1.2759e-06\n",
      "Epoch 374/10000\n",
      "98/98 [==============================] - 4s 36ms/step - loss: 5.7704e-06 - mse: 4.5099e-06 - f_pen: 1.2605e-06 - val_loss: 8.3812e-06 - val_mse: 7.1062e-06 - val_f_pen: 1.2749e-06\n",
      "Epoch 375/10000\n",
      "98/98 [==============================] - 4s 36ms/step - loss: 6.1311e-06 - mse: 4.8712e-06 - f_pen: 1.2599e-06 - val_loss: 8.3116e-06 - val_mse: 7.0374e-06 - val_f_pen: 1.2742e-06\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 376/10000\n",
      "98/98 [==============================] - 4s 36ms/step - loss: 7.4409e-06 - mse: 6.1815e-06 - f_pen: 1.2594e-06 - val_loss: 8.5610e-06 - val_mse: 7.2868e-06 - val_f_pen: 1.2742e-06\n",
      "Epoch 377/10000\n",
      "98/98 [==============================] - 4s 37ms/step - loss: 6.3954e-06 - mse: 5.1369e-06 - f_pen: 1.2585e-06 - val_loss: 7.9264e-06 - val_mse: 6.6524e-06 - val_f_pen: 1.2740e-06\n",
      "Epoch 378/10000\n",
      "98/98 [==============================] - 4s 37ms/step - loss: 7.2208e-06 - mse: 5.9626e-06 - f_pen: 1.2582e-06 - val_loss: 7.6247e-06 - val_mse: 6.3531e-06 - val_f_pen: 1.2716e-06\n",
      "Epoch 379/10000\n",
      "98/98 [==============================] - 4s 37ms/step - loss: 5.7904e-06 - mse: 4.5329e-06 - f_pen: 1.2575e-06 - val_loss: 7.5810e-06 - val_mse: 6.3089e-06 - val_f_pen: 1.2720e-06\n",
      "Epoch 380/10000\n",
      "98/98 [==============================] - 4s 38ms/step - loss: 9.0663e-06 - mse: 7.8095e-06 - f_pen: 1.2568e-06 - val_loss: 8.4114e-06 - val_mse: 7.1395e-06 - val_f_pen: 1.2718e-06\n",
      "Epoch 381/10000\n",
      "98/98 [==============================] - 4s 37ms/step - loss: 6.1065e-06 - mse: 4.8504e-06 - f_pen: 1.2561e-06 - val_loss: 7.7734e-06 - val_mse: 6.5032e-06 - val_f_pen: 1.2701e-06\n",
      "Epoch 382/10000\n",
      "98/98 [==============================] - 4s 37ms/step - loss: 5.3974e-06 - mse: 4.1416e-06 - f_pen: 1.2558e-06 - val_loss: 7.8673e-06 - val_mse: 6.5991e-06 - val_f_pen: 1.2682e-06\n",
      "Epoch 383/10000\n",
      "98/98 [==============================] - 4s 37ms/step - loss: 7.8190e-06 - mse: 6.5642e-06 - f_pen: 1.2548e-06 - val_loss: 8.0004e-06 - val_mse: 6.7297e-06 - val_f_pen: 1.2707e-06\n",
      "Epoch 384/10000\n",
      "98/98 [==============================] - 4s 37ms/step - loss: 6.6658e-06 - mse: 5.4112e-06 - f_pen: 1.2546e-06 - val_loss: 7.2832e-06 - val_mse: 6.0155e-06 - val_f_pen: 1.2677e-06\n",
      "Epoch 385/10000\n",
      "98/98 [==============================] - 4s 37ms/step - loss: 6.0234e-06 - mse: 4.7695e-06 - f_pen: 1.2539e-06 - val_loss: 7.0675e-06 - val_mse: 5.8007e-06 - val_f_pen: 1.2668e-06\n",
      "Epoch 386/10000\n",
      "98/98 [==============================] - 4s 38ms/step - loss: 7.4390e-06 - mse: 6.1858e-06 - f_pen: 1.2532e-06 - val_loss: 8.7438e-06 - val_mse: 7.4765e-06 - val_f_pen: 1.2673e-06\n",
      "Epoch 387/10000\n",
      "98/98 [==============================] - 4s 37ms/step - loss: 6.4170e-06 - mse: 5.1638e-06 - f_pen: 1.2532e-06 - val_loss: 7.8726e-06 - val_mse: 6.6074e-06 - val_f_pen: 1.2652e-06\n",
      "Epoch 388/10000\n",
      "98/98 [==============================] - 4s 37ms/step - loss: 6.3290e-06 - mse: 5.0767e-06 - f_pen: 1.2524e-06 - val_loss: 7.3238e-06 - val_mse: 6.0562e-06 - val_f_pen: 1.2676e-06\n",
      "Epoch 389/10000\n",
      "98/98 [==============================] - 4s 37ms/step - loss: 6.9738e-06 - mse: 5.7216e-06 - f_pen: 1.2522e-06 - val_loss: 8.2289e-06 - val_mse: 6.9615e-06 - val_f_pen: 1.2674e-06\n",
      "Epoch 390/10000\n",
      "98/98 [==============================] - 4s 37ms/step - loss: 8.2813e-06 - mse: 7.0296e-06 - f_pen: 1.2517e-06 - val_loss: 7.4350e-06 - val_mse: 6.1708e-06 - val_f_pen: 1.2642e-06\n",
      "Epoch 391/10000\n",
      "98/98 [==============================] - 4s 37ms/step - loss: 5.9966e-06 - mse: 4.7457e-06 - f_pen: 1.2509e-06 - val_loss: 7.1241e-06 - val_mse: 5.8604e-06 - val_f_pen: 1.2637e-06\n",
      "Epoch 392/10000\n",
      "98/98 [==============================] - 4s 37ms/step - loss: 5.6273e-06 - mse: 4.3771e-06 - f_pen: 1.2502e-06 - val_loss: 7.3560e-06 - val_mse: 6.0926e-06 - val_f_pen: 1.2634e-06\n",
      "Epoch 393/10000\n",
      "98/98 [==============================] - 4s 37ms/step - loss: 6.6626e-06 - mse: 5.4134e-06 - f_pen: 1.2493e-06 - val_loss: 7.1532e-06 - val_mse: 5.8899e-06 - val_f_pen: 1.2633e-06\n",
      "Epoch 394/10000\n",
      "98/98 [==============================] - 4s 37ms/step - loss: 6.1460e-06 - mse: 4.8971e-06 - f_pen: 1.2489e-06 - val_loss: 8.2084e-06 - val_mse: 6.9449e-06 - val_f_pen: 1.2635e-06\n",
      "Epoch 395/10000\n",
      "98/98 [==============================] - 4s 38ms/step - loss: 6.6610e-06 - mse: 5.4126e-06 - f_pen: 1.2484e-06 - val_loss: 7.2771e-06 - val_mse: 6.0152e-06 - val_f_pen: 1.2619e-06\n",
      "Epoch 396/10000\n",
      "98/98 [==============================] - 4s 37ms/step - loss: 6.1032e-06 - mse: 4.8555e-06 - f_pen: 1.2477e-06 - val_loss: 7.4665e-06 - val_mse: 6.2056e-06 - val_f_pen: 1.2609e-06\n",
      "Epoch 397/10000\n",
      "98/98 [==============================] - 4s 37ms/step - loss: 6.9807e-06 - mse: 5.7333e-06 - f_pen: 1.2474e-06 - val_loss: 7.4211e-06 - val_mse: 6.1617e-06 - val_f_pen: 1.2594e-06\n",
      "Epoch 398/10000\n",
      "98/98 [==============================] - 4s 37ms/step - loss: 6.5804e-06 - mse: 5.3331e-06 - f_pen: 1.2473e-06 - val_loss: 7.3439e-06 - val_mse: 6.0839e-06 - val_f_pen: 1.2600e-06\n",
      "Epoch 399/10000\n",
      "98/98 [==============================] - 4s 37ms/step - loss: 5.7220e-06 - mse: 4.4754e-06 - f_pen: 1.2465e-06 - val_loss: 7.6341e-06 - val_mse: 6.3727e-06 - val_f_pen: 1.2614e-06\n",
      "Epoch 400/10000\n",
      "98/98 [==============================] - 4s 38ms/step - loss: 7.5758e-06 - mse: 6.3299e-06 - f_pen: 1.2459e-06 - val_loss: 7.1691e-06 - val_mse: 5.9085e-06 - val_f_pen: 1.2606e-06\n",
      "Epoch 401/10000\n",
      "98/98 [==============================] - 4s 37ms/step - loss: 5.8998e-06 - mse: 4.6544e-06 - f_pen: 1.2454e-06 - val_loss: 7.5742e-06 - val_mse: 6.3149e-06 - val_f_pen: 1.2593e-06\n",
      "Epoch 402/10000\n",
      "98/98 [==============================] - 4s 37ms/step - loss: 7.5437e-06 - mse: 6.2986e-06 - f_pen: 1.2451e-06 - val_loss: 9.0154e-06 - val_mse: 7.7569e-06 - val_f_pen: 1.2585e-06\n",
      "Epoch 403/10000\n",
      "98/98 [==============================] - 4s 36ms/step - loss: 6.4858e-06 - mse: 5.2415e-06 - f_pen: 1.2442e-06 - val_loss: 7.2894e-06 - val_mse: 6.0308e-06 - val_f_pen: 1.2586e-06\n",
      "Epoch 404/10000\n",
      "98/98 [==============================] - 4s 37ms/step - loss: 6.1538e-06 - mse: 4.9103e-06 - f_pen: 1.2435e-06 - val_loss: 7.1254e-06 - val_mse: 5.8689e-06 - val_f_pen: 1.2565e-06\n",
      "Epoch 405/10000\n",
      "98/98 [==============================] - 4s 37ms/step - loss: 6.8954e-06 - mse: 5.6521e-06 - f_pen: 1.2433e-06 - val_loss: 8.2539e-06 - val_mse: 6.9980e-06 - val_f_pen: 1.2559e-06\n",
      "\n",
      "Epoch 00405: ReduceLROnPlateau reducing learning rate to 8.235429777414538e-05.\n",
      "Epoch 406/10000\n",
      "98/98 [==============================] - 4s 37ms/step - loss: 4.2428e-06 - mse: 2.9996e-06 - f_pen: 1.2432e-06 - val_loss: 6.3902e-06 - val_mse: 5.1333e-06 - val_f_pen: 1.2569e-06\n",
      "Epoch 407/10000\n",
      "98/98 [==============================] - 4s 36ms/step - loss: 4.0421e-06 - mse: 2.7995e-06 - f_pen: 1.2426e-06 - val_loss: 6.3508e-06 - val_mse: 5.0956e-06 - val_f_pen: 1.2552e-06\n",
      "Epoch 408/10000\n",
      "98/98 [==============================] - 4s 37ms/step - loss: 4.0471e-06 - mse: 2.8055e-06 - f_pen: 1.2417e-06 - val_loss: 6.5543e-06 - val_mse: 5.2997e-06 - val_f_pen: 1.2546e-06\n",
      "Epoch 409/10000\n",
      "98/98 [==============================] - 4s 36ms/step - loss: 4.0251e-06 - mse: 2.7840e-06 - f_pen: 1.2411e-06 - val_loss: 6.5578e-06 - val_mse: 5.3028e-06 - val_f_pen: 1.2550e-06\n",
      "Epoch 410/10000\n",
      "98/98 [==============================] - 4s 37ms/step - loss: 4.2458e-06 - mse: 3.0051e-06 - f_pen: 1.2407e-06 - val_loss: 6.4991e-06 - val_mse: 5.2455e-06 - val_f_pen: 1.2536e-06\n",
      "Epoch 411/10000\n",
      "98/98 [==============================] - 4s 37ms/step - loss: 4.2435e-06 - mse: 3.0036e-06 - f_pen: 1.2400e-06 - val_loss: 6.4529e-06 - val_mse: 5.2001e-06 - val_f_pen: 1.2528e-06\n",
      "Epoch 412/10000\n",
      "98/98 [==============================] - 4s 36ms/step - loss: 4.2752e-06 - mse: 3.0359e-06 - f_pen: 1.2393e-06 - val_loss: 6.9308e-06 - val_mse: 5.6784e-06 - val_f_pen: 1.2524e-06\n",
      "Epoch 413/10000\n",
      "98/98 [==============================] - 4s 36ms/step - loss: 4.5907e-06 - mse: 3.3522e-06 - f_pen: 1.2384e-06 - val_loss: 6.5850e-06 - val_mse: 5.3326e-06 - val_f_pen: 1.2524e-06\n",
      "Epoch 414/10000\n",
      "98/98 [==============================] - 4s 37ms/step - loss: 5.1182e-06 - mse: 3.8801e-06 - f_pen: 1.2381e-06 - val_loss: 6.8229e-06 - val_mse: 5.5725e-06 - val_f_pen: 1.2504e-06\n",
      "Epoch 415/10000\n",
      "98/98 [==============================] - 4s 37ms/step - loss: 4.8650e-06 - mse: 3.6274e-06 - f_pen: 1.2376e-06 - val_loss: 7.1171e-06 - val_mse: 5.8660e-06 - val_f_pen: 1.2511e-06\n",
      "Epoch 416/10000\n",
      "98/98 [==============================] - 4s 37ms/step - loss: 4.6982e-06 - mse: 3.4611e-06 - f_pen: 1.2372e-06 - val_loss: 6.8090e-06 - val_mse: 5.5586e-06 - val_f_pen: 1.2504e-06\n",
      "Epoch 417/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "98/98 [==============================] - 4s 37ms/step - loss: 4.7949e-06 - mse: 3.5584e-06 - f_pen: 1.2365e-06 - val_loss: 7.8795e-06 - val_mse: 6.6306e-06 - val_f_pen: 1.2489e-06\n",
      "Epoch 418/10000\n",
      "98/98 [==============================] - 4s 37ms/step - loss: 5.1089e-06 - mse: 3.8729e-06 - f_pen: 1.2361e-06 - val_loss: 6.2807e-06 - val_mse: 5.0311e-06 - val_f_pen: 1.2496e-06\n",
      "Epoch 419/10000\n",
      "98/98 [==============================] - 4s 36ms/step - loss: 5.1640e-06 - mse: 3.9287e-06 - f_pen: 1.2354e-06 - val_loss: 7.4275e-06 - val_mse: 6.1779e-06 - val_f_pen: 1.2497e-06\n",
      "Epoch 420/10000\n",
      "98/98 [==============================] - 4s 36ms/step - loss: 5.1894e-06 - mse: 3.9543e-06 - f_pen: 1.2350e-06 - val_loss: 6.4188e-06 - val_mse: 5.1718e-06 - val_f_pen: 1.2471e-06\n",
      "Epoch 421/10000\n",
      "98/98 [==============================] - 4s 39ms/step - loss: 4.8735e-06 - mse: 3.6391e-06 - f_pen: 1.2344e-06 - val_loss: 6.8055e-06 - val_mse: 5.5572e-06 - val_f_pen: 1.2483e-06\n",
      "Epoch 422/10000\n",
      "98/98 [==============================] - 4s 36ms/step - loss: 5.2807e-06 - mse: 4.0467e-06 - f_pen: 1.2340e-06 - val_loss: 6.8469e-06 - val_mse: 5.5992e-06 - val_f_pen: 1.2477e-06\n",
      "Epoch 423/10000\n",
      "98/98 [==============================] - 4s 37ms/step - loss: 4.7252e-06 - mse: 3.4919e-06 - f_pen: 1.2332e-06 - val_loss: 6.4693e-06 - val_mse: 5.2223e-06 - val_f_pen: 1.2470e-06\n",
      "Epoch 424/10000\n",
      "98/98 [==============================] - 4s 40ms/step - loss: 4.8539e-06 - mse: 3.6211e-06 - f_pen: 1.2328e-06 - val_loss: 6.4198e-06 - val_mse: 5.1728e-06 - val_f_pen: 1.2469e-06\n",
      "Epoch 425/10000\n",
      "98/98 [==============================] - 4s 37ms/step - loss: 5.2219e-06 - mse: 3.9899e-06 - f_pen: 1.2320e-06 - val_loss: 6.7203e-06 - val_mse: 5.4744e-06 - val_f_pen: 1.2460e-06\n",
      "Epoch 426/10000\n",
      "98/98 [==============================] - 4s 38ms/step - loss: 5.4593e-06 - mse: 4.2276e-06 - f_pen: 1.2317e-06 - val_loss: 6.5305e-06 - val_mse: 5.2860e-06 - val_f_pen: 1.2445e-06\n",
      "Epoch 427/10000\n",
      "98/98 [==============================] - 4s 37ms/step - loss: 4.3963e-06 - mse: 3.1651e-06 - f_pen: 1.2312e-06 - val_loss: 6.5803e-06 - val_mse: 5.3353e-06 - val_f_pen: 1.2450e-06\n",
      "Epoch 428/10000\n",
      "98/98 [==============================] - 4s 37ms/step - loss: 4.5384e-06 - mse: 3.3077e-06 - f_pen: 1.2307e-06 - val_loss: 6.2975e-06 - val_mse: 5.0538e-06 - val_f_pen: 1.2437e-06\n",
      "Epoch 429/10000\n",
      "98/98 [==============================] - 4s 38ms/step - loss: 5.5730e-06 - mse: 4.3429e-06 - f_pen: 1.2301e-06 - val_loss: 6.6563e-06 - val_mse: 5.4120e-06 - val_f_pen: 1.2443e-06\n",
      "Epoch 430/10000\n",
      "98/98 [==============================] - 4s 36ms/step - loss: 4.8082e-06 - mse: 3.5786e-06 - f_pen: 1.2296e-06 - val_loss: 6.5299e-06 - val_mse: 5.2870e-06 - val_f_pen: 1.2430e-06\n",
      "Epoch 431/10000\n",
      "98/98 [==============================] - 4s 37ms/step - loss: 4.9913e-06 - mse: 3.7625e-06 - f_pen: 1.2288e-06 - val_loss: 6.6805e-06 - val_mse: 5.4373e-06 - val_f_pen: 1.2432e-06\n",
      "Epoch 432/10000\n",
      "98/98 [==============================] - 4s 37ms/step - loss: 5.1233e-06 - mse: 3.8949e-06 - f_pen: 1.2285e-06 - val_loss: 6.4872e-06 - val_mse: 5.2457e-06 - val_f_pen: 1.2415e-06\n",
      "Epoch 433/10000\n",
      "98/98 [==============================] - 4s 37ms/step - loss: 4.7019e-06 - mse: 3.4741e-06 - f_pen: 1.2278e-06 - val_loss: 6.7588e-06 - val_mse: 5.5188e-06 - val_f_pen: 1.2400e-06\n",
      "Epoch 434/10000\n",
      "98/98 [==============================] - 4s 37ms/step - loss: 4.5949e-06 - mse: 3.3677e-06 - f_pen: 1.2272e-06 - val_loss: 6.4211e-06 - val_mse: 5.1806e-06 - val_f_pen: 1.2405e-06\n",
      "Epoch 435/10000\n",
      "98/98 [==============================] - 4s 37ms/step - loss: 5.1709e-06 - mse: 3.9443e-06 - f_pen: 1.2266e-06 - val_loss: 6.5727e-06 - val_mse: 5.3340e-06 - val_f_pen: 1.2387e-06\n",
      "Epoch 436/10000\n",
      "98/98 [==============================] - 4s 38ms/step - loss: 4.9370e-06 - mse: 3.7111e-06 - f_pen: 1.2259e-06 - val_loss: 6.5171e-06 - val_mse: 5.2774e-06 - val_f_pen: 1.2396e-06\n",
      "Epoch 437/10000\n",
      "98/98 [==============================] - 4s 37ms/step - loss: 4.8231e-06 - mse: 3.5974e-06 - f_pen: 1.2256e-06 - val_loss: 6.9085e-06 - val_mse: 5.6713e-06 - val_f_pen: 1.2372e-06\n",
      "Epoch 438/10000\n",
      "98/98 [==============================] - 4s 38ms/step - loss: 4.7879e-06 - mse: 3.5629e-06 - f_pen: 1.2250e-06 - val_loss: 7.1430e-06 - val_mse: 5.9072e-06 - val_f_pen: 1.2358e-06\n",
      "\n",
      "Epoch 00438: ReduceLROnPlateau reducing learning rate to 5.76480058953166e-05.\n",
      "Epoch 439/10000\n",
      "98/98 [==============================] - 4s 37ms/step - loss: 3.8899e-06 - mse: 2.6654e-06 - f_pen: 1.2246e-06 - val_loss: 5.7515e-06 - val_mse: 4.5136e-06 - val_f_pen: 1.2379e-06\n",
      "Epoch 440/10000\n",
      "98/98 [==============================] - 4s 37ms/step - loss: 3.6410e-06 - mse: 2.4169e-06 - f_pen: 1.2241e-06 - val_loss: 5.7360e-06 - val_mse: 4.4988e-06 - val_f_pen: 1.2372e-06\n",
      "Epoch 441/10000\n",
      "98/98 [==============================] - 4s 37ms/step - loss: 3.6482e-06 - mse: 2.4247e-06 - f_pen: 1.2235e-06 - val_loss: 5.6424e-06 - val_mse: 4.4054e-06 - val_f_pen: 1.2370e-06\n",
      "Epoch 442/10000\n",
      "98/98 [==============================] - 4s 37ms/step - loss: 3.6698e-06 - mse: 2.4467e-06 - f_pen: 1.2231e-06 - val_loss: 5.8118e-06 - val_mse: 4.5770e-06 - val_f_pen: 1.2349e-06\n",
      "Epoch 443/10000\n",
      "98/98 [==============================] - 4s 37ms/step - loss: 3.7386e-06 - mse: 2.5160e-06 - f_pen: 1.2227e-06 - val_loss: 5.7784e-06 - val_mse: 4.5428e-06 - val_f_pen: 1.2356e-06\n",
      "Epoch 444/10000\n",
      "98/98 [==============================] - 4s 38ms/step - loss: 3.7606e-06 - mse: 2.5385e-06 - f_pen: 1.2220e-06 - val_loss: 5.8969e-06 - val_mse: 4.6613e-06 - val_f_pen: 1.2356e-06\n",
      "Epoch 445/10000\n",
      "98/98 [==============================] - 4s 38ms/step - loss: 3.9124e-06 - mse: 2.6908e-06 - f_pen: 1.2216e-06 - val_loss: 6.0152e-06 - val_mse: 4.7801e-06 - val_f_pen: 1.2351e-06\n",
      "Epoch 446/10000\n",
      "98/98 [==============================] - 4s 37ms/step - loss: 3.7666e-06 - mse: 2.5455e-06 - f_pen: 1.2211e-06 - val_loss: 5.6924e-06 - val_mse: 4.4582e-06 - val_f_pen: 1.2341e-06\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00446: early stopping\n"
     ]
    }
   ],
   "source": [
    "history = dipole_model.train_model(\n",
    "    checkpoint_path=None,\n",
    "    batch_size=batch_size,\n",
    "    learning_rate=lr,\n",
    "    min_delta=min_delta\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18b66a5b",
   "metadata": {},
   "source": [
    "# Testing model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "471e1804",
   "metadata": {},
   "source": [
    "Predict on testing phase-space points generated earlier.\n",
    "\n",
    "This function will automatically calculate all the required dipoles and recoil factors to do the inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c88a40c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 0s 95ms/step\n"
     ]
    }
   ],
   "source": [
    "coefs, test_dipoles, y_preds = dipole_model.dipole_network_predictor(\n",
    "    model=dipole_model.model,\n",
    "    inputs=relevant_inputs,\n",
    "    momenta=X_test,\n",
    "    batch_size=2**16 # try increasing batch size to infer on more points at once\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "925368c4",
   "metadata": {},
   "source": [
    "Here we provide a simple plotting function that plots the error distributions in absolute percentage difference and as a prediction-to-truth ratio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f1a63409",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 648x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "utility_functions.plot_diffs(Y_test, {\"NN\": y_preds}, bins=np.linspace(-0.02, 0.02, 50));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9d1bd8c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "test_fame",
   "language": "python",
   "name": "test_fame"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
